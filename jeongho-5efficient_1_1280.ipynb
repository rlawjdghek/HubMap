{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indonesian-astrology",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ------------Library--------------#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.autograd import Variable\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import tifffile as tiff\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from sklearn.model_selection import KFold\n",
    "# loss\n",
    "#from lovasz import lovasz_hinge\n",
    "#from losses_pytorch.lovasz_loss import LovaszSoftmax\n",
    "PI  = np.pi\n",
    "INF = np.inf\n",
    "EPS = 1e-12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "upset-paint",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    amp = True\n",
    "    gpu = '0, 1'\n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    diff_arch = True\n",
    "    encoders = [\"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\"]\n",
    "    decoders = [\"unet\", \"unet\", \"unet\", \"unet\", \"unet\"]\n",
    "    \n",
    "    batch_size=8\n",
    "    weight_decay=1e-6\n",
    "    epochs=50\n",
    "    n_fold=5\n",
    "    fold=0 # [0, 1, 2, 3, 4]\n",
    "    all_fold_train = True # all fold training\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    image_size=1024 # crop size\n",
    "    crop_size=image_size\n",
    "    \n",
    "    tile_size = 1280\n",
    "    tile_step = 640\n",
    "    tile_scale = 1\n",
    "    dataset = f'{tile_scale}_{tile_size}_{tile_step}_train_fold'#'0.25_320_160_train_fold'\n",
    "    val_dataset = f'{tile_scale}_{tile_size}_{tile_size}_val_fold'\n",
    "    if diff_arch:\n",
    "        dir = f'{epochs}_{encoders}_{decoders}_{image_size}_{tile_size}_{tile_step}_{tile_scale}'\n",
    "    else:\n",
    "        dir = f'{epochs}_{encoder}_{decoder}_{image_size}_{tile_size}_{tile_step}_{tile_scale}' \n",
    "    # ---- optimizer, scheduler .. ---- #\n",
    "    T_max=10 # CosineAnnealingLR\n",
    "    opt =  'radam_look' # [adamw, radam_look]\n",
    "    scheduler='CosineAnnealingLR' #'MultiStepLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    loss = 'bce' # [lovasz, bce, bce_dice, dice]\n",
    "    factor=0.4 # ReduceLROnPlateau, MultiStepLR\n",
    "    patience=3 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    \n",
    "    decay_epoch = [4, 8, 12]\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "    #encoder_lr=4e-4\n",
    "    #decoder_lr=4e-4\n",
    "    start_lr = 1e-3\n",
    "    min_lr=1e-6\n",
    "    #----------------------------------#\n",
    "    \n",
    "    \n",
    "    # ----- 여러 시도 ------#\n",
    "    clf_head=False # encoder에 classfication head 붙일지 여부\n",
    "    label_smoothing = False # label smoothing 여부\n",
    "    multi_gpu=True if len(gpu)>1 else False # multi gpu 사용\n",
    "    clf_alpha = 0.3 # classification head 의 loss 비율\n",
    "    smoothing = 0.1 # label smoothing factor\n",
    "    dice_smoothing = 1 # dice loss 사용시 하이퍼 파라미터\n",
    "    \n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=42\n",
    "    \n",
    "data_dir = '/home/jeonghokim/competition/HubMap/data/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-product",
   "metadata": {},
   "source": [
    "# useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continental-community",
   "metadata": {
    "code_folding": [
     0,
     2,
     13,
     24,
     42,
     56,
     73,
     87,
     108,
     125,
     133,
     146,
     192,
     228,
     230,
     234,
     249
    ]
   },
   "outputs": [],
   "source": [
    "#-------evaluation metric, loss---------#\n",
    "###################################\n",
    "def np_binary_cross_entropy_loss(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    #---\n",
    "    logp = -np.log(np.clip(p,1e-6,1))\n",
    "    logn = -np.log(np.clip(1-p,1e-6,1))\n",
    "    loss = t*logp +(1-t)*logn\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def np_dice_score(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "def dice_score(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "    https://catalyst-team.github.io/catalyst/_modules/catalyst/dl/utils/criterion/dice.html\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "        targets = (targets > threshold).float()\n",
    "\n",
    "    intersection = torch.sum(targets * outputs)\n",
    "    union = torch.sum(targets) + torch.sum(outputs)\n",
    "    dice = 2 * intersection / (union + eps)\n",
    "\n",
    "    return dice\n",
    "def torch_accuracy(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,):\n",
    "\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "        \n",
    "    tp = torch.sum(targets*outputs)/torch.sum(targets)\n",
    "    tn = torch.sum((1-outputs)*(1-targets))/torch.sum(1-targets)\n",
    "\n",
    "    return tp, tn\n",
    "\n",
    "def np_accuracy(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    tp = (p*t).sum()/((t).sum()+1e-7)\n",
    "    tn = ((1-p)*(1-t)).sum()/(1-t).sum()\n",
    "    return tp, tn\n",
    "\n",
    "def criterion_binary_cross_entropy(logit, mask):\n",
    "    logit = logit.reshape(-1)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logit, mask)\n",
    "    return loss\n",
    "\n",
    "# threshold dice score\n",
    "def np_dice_score2(probability, mask, threshold):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>threshold\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "# --------------------\n",
    "# Loss\n",
    "# --------------------\n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=args.smoothing):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "        \n",
    "        Dice_BCE = BCE*0.6 + dice_loss*0.4\n",
    "        \n",
    "        return Dice_BCE.mean()\n",
    "class DiceLoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=args.dice_smoothing):\n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        inputs = F.sigmoid(inputs)   \n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "                \n",
    "        return dice_loss.mean()\n",
    "    \n",
    "#PyTorch lovasz\n",
    "def symmetric_lovasz(outputs, targets):\n",
    "    return 0.5*(lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1.0 - targets))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#from torch.autograd import Function\n",
    "# copy from: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/LovaszSoftmax/lovasz_loss.py\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "class LovaszSoftmax(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(LovaszSoftmax, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def prob_flatten(self, input, target):\n",
    "        assert input.dim() in [4, 5]\n",
    "        num_class = input.size(1)\n",
    "        if input.dim() == 4:\n",
    "            input = input.permute(0, 2, 3, 1).contiguous()\n",
    "            input_flatten = input.view(-1, num_class)\n",
    "        elif input.dim() == 5:\n",
    "            input = input.permute(0, 2, 3, 4, 1).contiguous()\n",
    "            input_flatten = input.view(-1, num_class)\n",
    "        target_flatten = target.view(-1)\n",
    "        return input_flatten, target_flatten\n",
    "\n",
    "    def lovasz_softmax_flat(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "        losses = []\n",
    "        for c in range(num_classes):\n",
    "            target_c = (targets == c).float()\n",
    "            if num_classes == 1:\n",
    "                input_c = inputs[:, 0]\n",
    "            else:\n",
    "                input_c = inputs[:, c]\n",
    "            loss_c = (torch.autograd.Variable(target_c) - input_c).abs()\n",
    "            loss_c_sorted, loss_index = torch.sort(loss_c, 0, descending=True)\n",
    "            target_c_sorted = target_c[loss_index]\n",
    "            losses.append(torch.dot(loss_c_sorted, torch.autograd.Variable(lovasz_grad(target_c_sorted))))\n",
    "        losses = torch.stack(losses)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            loss = losses\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = losses.sum()\n",
    "        else:\n",
    "            loss = losses.mean()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # print(inputs.shape, targets.shape) # (batch size, class_num, x,y,z), (batch size, 1, x,y,z)\n",
    "        inputs, targets = self.prob_flatten(inputs, targets)\n",
    "        # print(inputs.shape, targets.shape)\n",
    "        losses = self.lovasz_softmax_flat(inputs, targets)\n",
    "        return losses\n",
    "class Lovasz_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lovasz_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        return LovaszSoftmax()(inputs, targets)\n",
    "###################################\n",
    "#-------ELSE function---------#\n",
    "###################################\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "def print_args(args, logger=None):\n",
    "    for k, v in vars(args).items():\n",
    "        if logger is not None:\n",
    "            logger.write('{:<16} : {}\\n'.format(k, v))\n",
    "        else:\n",
    "            print('{:<16} : {}'.format(k, v))\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "###########################\n",
    "#---- label smoothing -----\n",
    "###########################\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = args.smoothing):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = x.float().flatten()\n",
    "        target = target.float() * (1-self.smoothing) + 0.5 * self.smoothing\n",
    "        target = target.flatten()\n",
    "\n",
    "\n",
    "        loss  = F.binary_cross_entropy_with_logits(x, target, reduction='mean')\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contained-information",
   "metadata": {
    "code_folding": [
     0,
     1,
     20,
     27,
     52,
     67,
     79,
     95,
     153,
     196,
     208
    ]
   },
   "outputs": [],
   "source": [
    "#-------masking & tile & decode---------#\n",
    "def read_tiff(image_file):\n",
    "    \"\"\"\n",
    "    *data size*\n",
    "    e.g.) (3, w, h) or (1,1,3,w,h) or (w, h, 3)  --> transform --> (w, h, 3)\n",
    "    \"\"\"\n",
    "    image = tiff.imread(image_file)\n",
    "    if image.shape[0] == 1:\n",
    "        image = image[0][0]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    elif image.shape[0] == 3:\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    return image\n",
    "\n",
    "def read_mask(mask_file):\n",
    "    mask = np.array(Image.open(mask_file))\n",
    "    return mask\n",
    "\n",
    "def read_json_as_df(json_file):\n",
    "    with open(json_file) as f:\n",
    "        j = json.load(f)\n",
    "    df = pd.json_normalize(j)\n",
    "    return df\n",
    "\n",
    "\n",
    "def draw_strcuture(df, height, width, fill=255, structure=[]):\n",
    "    mask = np.zeros((height, width), np.uint8)\n",
    "    for row in df.values:\n",
    "        type  = row[2]  #geometry.type\n",
    "        coord = row[3]  # geometry.coordinates\n",
    "        name  = row[4]   # properties.classification.name\n",
    "\n",
    "        if structure !=[]:\n",
    "            if not any(s in name for s in structure): continue\n",
    "\n",
    "\n",
    "        if type=='Polygon':\n",
    "            pt = np.array(coord).astype(np.int32)\n",
    "            #cv2.polylines(mask, [coord.reshape((-1, 1, 2))], True, 255, 1)\n",
    "            cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "        if type=='MultiPolygon':\n",
    "            for pt in coord:\n",
    "                pt = np.array(pt).astype(np.int32)\n",
    "                cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# resize, cvtcolor, generate mask\n",
    "# 원하는 object 영역만 따오는 mask\n",
    "def draw_strcuture_from_hue(image, fill=255, scale=1/32): # 0.25/32 default\n",
    "    height, width, _ = image.shape\n",
    "    vv = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = cv2.cvtColor(vv, cv2.COLOR_RGB2HSV)\n",
    "    # image_show('v[0]', v[:,:,0])\n",
    "    # image_show('v[1]', v[:,:,1])\n",
    "    # image_show('v[2]', v[:,:,2])\n",
    "    # cv2.waitKey(0)\n",
    "    mask = (vv[:, :, 1] > 32).astype(np.uint8) # rgb2hsv를 하고나서 1채널에 대해 시행하면 원하는 object만 잘따온다.\n",
    "    mask = mask*fill\n",
    "    mask = cv2.resize(mask, dsize=(width, height), interpolation=cv2.INTER_LINEAR) # 다시 원래사이즈로 복구\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --- rle ---------------------------------\n",
    "def rle_decode(rle, height, width , fill=255):\n",
    "    s = rle.split()\n",
    "    start, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    start -= 1\n",
    "    mask = np.zeros(height*width, dtype=np.uint8)\n",
    "    for i, l in zip(start, length):\n",
    "        mask[i:i+l] = fill\n",
    "    mask = mask.reshape(width,height).T\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def rle_encode(mask):\n",
    "    m = mask.T.flatten()\n",
    "    m = np.concatenate([[0], m, [0]])\n",
    "    run = np.where(m[1:] != m[:-1])[0] + 1\n",
    "    run[1::2] -= run[::2]\n",
    "    rle =  ' '.join(str(r) for r in run)\n",
    "    return rle\n",
    "\n",
    "\n",
    "# --- tile ---------------------------------\n",
    "\"\"\"\n",
    "-결국, tile_image, tile_mask만 가져다가 쓴다.\n",
    "1. scale로 resize를 하고 image size와 step만큼 건너뛰며 이미지를 만든다.\n",
    "2. 이때 일정 영역이 빈마스크면 데이터에서 제외한다.\n",
    "3. 쌓은 image와 mask를 return\n",
    "\"\"\"\n",
    "def to_tile(image, mask, structure, scale, size, step, min_score): \n",
    "    half = size//2\n",
    "    image_small = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR) # defualt는 1/4만큼 w,h를 줄인다.\n",
    "    height, width, _ = image_small.shape\n",
    "\n",
    "    #make score\n",
    "    structure_small = cv2.resize(structure, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = structure_small.astype(np.float32)/255\n",
    "\n",
    "    #make coord\n",
    "    xx = np.linspace(half, width  - half, int(np.ceil((width  - size) / step)))\n",
    "    yy = np.linspace(half, height - half, int(np.ceil((height - size) / step)))\n",
    "    xx = [int(x) for x in xx]\n",
    "    yy = [int(y) for y in yy]\n",
    "\n",
    "    coord  = []\n",
    "    reject = []\n",
    "    for cy in yy:\n",
    "        for cx in xx:\n",
    "            cv = vv[cy - half:cy + half, cx - half:cx + half].mean() # h, w // tiling한 마스크(structure)가 평균 0.25를 안넘으면 버린다.\n",
    "            if cv>min_score: # min_score ,default:0.25, 0.25의 의미?, 타일링 이미지의 1/4는 object여야 한다는 의미?\n",
    "                coord.append([cx,cy,cv])\n",
    "            else:\n",
    "                reject.append([cx,cy,cv])\n",
    "    #-----\n",
    "    if 1: # resize한 image를 tiling 하여 리스트만든다\n",
    "        tile_image = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = image_small[cy - half:cy + half, cx - half:cx + half] # resize한 image에서 indexing만 하는과정\n",
    "            assert (t.shape == (size, size, 3))\n",
    "            tile_image.append(t)\n",
    "\n",
    "    if mask is not None: # mask를 resize하고 tiling하여 리스트 만든다\n",
    "        mask_small = cv2.resize(mask, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        tile_mask = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = mask_small[cy - half:cy + half, cx - half:cx + half]\n",
    "            assert (t.shape == (size, size))\n",
    "            tile_mask.append(t)\n",
    "    else:\n",
    "        mask_small = None\n",
    "        tile_mask  = None\n",
    "\n",
    "    return {\n",
    "        'image_small': image_small,\n",
    "        'mask_small' : mask_small,\n",
    "        'structure_small' : structure_small,\n",
    "        'tile_image' : tile_image,\n",
    "        'tile_mask'  : tile_mask,\n",
    "        'coord'  : coord,\n",
    "        'reject' : reject,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "submission할때 쓰임\n",
    "\"\"\"\n",
    "def to_mask(tile, coord, height, width, scale, size, step, min_score, aggregate='mean'):\n",
    "\n",
    "    half = size//2\n",
    "    mask  = np.zeros((height, width), np.float32)\n",
    "\n",
    "    if 'mean' in aggregate:\n",
    "        w = np.ones((size,size), np.float32)\n",
    "\n",
    "        #if 'sq' in aggregate:\n",
    "        if 1:\n",
    "            #https://stackoverflow.com/questions/17190649/how-to-obtain-a-gaussian-filter-in-python\n",
    "            y,x = np.mgrid[-half:half,-half:half]\n",
    "            y = half-abs(y)\n",
    "            x = half-abs(x)\n",
    "            w = np.minimum(x,y)\n",
    "            w = w/w.max()#*2.5\n",
    "            w = np.minimum(w,1)\n",
    "\n",
    "        #--------------\n",
    "        count = np.zeros((height, width), np.float32)\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask [cy - half:cy + half, cx - half:cx + half] += tile[t]*w\n",
    "            count[cy - half:cy + half, cx - half:cx + half] += w\n",
    "               # see unet paper for \"Overlap-tile strategy for seamless segmentation of arbitrary large images\"\n",
    "        m = (count != 0)\n",
    "        mask[m] /= count[m]\n",
    "\n",
    "    if aggregate=='max':\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask[cy - half:cy + half, cx - half:cx + half] = np.maximum(\n",
    "                mask[cy - half:cy + half, cx - half:cx + half], tile[t] )\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --------------이 아래로 안씀 ------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# --draw ------------------------------------------\n",
    "\"\"\"\n",
    "경계선을 그리게 만든다, 컨투어\n",
    "하지만 안씀\n",
    "\"\"\"\n",
    "def mask_to_inner_contour(mask):\n",
    "    mask = mask>0.5\n",
    "    pad = np.lib.pad(mask, ((1, 1), (1, 1)), 'reflect')\n",
    "    contour = mask & (\n",
    "            (pad[1:-1,1:-1] != pad[:-2,1:-1]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[2:,1:-1])  \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,:-2]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,2:])\n",
    "    )\n",
    "    return contour\n",
    "\n",
    "\n",
    "def draw_contour_overlay(image, mask, color=(0,0,255), thickness=1):\n",
    "    contour =  mask_to_inner_contour(mask)\n",
    "    if thickness==1:\n",
    "        image[contour] = color\n",
    "    else:\n",
    "        r = max(1,thickness//2)\n",
    "        for y,x in np.stack(np.where(contour)).T:\n",
    "            cv2.circle(image, (x,y), r, color, lineType=cv2.LINE_4 )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-external",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intense-metro",
   "metadata": {
    "code_folding": [
     15,
     75,
     137,
     198,
     227,
     291,
     318
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------ make dataset  new version image fold--------- #\n",
    "#################################\n",
    "\"\"\"\n",
    "- robust validation을 위해 overlap 없는 데이터도 만든다\n",
    "\"\"\"\n",
    "# <todo> make difference scale tile\n",
    "\n",
    "tile_scale = 1\n",
    "tile_min_score = 0.25\n",
    "tile_size = 1280#320  # 480 #\n",
    "tile_average_step = 640#160 #240  # 160 #192\n",
    "tile_average_step2 = tile_size\n",
    "\n",
    "#make tile train image\n",
    "# train,tiling (image,mask) png 저장용도\n",
    "def run_make_train_tile():\n",
    "\n",
    "    train_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_train_fold' #nipa2\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "    \n",
    "    df_all = []\n",
    "    \n",
    "    os.makedirs(train_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "        # 1. image 불러오고\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        #mask = rle_decode(encoding, height, width, 255)\n",
    "        # 2. mask, target 불러오고\n",
    "        mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "        mask = read_mask(mask_file)\n",
    "        \n",
    "        # 3. 일정영역,object만 표시한 mask불러오기.\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "        print(id, mask_file)\n",
    "        \n",
    "        # make tile\n",
    "        # 4. 학습할 tile image, mask를 생성한다.\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(train_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            #s = '%s_y%08d_x%08d' % (id, cy, cx)\n",
    "            s = 'y%08d_x%08d' %(cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.png' %(id, s), tile_image)\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.mask.png' %(id, s), tile_mask)\n",
    "\n",
    "\n",
    "        df_image['tile_id']= [f'{train_tile_dir}/{id}/'+ x for x in tile_id]\n",
    "        df_all.append(df_image)\n",
    "    df_all = pd.concat(df_all, 0).reset_index(drop=True)\n",
    "    df_all[['tile_id','cx','cy','cv']].to_csv(train_tile_dir+'/image_id.csv', index=False)\n",
    "#------\n",
    "# maek tile val image\n",
    "def run_make_val_tile():\n",
    "\n",
    "    train_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step2}_val_fold' #nipa2\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "    \n",
    "    df_all = []\n",
    "    \n",
    "    os.makedirs(train_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "        # 1. image 불러오고\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        #mask = rle_decode(encoding, height, width, 255)\n",
    "        # 2. mask, target 불러오고\n",
    "        mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "        mask = read_mask(mask_file)\n",
    "        \n",
    "        # 3. 일정영역,object만 표시한 mask불러오기.\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "        print(id, mask_file)\n",
    "        \n",
    "        # make tile\n",
    "        # 4. 학습할 tile image, mask를 생성한다.\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step2, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(train_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            #s = '%s_y%08d_x%08d' % (id, cy, cx)\n",
    "            s = 'y%08d_x%08d' %(cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.png' %(id, s), tile_image)\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.mask.png' %(id, s), tile_mask)\n",
    "\n",
    "\n",
    "        df_image['tile_id']= [f'{train_tile_dir}/{id}/'+ x for x in tile_id]\n",
    "        df_all.append(df_image)\n",
    "    df_all = pd.concat(df_all, 0).reset_index(drop=True)\n",
    "    df_all[['tile_id','cx','cy','cv']].to_csv(train_tile_dir+'/image_id.csv', index=False)\n",
    "\n",
    "    \n",
    "#make tile train image\n",
    "# test tiling image png 저장용도\n",
    "def run_make_test_tile():\n",
    "    #tile_scale = 0.25\n",
    "    #tile_min_score = 0.25\n",
    "    #tile_size = 480#320  # 480 #\n",
    "    #tile_average_step = 240#160 #240  # 160 #192\n",
    "\n",
    "    #test_tile_dir = '/home/ubuntu/gwang/hubmap/etc/tile/0.25_640_320_test'\n",
    "    test_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_test'\n",
    "    #---\n",
    "\n",
    "\n",
    "    os.makedirs(test_tile_dir, exist_ok=True)\n",
    "    assert False, 'todo modify test file'\n",
    "    for id in ['c68fe75ea','afa5e8098',]:\n",
    "        print(id)\n",
    "\n",
    "        # 1. test image load\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "        image = read_tiff(image_file)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        mask = None\n",
    "        # 2. test structure load\n",
    "        structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "        # structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "\n",
    "        # 3. test를 위한 tile image 생성\n",
    "        #make tile\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(test_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            s = 'y%08d_x%08d' % (cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            cv2.imwrite(test_tile_dir + '/%s/%s.png' % (id, s), tile_image)\n",
    "            #image_show('tile_image', tile_image)\n",
    "            #cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        df_image['tile_id']=tile_id\n",
    "        df_image[['tile_id','cx','cy','cv']].to_csv(test_tile_dir+'/%s.csv'%id, index=False)\n",
    "        #------\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "# tile이 아닌 train image의 mask생성\n",
    "def run_make_train_mask():\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        if image.shape[0]==1:\n",
    "            image = image[0][0]\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            image = np.ascontiguousarray(image)\n",
    "            height, width = image.shape[:2]\n",
    "        elif image.shape[0] == 3:\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            image = np.ascontiguousarray(image)\n",
    "            height, width = image.shape[:2]\n",
    "        else:\n",
    "            height, width = image.shape[:2]\n",
    "        mask = rle_decode(encoding, height, width, 255)\n",
    "\n",
    "        cv2.imwrite(data_dir + '/train/%s.mask.png' % id, mask)\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "def run_make_pseudo_tile():\n",
    "\n",
    "    \n",
    "    tile_scale = 0.25\n",
    "    tile_min_score = 0.25\n",
    "    tile_size = 480#320  #480 #\n",
    "    tile_average_step = 240 #160 #240  # 192\n",
    "    #---\n",
    "    pseudo_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_pseudo_0.95'\n",
    "    #df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    #df_pseudo = pd.read_csv('/root/share1/kaggle/2020/hubmap/result/resnet34/fold2/submit-fold-2-resnet34-00010000_model_lb0.837.csv')\n",
    "    df_pseudo = pd.read_csv('../../submission/0.891_submission-fold6-00004000_model_thres-0.9.csv')\n",
    "    \n",
    "    print(df_pseudo)\n",
    "    print(df_pseudo.shape)\n",
    "\n",
    "    os.makedirs(pseudo_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_pseudo)):\n",
    "        id, encoding = df_pseudo.iloc[i]\n",
    "\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        mask = rle_decode(encoding, height, width, 255)\n",
    "\n",
    "        #make tile\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "        #to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        #mask, scale, size, step, min_score\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(pseudo_tile_dir + '/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            s = 'y%08d_x%08d' % (cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(pseudo_tile_dir + '/%s/%s.png' % (id, s), tile_image)\n",
    "            cv2.imwrite(pseudo_tile_dir + '/%s/%s.mask.png' % (id, s), tile_mask)\n",
    "\n",
    "            #image_show('tile_image', tile_image)\n",
    "            #image_show('tile_mask', tile_mask)\n",
    "            #cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        df_image['tile_id']=tile_id\n",
    "        df_image[['tile_id','cx','cy','cv']].to_csv(pseudo_tile_dir+'/%s.csv'%id, index=False)\n",
    "        #------\n",
    "\n",
    "def split_fold():\n",
    "    \n",
    "    df = pd.read_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step}_train_fold/image_id.csv')\n",
    "    df2 = pd.read_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step2}_val_fold/image_id.csv')\n",
    "\n",
    "    a = {0 : '0486052bb', 1 : '095bf7a1f', 2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce', 6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', 10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4', 14 :'e79de561c'}\n",
    "    #\n",
    "    kf = KFold(n_splits=args.n_fold, random_state=args.seed, shuffle=True)\n",
    "    fold_dict={}\n",
    "    for n, (t,v) in enumerate(kf.split(a)):\n",
    "        for f in v:\n",
    "            fold_dict[a[f]] = n\n",
    "\n",
    "    df['fold'] = df['tile_id'].apply(lambda x : x.split('/')[-2])\n",
    "    df['fold'] = df['fold'].apply(lambda x :fold_dict[x])\n",
    "    \n",
    "    df2['fold'] = df2['tile_id'].apply(lambda x : x.split('/')[-2])\n",
    "    df2['fold'] = df2['fold'].apply(lambda x :fold_dict[x])\n",
    "    \n",
    "    df.to_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step}_train_fold/image_id_split.csv', index=False)\n",
    "    df2.to_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step2}_val_fold/image_id_split.csv', index=False)\n",
    "    print('saved split fold')\n",
    "    \n",
    "# main #################################################################\n",
    "if 0:\n",
    "    if __name__ == '__main__':\n",
    "        #print('started run make train mask')\n",
    "        # 1.\n",
    "        print('started 1')\n",
    "        run_make_train_mask()\n",
    "        # 2.\n",
    "        print('started 2')\n",
    "        run_make_train_tile()\n",
    "        # 3.\n",
    "        print('started 3')\n",
    "        run_make_val_tile()\n",
    "        \n",
    "        #print('started 3')\n",
    "        #run_make_test_tile()\n",
    "        # 4. if use pseudo datasets\n",
    "        #run_make_pseudo_tile()\n",
    "        \n",
    "        print('split kfold csv')\n",
    "        split_fold()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-thing",
   "metadata": {},
   "source": [
    "# Dataset & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hired-updating",
   "metadata": {
    "code_folding": [
     6,
     44,
     92,
     130,
     131,
     206,
     242
    ]
   },
   "outputs": [],
   "source": [
    "#--------------- Dataset ----------------#\n",
    "##########################################\n",
    "\n",
    "#--------------- \n",
    "# Old version\n",
    "#--------------- \n",
    "def make_image_id_v1(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset_v1(Dataset):\n",
    "    def __init__(self, image_id, image_dir, augment=None):\n",
    "        self.augment = augment\n",
    "        self.image_id = image_id\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        tile_id = []\n",
    "        for i in range(len(image_dir)):\n",
    "            for id in image_id[i]: \n",
    "                df = pd.read_csv(data_dir + '/tile/%s/%s.csv'% (self.image_dir[i],id) )\n",
    "                tile_id += ('%s/%s/'%(self.image_dir[i],id) + df.tile_id).tolist()\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        string += '\\timage_dir = %s\\n'%self.image_dir\n",
    "        string += '\\timage_id  = %s\\n'%str(self.image_id)\n",
    "        string += '\\t          = %d\\n'%sum(len(i) for i in self.image_id)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(data_dir + '/tile/%s.png'%(id), cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(data_dir + '/tile/%s.mask.png'%(id), cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "\n",
    "#--------------- \n",
    "# Old version (simple fold)\n",
    "#--------------- \n",
    "def make_image_id_(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset_(Dataset):\n",
    "    def __init__(self, tile_id, augment=None):\n",
    "        self.augment = augment\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.png', cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment()\n",
    "        \n",
    "        return r\n",
    "\n",
    "#--------------- \n",
    "# New version(image fold)\n",
    "#--------------- \n",
    "def make_image_id(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "        self.augment = augment\n",
    "\n",
    "        #self.tile_id = tile_id\n",
    "        #self.len =len(self.tile_id)\n",
    "        self.df = df\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.df['tile_id'].loc[index]\n",
    "        image = cv2.imread(f'{id}.png', cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(f'{id}.mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        #if self.augment is not None: r = self.augment(image=r['image'], mask=r['mask'])\n",
    "        return r\n",
    "\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    index = []\n",
    "    mask = []\n",
    "    image = []\n",
    "    for r in batch:\n",
    "        index.append(r['index'])\n",
    "        mask.append(r['mask'])\n",
    "        image.append(r['image'])\n",
    "\n",
    "    image = np.stack(image)\n",
    "    image = image[...,::-1]\n",
    "    image = image.transpose(0,3,1,2)\n",
    "    image = np.ascontiguousarray(image)\n",
    "\n",
    "    mask  = np.stack(mask)\n",
    "    mask  = np.ascontiguousarray(mask)\n",
    "\n",
    "    #---\n",
    "    image = torch.from_numpy(image).contiguous().float()\n",
    "    mask  = torch.from_numpy(mask).contiguous().unsqueeze(1)\n",
    "    mask  = (mask>0.5).float()\n",
    "\n",
    "    return {\n",
    "        'index' : index,\n",
    "        'mask' : mask,\n",
    "        'image' : image,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "narrow-schema",
   "metadata": {
    "code_folding": [
     0,
     3,
     19,
     27,
     42,
     82,
     91,
     97,
     103,
     122
    ]
   },
   "outputs": [],
   "source": [
    "#---------- augmentation ---------------------#\n",
    "###############################################\n",
    "#flip\n",
    "def do_random_flip_transpose(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = image.transpose(1,0,2)\n",
    "        mask = mask.transpose(1,0)\n",
    "\n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return image, mask\n",
    "\n",
    "#geometric\n",
    "def do_random_crop(image, mask, size):\n",
    "    height, width = image.shape[:2]\n",
    "    x = np.random.choice(width -size)\n",
    "    y = np.random.choice(height-size)\n",
    "    image = image[y:y+size,x:x+size]\n",
    "    mask  = mask[y:y+size,x:x+size]\n",
    "    return image, mask\n",
    "\n",
    "def do_random_scale_crop(image, mask, size, mag):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    s = 1 + np.random.uniform(-1, 1)*mag\n",
    "    s =  int(s*size)\n",
    "\n",
    "    x = np.random.choice(width -s)\n",
    "    y = np.random.choice(height-s)\n",
    "    image = image[y:y+s,x:x+s]\n",
    "    mask  = mask[y:y+s,x:x+s]\n",
    "    if s!=size:\n",
    "        image = cv2.resize(image, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "        mask  = cv2.resize(mask, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rotate_crop(image, mask, size, mag=30 ):\n",
    "    angle = 1+np.random.uniform(-1, 1)*mag\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    dst = np.array([\n",
    "        [0,0],[size,size], [size,0], [0,size],\n",
    "    ])\n",
    "\n",
    "    c = np.cos(angle/180*2*PI)\n",
    "    s = np.sin(angle/180*2*PI)\n",
    "    src = (dst-size//2)@np.array([[c, -s],[s, c]]).T\n",
    "    src[:,0] -= src[:,0].min()\n",
    "    src[:,1] -= src[:,1].min()\n",
    "\n",
    "    src[:,0] = src[:,0] + np.random.uniform(0,width -src[:,0].max())\n",
    "    src[:,1] = src[:,1] + np.random.uniform(0,height-src[:,1].max())\n",
    "\n",
    "    if 0: #debug\n",
    "        def to_int(f):\n",
    "            return (int(f[0]),int(f[1]))\n",
    "\n",
    "        cv2.line(image, to_int(src[0]), to_int(src[1]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[1]), to_int(src[2]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[2]), to_int(src[3]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[3]), to_int(src[0]), (0,0,1), 16)\n",
    "        image_show_norm('image', image, min=0, max=1)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "    transform = cv2.getAffineTransform(src[:3].astype(np.float32), dst[:3].astype(np.float32))\n",
    "    image = cv2.warpAffine( image, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "    mask  = cv2.warpAffine( mask, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask\n",
    "\n",
    "#warp/elastic deform ...\n",
    "#<todo>\n",
    "\n",
    "#noise\n",
    "def do_random_noise(image, mask, mag=0.1):\n",
    "    height, width = image.shape[:2]\n",
    "    noise = np.random.uniform(-1,1, (height, width,1))*mag\n",
    "    image = image + noise\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "#intensity\n",
    "def do_random_contast(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image * alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_gain(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image ** alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h = hsv[:, :, 0].astype(np.float32)  # hue\n",
    "    s = hsv[:, :, 1].astype(np.float32)  # saturation\n",
    "    v = hsv[:, :, 2].astype(np.float32)  # value\n",
    "    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n",
    "    s =  s*(1 + random.uniform(-1,1)*mag[1])\n",
    "    v =  v*(1 + random.uniform(-1,1)*mag[2])\n",
    "\n",
    "    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n",
    "    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n",
    "    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    image = image.astype(np.float32)/255\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def filter_small(mask, min_size):\n",
    "\n",
    "    m = (mask*255).astype(np.uint8)\n",
    "\n",
    "    num_comp, comp, stat, centroid = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
    "    if num_comp==1: return mask\n",
    "\n",
    "    filtered = np.zeros(comp.shape,dtype=np.uint8)\n",
    "    area = stat[:, -1]\n",
    "    for i in range(1, num_comp):\n",
    "        if area[i] >= min_size:\n",
    "            filtered[comp == i] = 255\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-commander",
   "metadata": {
    "code_folding": [
     0,
     2,
     41,
     117
    ]
   },
   "outputs": [],
   "source": [
    "#---------- optimizer, scheduler ---------------------#\n",
    "############################################\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, alpha=0.5, k=6):\n",
    "\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "\n",
    "        self.slow_weights = [\n",
    "                [p.clone().detach() for p in group['params']]\n",
    "            for group in self.param_groups]\n",
    "\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        self.state = optimizer.state\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        loss = self.optimizer.step()\n",
    "\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(p.data - q.data, alpha=self.alpha )\n",
    "                p.data.copy_(q.data)\n",
    "        return loss\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "#---------- scheduler ---------------------#\n",
    "def get_scheduler(optimizer):\n",
    "    if args.scheduler =='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = args.epochs//args.T_0, T_mult=1, eta_min=0, last_epoch=-1)\n",
    "    elif args.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.min_lr, last_epoch=-1)\n",
    "    elif args.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=args.factor, patience=args.patience, verbose=True, \n",
    "                                      min_lr = args.min_lr, eps=args.eps)\n",
    "    else:\n",
    "        scheduler=None\n",
    "        assert False, 'not implement'\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-relief",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "established-glasgow",
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "class DOWNBLOCK(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DOWNBLOCK, self).__init__()\n",
    "        self.down_conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.down_bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.down_bn1(self.down_conv1(x)))\n",
    "        return x\n",
    "    \n",
    "def SegModel():\n",
    "    models = []\n",
    "    # 다른 모덷들일때\n",
    "    if args.diff_arch:\n",
    "        for i in range(args.n_fold):\n",
    "            en_name = args.encoders[i]\n",
    "            de_name = args.decoders[i]\n",
    "            # decoder별로 로드\n",
    "            if de_name.lower() == \"unet\":\n",
    "                if args.clf_head:\n",
    "                    print('classification head')\n",
    "                    aux_params=dict(\n",
    "                        pooling='avg',             # one of 'avg', 'max'\n",
    "                        dropout=0.5,               # dropout ratio, default is None\n",
    "                        activation='sigmoid',      # activation function, default is None\n",
    "                        classes=1,                 # define number of output labels\n",
    "                    )\n",
    "                    model = smp.Unet(\n",
    "                        encoder_name=en_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                        in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                        classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                        aux_params=aux_params\n",
    "                        )\n",
    "                else:\n",
    "                    model = smp.Unet(\n",
    "                        encoder_name=en_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                        in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                        classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                        )\n",
    "            elif de_name.lower() == \"fpn\":\n",
    "                model = smp.FPN(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            elif de_name.lower() == \"upp\":\n",
    "                model = smp.UnetPlusPlus(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            elif de_name.lower() == \"linknet\":\n",
    "                model = smp.Linknet(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            models.append(model)\n",
    "                \n",
    "        \n",
    "    # 같은 모델 일 때 5개 복사\n",
    "    else:\n",
    "        if args.encoder in ['b0','b1','b2','b3','b4','b5','b6','b7']:\n",
    "            encoder_name_ = f'efficientnet-{args.encoder}' #'timm-efficientnet-b4'\n",
    "            print('encoder : ', encoder_name_)\n",
    "        else:\n",
    "            encoder_name_ = args.encoder\n",
    "        if args.decoder =='fpn':\n",
    "            print('fpn loaded')\n",
    "            model = smp.FPN(\n",
    "                encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                )\n",
    "        elif args.decoder =='unet':\n",
    "            print('unet loaded')\n",
    "            if args.clf_head:\n",
    "                print('classification head')\n",
    "                aux_params=dict(\n",
    "                    pooling='avg',             # one of 'avg', 'max'\n",
    "                    dropout=0.5,               # dropout ratio, default is None\n",
    "                    activation='sigmoid',      # activation function, default is None\n",
    "                    classes=1,                 # define number of output labels\n",
    "                )\n",
    "                model = smp.Unet(\n",
    "                    encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                    aux_params=aux_params\n",
    "                    )\n",
    "            else:\n",
    "                model = smp.Unet(\n",
    "                    encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "                #list_ = [DOWNBLOCK(), model, nn.Upsample(size=640, mode='bilinear', align_corners=True)]\n",
    "                #model = nn.Sequential(*list_)\n",
    "        if args.encoder=='R50':\n",
    "            if args.decoder=='ViT':\n",
    "                vit_name='R50-ViT-B_16'\n",
    "                config_vit = CONFIGS[vit_name]\n",
    "                config_vit.n_classes = 1\n",
    "                config_vit.n_skip = 3\n",
    "                if vit_name.find('R50') != -1:\n",
    "                    config_vit.patches.grid = (int(args.image_size / 16), int(args.image_size / 16))\n",
    "                model = VisionTransformer(config_vit, img_size=args.image_size, num_classes=config_vit.n_classes) \n",
    "        for i in range(args.n_fold):\n",
    "            models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-participant",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "inappropriate-curve",
   "metadata": {
    "code_folding": [
     4,
     30
    ]
   },
   "outputs": [],
   "source": [
    "# ------- image fold train version ------- #\n",
    "\n",
    "# augmentation\n",
    "#\"\"\"현재 crop 없는상태\"\"\"\n",
    "def train_augment(record):\n",
    "    image = record['image']\n",
    "    mask  = record['mask']\n",
    "    \n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask : do_random_rotate_crop(image, mask, size=args.crop_size, mag=45),\n",
    "        lambda image, mask : do_random_scale_crop(image, mask, size=args.crop_size, mag=0.075),\n",
    "        lambda image, mask : do_random_crop(image, mask, size=args.crop_size),\n",
    "    ],1): image, mask = fn(image, mask)\n",
    "\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask : (image, mask),\n",
    "        lambda image, mask : do_random_contast(image, mask, mag=0.8),\n",
    "        lambda image, mask : do_random_gain(image, mask, mag=0.9),\n",
    "        #lambda image, mask : do_random_hsv(image, mask, mag=[0.1, 0.2, 0]),\n",
    "        lambda image, mask : do_random_noise(image, mask, mag=0.1),\n",
    "    ],2): image, mask =  fn(image, mask)\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    image, mask = do_random_hsv(image, mask, mag=[0.1, 0.2, 0])\n",
    "    image, mask = do_random_flip_transpose(image, mask)\n",
    "\n",
    "    record['mask'] = mask\n",
    "    record['image'] = image\n",
    "    return record\n",
    "#그냥 데이터 로더 3개 만들어서 이미지별로 각각 계산해서 평균하자..\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2, valid_probability3 = [],[],[]\n",
    "    valid_mask, valid_mask2, valid_mask3 = [],[],[]\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            batch_size = len(batch['index'])\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "            \n",
    "            if args.clf_head:\n",
    "                logit, _ = net(image) # seg, clf\n",
    "            else:\n",
    "                logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "                \n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "    #assert(valid_num == len(valid_loader.dataset)) # drop last True이면 assert되는거임\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    if args.loss =='bce':\n",
    "        loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "    elif args.loss =='lovasz':\n",
    "        loss = 0\n",
    "    \n",
    "    # mean loss, dice ..\n",
    "    dice = np_dice_score(probability, mask)\n",
    "    tp, tn = np_accuracy(probability, mask)\n",
    "\n",
    "    return [dice, loss,  tp, tn]\n",
    "\n",
    "def run_train(args):\n",
    "    out_dir = data_dir + f'/result/{args.dir}_{args.encoder}_{args.image_size}'\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint','train','valid'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "    #backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.train.txt',mode='a')\n",
    "\n",
    "    # my log argument\n",
    "    print_args(args, log)\n",
    "\n",
    "    log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "    log.write('** dataset setting **\\n')\n",
    "    #-----------dataset split --------------------#\n",
    "    tile_id = []\n",
    "    image_dir_ = f'{args.dataset}'#'0.25_320_160_train'\n",
    "    image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "    \n",
    "    image_dir_val_ = f'{args.val_dataset}'#'0.25_320_320_val'\n",
    "    image_dir_val=[image_dir_val_, ]\n",
    "    \n",
    "    for i in range(len(image_dir)):\n",
    "        df = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir[i]) )\n",
    "\n",
    "    for i in range(len(image_dir_val)):\n",
    "        df2 = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir_val[i]) )\n",
    "    df2['img_id'] = df2['tile_id'].apply(lambda x: x.split('/')[-2])\n",
    "        \n",
    "    kf = KFold(n_splits=args.n_fold, random_state=args.seed, shuffle=True)\n",
    "    all_dice = []\n",
    "    models = SegModel()\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        if n_fold == 0:\n",
    "            continue\n",
    "        train_df = df[df['fold']!= n_fold].reset_index(drop=True)\n",
    "        val_df = df2[df2['fold']== n_fold].reset_index(drop=True).copy()\n",
    "        \n",
    "        # validation loader 3개 만들기 위함\n",
    "        unique_value = val_df['tile_id'].apply(lambda x: x.split('/')[-2]).unique() #[valid_id1, valid_id2, valid_id3 ]\n",
    "        val_img_id1 = unique_value[0] ; val_img_id2 = unique_value[1] ; val_img_id3= unique_value[2]\n",
    "        val_df1= val_df[val_df['img_id']==val_img_id1].reset_index(drop=True)\n",
    "        val_df2= val_df[val_df['img_id']==val_img_id2].reset_index(drop=True)\n",
    "        val_df3= val_df[val_df['img_id']==val_img_id3].reset_index(drop=True)\n",
    "        #####################################################\n",
    "        train_dataset = HuDataset(\n",
    "            df = train_df,\n",
    "            augment = train_augment\n",
    "        )\n",
    "        train_loader  = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 8,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader1\n",
    "        valid_dataset1 = HuDataset(\n",
    "            df = val_df1\n",
    "            ,\n",
    "        )\n",
    "        valid_loader1 = DataLoader(\n",
    "            valid_dataset1,\n",
    "            sampler = SequentialSampler(valid_dataset1),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader2\n",
    "        valid_dataset2 = HuDataset(\n",
    "            df = val_df2\n",
    "            ,\n",
    "        )\n",
    "        \n",
    "        valid_loader2 = DataLoader(\n",
    "            valid_dataset2,\n",
    "            sampler = SequentialSampler(valid_dataset2),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader3\n",
    "        valid_dataset3 = HuDataset(\n",
    "            df = val_df3\n",
    "            ,\n",
    "        )\n",
    "        valid_loader3 = DataLoader(\n",
    "            valid_dataset3,\n",
    "            sampler = SequentialSampler(valid_dataset3),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        log.write('fold = %s\\n'%str(n_fold))\n",
    "        log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "        log.write('valid_dataset1 : \\n%s\\n'%(valid_dataset1))\n",
    "        log.write('valid_dataset2 : \\n%s\\n'%(valid_dataset2))\n",
    "        log.write('valid_dataset3 : \\n%s\\n'%(valid_dataset3))\n",
    "        log.write('\\n')\n",
    "\n",
    "        # ------------------------\n",
    "        #  Model\n",
    "        # ------------------------\n",
    "        log.write('** net setting **\\n')\n",
    "\n",
    "        scaler = GradScaler()\n",
    "        models = SegModel()\n",
    "        \n",
    "        net = models[n_fold]\n",
    "        net = net.to(device)\n",
    "        if args.multi_gpu:\n",
    "            log.write('multi gpu')\n",
    "            net = nn.DataParallel(net)\n",
    "        \n",
    "        \n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        if args.opt =='adamw':\n",
    "            optimizer = torch.optim.AdamW(net.parameters(), lr = args.start_lr)\n",
    "\n",
    "        elif args.opt =='radam_look':\n",
    "            optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=args.start_lr), alpha=0.5, k=5)\n",
    "        if optimizer == None:\n",
    "            assert False, 'no have optimizer'\n",
    "        \n",
    "        # ------------------------\n",
    "        #  scheduler\n",
    "        # ------------------------\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "\n",
    "        log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "        #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "        log.write('\\n')\n",
    "\n",
    "        ## start training here! ##############################################\n",
    "        #array([0.57142857, 0.42857143])\n",
    "        log.write('** start training here! **\\n')\n",
    "        log.write('   is_mixed_precision = %s \\n'%str(args.amp))\n",
    "        log.write('   batch_size = %d \\n'%(args.batch_size))\n",
    "        log.write('             |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n",
    "        log.write('rate  epoch  | dice   loss   tp     tn     | loss           | time           \\n')\n",
    "        log.write('-------------------------------------------------------------------------------------\\n')\n",
    "                  #0.00100   0.50  0.80 | 0.891  0.020  0.000  0.000  | 0.000  0.000   |  0 hr 02 min\n",
    "\n",
    "        def message(mode='print'):\n",
    "            if mode==('print'):\n",
    "                asterisk = ' '\n",
    "                loss = batch_loss\n",
    "            if mode==('log'):\n",
    "                asterisk = '*'\n",
    "                loss = train_loss\n",
    "\n",
    "            text = \\\n",
    "                '%0.5f  %s%s    | '%(rate, epoch, asterisk,) +\\\n",
    "                '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n",
    "                '%4.3f  %4.3f   | '%(*loss,) +\\\n",
    "                '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "\n",
    "            return text\n",
    "\n",
    "        #----\n",
    "        valid_loss = np.zeros(4,np.float32)\n",
    "        train_loss = np.zeros(2,np.float32)\n",
    "        batch_loss = np.zeros_like(train_loss)\n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = 0\n",
    "        loss = torch.FloatTensor([0]).sum()\n",
    "\n",
    "\n",
    "        start_timer = timer()\n",
    "        rate = 0\n",
    "        best_dice = 0\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            #print('\\r',end='',flush=True)\n",
    "            #log.write(message(mode='log')+'\\n')\n",
    "            # training\n",
    "            for t, batch in enumerate(train_loader):\n",
    "\n",
    "                # learning rate schduler -------------\n",
    "                #adjust_learning_rate(optimizer, schduler(iteration))\n",
    "                rate = get_learning_rate(optimizer)\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                batch_size = len(batch['index'])\n",
    "                net.train()\n",
    "\n",
    "                if args.amp:\n",
    "                    #image = image.half()\n",
    "                    with autocast():\n",
    "                        mask  = batch['mask'].to(device)\n",
    "                        image = batch['image'].to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        #logit = data_parallel(net, image)\n",
    "                        if args.clf_head:\n",
    "                            logit, logit2 = net(image) # seg logit, clf logit\n",
    "                        else:\n",
    "                            logit = net(image)\n",
    "                        if args.loss == 'bce':\n",
    "                            if args.label_smoothing:\n",
    "                                loss = LabelSmoothing()(logit, mask)\n",
    "                            else:\n",
    "                                loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                            if args.clf_head:\n",
    "                                loss += args.clf_alpha *nn.BCEWithLogitsLoss()(logit2, (mask.sum(dim=(2,3))>0).float() )\n",
    "                        elif args.loss =='lovasz':\n",
    "                            #loss = LovaszHingeLoss()(logit, mask)\n",
    "                            loss = symmetric_lovasz(logit, mask)\n",
    "                            \n",
    "                        elif args.loss == 'bce_dice':\n",
    "                            loss = DiceBCELoss()(logit, mask)\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else :\n",
    "                    mask  = batch['mask'].to(device)\n",
    "                    image = batch['image'].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    #logit = data_parallel(net, image)\n",
    "                    logit = net(image)\n",
    "                    if args.loss == 'bce':\n",
    "                        loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                    elif args.loss =='lovasz':\n",
    "                        loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # print statistics  --------\n",
    "\n",
    "                batch_loss = np.array([ loss.item(), 0 ])\n",
    "                sum_train_loss += batch_loss\n",
    "                sum_train += 1\n",
    "\n",
    "                #print('\\r',end='',flush=True)\n",
    "                #print(message(mode='print'), end='',flush=True)\n",
    "            \n",
    "\n",
    "            # train loss\n",
    "            train_loss = sum_train_loss/(sum_train+1e-12)\n",
    "            sum_train_loss[...] = 0\n",
    "            sum_train = 0\n",
    "            print(\"do valid...\")\n",
    "            # scheudler\n",
    "            valid_loss1 = do_valid(net, valid_loader1) #\n",
    "            valid_loss2 = do_valid(net, valid_loader2)\n",
    "            valid_loss3 = do_valid(net, valid_loader3)\n",
    "            valid_loss = (np.array(valid_loss1) + np.array(valid_loss2) + np.array(valid_loss3))/3\n",
    "            \n",
    "            log.write(message(mode='log')+'\\n')\n",
    "            log.write(f'{val_img_id1} dice : {valid_loss1[0]:.5f}, {val_img_id2} dice : {valid_loss2[0]:.5f}, {val_img_id3} dice : {valid_loss3[0]:.5f}\\n')\n",
    "            \n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(valid_loss[0])\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # saved models\n",
    "            #if valid_loss[0] > best_dice:\n",
    "            if valid_loss[0] > best_dice:\n",
    "                best_dice = valid_loss[0]\n",
    "                log.write(f'\\n saved best models, dice:{best_dice:.5f}\\n')\n",
    "                torch.save({\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                }, out_dir + f'/checkpoint/{n_fold}fold_{epoch}epoch_{best_dice:.4f}_{args.encoders[n_fold]}_{args.decoders[n_fold]}model.pth')\n",
    "            \n",
    "            log.write('='*80+'\\n')\n",
    "\n",
    "        log.write('\\n')\n",
    "        \n",
    "        all_dice.append(best_dice)\n",
    "    \n",
    "    print(f'all dice score : {sum(all_dice)/len(all_dice) : .4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-lexington",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__module__       : __main__\n",
      "amp              : True\n",
      "gpu              : 0, 1\n",
      "encoder          : b4\n",
      "decoder          : unet\n",
      "diff_arch        : True\n",
      "encoders         : ['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']\n",
      "decoders         : ['unet', 'unet', 'unet', 'unet', 'unet']\n",
      "batch_size       : 8\n",
      "weight_decay     : 1e-06\n",
      "epochs           : 50\n",
      "n_fold           : 5\n",
      "fold             : 0\n",
      "all_fold_train   : True\n",
      "image_size       : 1024\n",
      "crop_size        : 1024\n",
      "tile_size        : 1280\n",
      "tile_step        : 640\n",
      "tile_scale       : 1\n",
      "dataset          : 1_1280_640_train_fold\n",
      "val_dataset      : 1_1280_1280_val_fold\n",
      "dir              : 50_['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']_['unet', 'unet', 'unet', 'unet', 'unet']_1024_1280_640_1\n",
      "T_max            : 10\n",
      "opt              : radam_look\n",
      "scheduler        : CosineAnnealingLR\n",
      "loss             : bce\n",
      "factor           : 0.4\n",
      "patience         : 3\n",
      "eps              : 1e-06\n",
      "decay_epoch      : [4, 8, 12]\n",
      "T_0              : 4\n",
      "start_lr         : 0.001\n",
      "min_lr           : 1e-06\n",
      "clf_head         : False\n",
      "label_smoothing  : False\n",
      "multi_gpu        : True\n",
      "clf_alpha        : 0.3\n",
      "smoothing        : 0.1\n",
      "dice_smoothing   : 1\n",
      "num_workers      : 8\n",
      "seed             : 42\n",
      "__dict__         : <attribute '__dict__' of 'args' objects>\n",
      "__weakref__      : <attribute '__weakref__' of 'args' objects>\n",
      "__doc__          : None\n",
      "\tout_dir  = /home/jeonghokim/competition/HubMap/data//result/50_['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']_['unet', 'unet', 'unet', 'unet', 'unet']_1024_1280_640_1_b4_1024\n",
      "\n",
      "** dataset setting **\n",
      "fold = 1\n",
      "train_dataset : \n",
      "\tlen  = 16712\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 72\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 440\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 714\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 8 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.864  0.027  0.772  0.999  | 0.098  0.000   |  0 hr 23 min\n",
      "aaa6a05cc dice : 0.82231, cb2d976f4 dice : 0.88388, 4ef6695ce dice : 0.88525\n",
      "\n",
      " saved best models, dice:0.86381\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.904  0.018  0.927  0.995  | 0.017  0.000   |  0 hr 45 min\n",
      "aaa6a05cc dice : 0.88081, cb2d976f4 dice : 0.90529, 4ef6695ce dice : 0.92475\n",
      "\n",
      " saved best models, dice:0.90362\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.911  0.017  0.919  0.996  | 0.016  0.000   |  1 hr 08 min\n",
      "aaa6a05cc dice : 0.90168, cb2d976f4 dice : 0.90475, 4ef6695ce dice : 0.92717\n",
      "\n",
      " saved best models, dice:0.91120\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.911  0.017  0.865  0.999  | 0.014  0.000   |  1 hr 31 min\n",
      "aaa6a05cc dice : 0.87796, cb2d976f4 dice : 0.93345, 4ef6695ce dice : 0.92192\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.725  0.034  0.594  1.000  | 0.012  0.000   |  1 hr 53 min\n",
      "aaa6a05cc dice : 0.51119, cb2d976f4 dice : 0.84098, 4ef6695ce dice : 0.82167\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.932  0.012  0.924  0.998  | 0.012  0.000   |  2 hr 16 min\n",
      "aaa6a05cc dice : 0.91929, cb2d976f4 dice : 0.94166, 4ef6695ce dice : 0.93616\n",
      "\n",
      " saved best models, dice:0.93237\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.929  0.013  0.909  0.998  | 0.011  0.000   |  2 hr 38 min\n",
      "aaa6a05cc dice : 0.91382, cb2d976f4 dice : 0.94392, 4ef6695ce dice : 0.92952\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.931  0.012  0.917  0.998  | 0.010  0.000   |  3 hr 01 min\n",
      "aaa6a05cc dice : 0.91643, cb2d976f4 dice : 0.94370, 4ef6695ce dice : 0.93397\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.934  0.012  0.921  0.998  | 0.009  0.000   |  3 hr 24 min\n",
      "aaa6a05cc dice : 0.91877, cb2d976f4 dice : 0.94764, 4ef6695ce dice : 0.93524\n",
      "\n",
      " saved best models, dice:0.93388\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.933  0.012  0.917  0.998  | 0.009  0.000   |  3 hr 46 min\n",
      "aaa6a05cc dice : 0.91797, cb2d976f4 dice : 0.94748, 4ef6695ce dice : 0.93210\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.934  0.011  0.924  0.998  | 0.009  0.000   |  4 hr 09 min\n",
      "aaa6a05cc dice : 0.91873, cb2d976f4 dice : 0.94852, 4ef6695ce dice : 0.93413\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.932  0.012  0.913  0.998  | 0.009  0.000   |  4 hr 31 min\n",
      "aaa6a05cc dice : 0.91900, cb2d976f4 dice : 0.94721, 4ef6695ce dice : 0.93128\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.934  0.012  0.925  0.998  | 0.009  0.000   |  4 hr 54 min\n",
      "aaa6a05cc dice : 0.91909, cb2d976f4 dice : 0.94677, 4ef6695ce dice : 0.93705\n",
      "\n",
      " saved best models, dice:0.93430\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.935  0.012  0.922  0.998  | 0.010  0.000   |  5 hr 16 min\n",
      "aaa6a05cc dice : 0.92295, cb2d976f4 dice : 0.94455, 4ef6695ce dice : 0.93745\n",
      "\n",
      " saved best models, dice:0.93498\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.932  0.012  0.926  0.998  | 0.010  0.000   |  5 hr 39 min\n",
      "aaa6a05cc dice : 0.91613, cb2d976f4 dice : 0.94377, 4ef6695ce dice : 0.93625\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  16*    | 0.933  0.012  0.934  0.998  | 0.010  0.000   |  6 hr 01 min\n",
      "aaa6a05cc dice : 0.91920, cb2d976f4 dice : 0.94034, 4ef6695ce dice : 0.93818\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  17*    | 0.928  0.013  0.914  0.998  | 0.011  0.000   |  6 hr 24 min\n",
      "aaa6a05cc dice : 0.91197, cb2d976f4 dice : 0.94444, 4ef6695ce dice : 0.92759\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  18*    | 0.928  0.013  0.909  0.998  | 0.011  0.000   |  6 hr 46 min\n",
      "aaa6a05cc dice : 0.91783, cb2d976f4 dice : 0.93964, 4ef6695ce dice : 0.92664\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  19*    | 0.926  0.013  0.900  0.998  | 0.012  0.000   |  7 hr 09 min\n",
      "aaa6a05cc dice : 0.90374, cb2d976f4 dice : 0.94693, 4ef6695ce dice : 0.92653\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  20*    | 0.926  0.013  0.930  0.997  | 0.012  0.000   |  7 hr 32 min\n",
      "aaa6a05cc dice : 0.91007, cb2d976f4 dice : 0.93594, 4ef6695ce dice : 0.93274\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00100  21*    | 0.915  0.015  0.874  0.999  | 0.012  0.000   |  7 hr 54 min\n",
      "aaa6a05cc dice : 0.89639, cb2d976f4 dice : 0.93133, 4ef6695ce dice : 0.91852\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  22*    | 0.925  0.014  0.900  0.998  | 0.011  0.000   |  8 hr 17 min\n",
      "aaa6a05cc dice : 0.90300, cb2d976f4 dice : 0.94217, 4ef6695ce dice : 0.92861\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  23*    | 0.927  0.013  0.908  0.998  | 0.011  0.000   |  8 hr 40 min\n",
      "aaa6a05cc dice : 0.90958, cb2d976f4 dice : 0.94273, 4ef6695ce dice : 0.92968\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do valid...\n",
      "0.00079  24*    | 0.930  0.013  0.912  0.998  | 0.011  0.000   |  9 hr 03 min\n",
      "aaa6a05cc dice : 0.91418, cb2d976f4 dice : 0.94473, 4ef6695ce dice : 0.93234\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  25*    | 0.932  0.013  0.908  0.999  | 0.010  0.000   |  9 hr 25 min\n",
      "aaa6a05cc dice : 0.91656, cb2d976f4 dice : 0.94659, 4ef6695ce dice : 0.93184\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  26*    | 0.929  0.012  0.906  0.998  | 0.009  0.000   |  9 hr 47 min\n",
      "aaa6a05cc dice : 0.91070, cb2d976f4 dice : 0.94613, 4ef6695ce dice : 0.93029\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  27*    | 0.932  0.012  0.911  0.998  | 0.009  0.000   | 10 hr 10 min\n",
      "aaa6a05cc dice : 0.91677, cb2d976f4 dice : 0.94683, 4ef6695ce dice : 0.93094\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  28*    | 0.933  0.012  0.915  0.998  | 0.009  0.000   | 10 hr 32 min\n",
      "aaa6a05cc dice : 0.92076, cb2d976f4 dice : 0.94736, 4ef6695ce dice : 0.93177\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  29*    | 0.935  0.012  0.925  0.998  | 0.008  0.000   | 10 hr 55 min\n",
      "aaa6a05cc dice : 0.92138, cb2d976f4 dice : 0.94718, 4ef6695ce dice : 0.93502\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  30*    | 0.935  0.012  0.921  0.998  | 0.008  0.000   | 11 hr 17 min\n",
      "aaa6a05cc dice : 0.92136, cb2d976f4 dice : 0.94798, 4ef6695ce dice : 0.93426\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  31*    | 0.933  0.012  0.916  0.998  | 0.008  0.000   | 11 hr 40 min\n",
      "aaa6a05cc dice : 0.92019, cb2d976f4 dice : 0.94730, 4ef6695ce dice : 0.93247\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  32*    | 0.934  0.012  0.916  0.998  | 0.008  0.000   | 12 hr 02 min\n",
      "aaa6a05cc dice : 0.92065, cb2d976f4 dice : 0.94781, 4ef6695ce dice : 0.93333\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  33*    | 0.929  0.013  0.904  0.998  | 0.008  0.000   | 12 hr 25 min\n",
      "aaa6a05cc dice : 0.90846, cb2d976f4 dice : 0.94529, 4ef6695ce dice : 0.93193\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  34*    | 0.933  0.013  0.915  0.998  | 0.008  0.000   | 12 hr 47 min\n",
      "aaa6a05cc dice : 0.92024, cb2d976f4 dice : 0.94662, 4ef6695ce dice : 0.93217\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  35*    | 0.933  0.013  0.918  0.998  | 0.008  0.000   | 13 hr 10 min\n",
      "aaa6a05cc dice : 0.92161, cb2d976f4 dice : 0.94698, 4ef6695ce dice : 0.92910\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  36*    | 0.932  0.012  0.926  0.998  | 0.009  0.000   | 13 hr 32 min\n",
      "aaa6a05cc dice : 0.92102, cb2d976f4 dice : 0.94108, 4ef6695ce dice : 0.93449\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  37*    | 0.929  0.014  0.906  0.998  | 0.009  0.000   | 13 hr 54 min\n",
      "aaa6a05cc dice : 0.91218, cb2d976f4 dice : 0.94407, 4ef6695ce dice : 0.93159\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  38*    | 0.930  0.013  0.929  0.998  | 0.009  0.000   | 14 hr 17 min\n",
      "aaa6a05cc dice : 0.91093, cb2d976f4 dice : 0.94486, 4ef6695ce dice : 0.93543\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  39*    | 0.925  0.014  0.896  0.999  | 0.010  0.000   | 14 hr 39 min\n",
      "aaa6a05cc dice : 0.90552, cb2d976f4 dice : 0.94436, 4ef6695ce dice : 0.92661\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  40*    | 0.932  0.013  0.915  0.998  | 0.010  0.000   | 15 hr 02 min\n",
      "aaa6a05cc dice : 0.91832, cb2d976f4 dice : 0.94693, 4ef6695ce dice : 0.93147\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00100  41*    | 0.931  0.012  0.915  0.998  | 0.010  0.000   | 15 hr 24 min\n",
      "aaa6a05cc dice : 0.91641, cb2d976f4 dice : 0.93856, 4ef6695ce dice : 0.93660\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  42*    | 0.929  0.013  0.915  0.998  | 0.009  0.000   | 15 hr 47 min\n",
      "aaa6a05cc dice : 0.91928, cb2d976f4 dice : 0.93976, 4ef6695ce dice : 0.92856\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  43*    | 0.926  0.013  0.898  0.998  | 0.009  0.000   | 16 hr 09 min\n",
      "aaa6a05cc dice : 0.91230, cb2d976f4 dice : 0.93797, 4ef6695ce dice : 0.92647\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  44*    | 0.926  0.014  0.900  0.998  | 0.009  0.000   | 16 hr 32 min\n",
      "aaa6a05cc dice : 0.90545, cb2d976f4 dice : 0.94434, 4ef6695ce dice : 0.92733\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  45*    | 0.920  0.015  0.884  0.999  | 0.009  0.000   | 16 hr 55 min\n",
      "aaa6a05cc dice : 0.89395, cb2d976f4 dice : 0.94010, 4ef6695ce dice : 0.92637\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  46*    | 0.923  0.014  0.891  0.999  | 0.008  0.000   | 17 hr 17 min\n",
      "aaa6a05cc dice : 0.90473, cb2d976f4 dice : 0.93886, 4ef6695ce dice : 0.92631\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  47*    | 0.931  0.013  0.918  0.998  | 0.008  0.000   | 17 hr 40 min\n",
      "aaa6a05cc dice : 0.91665, cb2d976f4 dice : 0.94356, 4ef6695ce dice : 0.93199\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  48*    | 0.933  0.012  0.921  0.998  | 0.007  0.000   | 18 hr 02 min\n",
      "aaa6a05cc dice : 0.92069, cb2d976f4 dice : 0.94410, 4ef6695ce dice : 0.93385\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  49*    | 0.931  0.013  0.915  0.998  | 0.007  0.000   | 18 hr 25 min\n",
      "aaa6a05cc dice : 0.91690, cb2d976f4 dice : 0.94451, 4ef6695ce dice : 0.93201\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  50*    | 0.929  0.013  0.906  0.998  | 0.007  0.000   | 18 hr 48 min\n",
      "aaa6a05cc dice : 0.91698, cb2d976f4 dice : 0.94244, 4ef6695ce dice : 0.92879\n",
      "================================================================================\n",
      "\n",
      "fold = 2\n",
      "train_dataset : \n",
      "\tlen  = 18063\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 166\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 378\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 334\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 8 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.912  0.025  0.863  0.999  | 0.073  0.000   |  0 hr 23 min\n",
      "e79de561c dice : 0.90621, 095bf7a1f dice : 0.90916, 1e2425f28 dice : 0.92060\n",
      "\n",
      " saved best models, dice:0.91199\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.934  0.016  0.929  0.997  | 0.017  0.000   |  0 hr 47 min\n",
      "e79de561c dice : 0.93793, 095bf7a1f dice : 0.93195, 1e2425f28 dice : 0.93171\n",
      "\n",
      " saved best models, dice:0.93387\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.920  0.020  0.881  0.998  | 0.014  0.000   |  1 hr 11 min\n",
      "e79de561c dice : 0.91668, 095bf7a1f dice : 0.92116, 1e2425f28 dice : 0.92332\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.934  0.017  0.929  0.997  | 0.013  0.000   |  1 hr 34 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e79de561c dice : 0.93570, 095bf7a1f dice : 0.93395, 1e2425f28 dice : 0.93364\n",
      "\n",
      " saved best models, dice:0.93443\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.919  0.021  0.886  0.998  | 0.012  0.000   |  1 hr 58 min\n",
      "e79de561c dice : 0.89857, 095bf7a1f dice : 0.92272, 1e2425f28 dice : 0.93515\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.935  0.016  0.933  0.997  | 0.011  0.000   |  2 hr 22 min\n",
      "e79de561c dice : 0.93548, 095bf7a1f dice : 0.93752, 1e2425f28 dice : 0.93285\n",
      "\n",
      " saved best models, dice:0.93528\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.940  0.015  0.947  0.997  | 0.010  0.000   |  3 hr 34 min\n",
      "e79de561c dice : 0.94295, 095bf7a1f dice : 0.93841, 1e2425f28 dice : 0.93870\n",
      "\n",
      " saved best models, dice:0.94002\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.938  0.016  0.925  0.998  | 0.009  0.000   |  3 hr 58 min\n",
      "e79de561c dice : 0.93822, 095bf7a1f dice : 0.93744, 1e2425f28 dice : 0.93750\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.937  0.016  0.922  0.998  | 0.009  0.000   |  4 hr 21 min\n",
      "e79de561c dice : 0.93477, 095bf7a1f dice : 0.93795, 1e2425f28 dice : 0.93869\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.939  0.015  0.933  0.997  | 0.009  0.000   |  4 hr 45 min\n",
      "e79de561c dice : 0.93702, 095bf7a1f dice : 0.93966, 1e2425f28 dice : 0.93923\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.938  0.016  0.926  0.998  | 0.008  0.000   |  5 hr 09 min\n",
      "e79de561c dice : 0.93504, 095bf7a1f dice : 0.93843, 1e2425f28 dice : 0.93907\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.936  0.016  0.922  0.998  | 0.008  0.000   |  5 hr 32 min\n",
      "e79de561c dice : 0.93317, 095bf7a1f dice : 0.93717, 1e2425f28 dice : 0.93909\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.941  0.014  0.943  0.997  | 0.008  0.000   |  5 hr 56 min\n",
      "e79de561c dice : 0.94317, 095bf7a1f dice : 0.94014, 1e2425f28 dice : 0.93830\n",
      "\n",
      " saved best models, dice:0.94054\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.934  0.016  0.917  0.998  | 0.009  0.000   |  6 hr 20 min\n",
      "e79de561c dice : 0.92887, 095bf7a1f dice : 0.93455, 1e2425f28 dice : 0.93899\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.870  0.027  0.786  0.999  | 0.009  0.000   |  6 hr 43 min\n",
      "e79de561c dice : 0.85101, 095bf7a1f dice : 0.87495, 1e2425f28 dice : 0.88433\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  16*    | 0.920  0.018  0.876  0.999  | 0.009  0.000   |  7 hr 07 min\n",
      "e79de561c dice : 0.90834, 095bf7a1f dice : 0.92225, 1e2425f28 dice : 0.92990\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  17*    | 0.938  0.016  0.948  0.996  | 0.010  0.000   |  7 hr 31 min\n",
      "e79de561c dice : 0.94080, 095bf7a1f dice : 0.93900, 1e2425f28 dice : 0.93411\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  18*    | 0.917  0.020  0.898  0.997  | 0.011  0.000   |  7 hr 54 min\n",
      "e79de561c dice : 0.91896, 095bf7a1f dice : 0.92538, 1e2425f28 dice : 0.90577\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  19*    | 0.936  0.016  0.923  0.998  | 0.011  0.000   |  8 hr 18 min\n",
      "e79de561c dice : 0.93404, 095bf7a1f dice : 0.93612, 1e2425f28 dice : 0.93783\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  20*    | 0.934  0.016  0.924  0.997  | 0.011  0.000   |  8 hr 42 min\n",
      "e79de561c dice : 0.92821, 095bf7a1f dice : 0.93580, 1e2425f28 dice : 0.93692\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00100  21*    | 0.858  0.030  0.780  0.998  | 0.011  0.000   |  9 hr 05 min\n",
      "e79de561c dice : 0.83391, 095bf7a1f dice : 0.88323, 1e2425f28 dice : 0.85684\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  22*    | 0.934  0.016  0.916  0.998  | 0.011  0.000   |  9 hr 29 min\n",
      "e79de561c dice : 0.93452, 095bf7a1f dice : 0.93254, 1e2425f28 dice : 0.93603\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  23*    | 0.927  0.018  0.897  0.998  | 0.010  0.000   |  9 hr 53 min\n",
      "e79de561c dice : 0.91846, 095bf7a1f dice : 0.92967, 1e2425f28 dice : 0.93258\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  24*    | 0.935  0.016  0.943  0.996  | 0.010  0.000   | 10 hr 17 min\n",
      "e79de561c dice : 0.93297, 095bf7a1f dice : 0.93758, 1e2425f28 dice : 0.93473\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  25*    | 0.939  0.015  0.938  0.997  | 0.009  0.000   | 10 hr 40 min\n",
      "e79de561c dice : 0.93921, 095bf7a1f dice : 0.93715, 1e2425f28 dice : 0.93951\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  26*    | 0.931  0.017  0.905  0.998  | 0.009  0.000   | 11 hr 04 min\n",
      "e79de561c dice : 0.92375, 095bf7a1f dice : 0.93061, 1e2425f28 dice : 0.93764\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  27*    | 0.937  0.016  0.921  0.998  | 0.008  0.000   | 11 hr 28 min\n",
      "e79de561c dice : 0.93349, 095bf7a1f dice : 0.93718, 1e2425f28 dice : 0.93907\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  28*    | 0.938  0.015  0.931  0.997  | 0.008  0.000   | 11 hr 51 min\n",
      "e79de561c dice : 0.93368, 095bf7a1f dice : 0.94094, 1e2425f28 dice : 0.93979\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  29*    | 0.937  0.016  0.928  0.997  | 0.007  0.000   | 12 hr 15 min\n",
      "e79de561c dice : 0.93022, 095bf7a1f dice : 0.93942, 1e2425f28 dice : 0.94006\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  30*    | 0.936  0.016  0.927  0.998  | 0.007  0.000   | 12 hr 39 min\n",
      "e79de561c dice : 0.93052, 095bf7a1f dice : 0.93970, 1e2425f28 dice : 0.93896\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  31*    | 0.936  0.016  0.928  0.998  | 0.007  0.000   | 13 hr 02 min\n",
      "e79de561c dice : 0.93014, 095bf7a1f dice : 0.93960, 1e2425f28 dice : 0.93966\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  32*    | 0.937  0.016  0.932  0.997  | 0.007  0.000   | 13 hr 26 min\n",
      "e79de561c dice : 0.93189, 095bf7a1f dice : 0.94025, 1e2425f28 dice : 0.93928\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  33*    | 0.936  0.016  0.929  0.997  | 0.007  0.000   | 13 hr 50 min\n",
      "e79de561c dice : 0.92901, 095bf7a1f dice : 0.94066, 1e2425f28 dice : 0.93928\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  34*    | 0.931  0.018  0.905  0.998  | 0.008  0.000   | 14 hr 13 min\n",
      "e79de561c dice : 0.91988, 095bf7a1f dice : 0.93270, 1e2425f28 dice : 0.93996\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  35*    | 0.935  0.016  0.919  0.998  | 0.008  0.000   | 14 hr 37 min\n",
      "e79de561c dice : 0.92471, 095bf7a1f dice : 0.93770, 1e2425f28 dice : 0.94210\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  36*    | 0.932  0.017  0.917  0.998  | 0.008  0.000   | 15 hr 00 min\n",
      "e79de561c dice : 0.91974, 095bf7a1f dice : 0.93766, 1e2425f28 dice : 0.93797\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do valid...\n",
      "0.00065  37*    | 0.924  0.019  0.905  0.998  | 0.008  0.000   | 15 hr 24 min\n",
      "e79de561c dice : 0.89760, 095bf7a1f dice : 0.93663, 1e2425f28 dice : 0.93891\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  38*    | 0.930  0.017  0.914  0.998  | 0.009  0.000   | 15 hr 48 min\n",
      "e79de561c dice : 0.91427, 095bf7a1f dice : 0.93621, 1e2425f28 dice : 0.93935\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  39*    | 0.933  0.016  0.918  0.998  | 0.009  0.000   | 16 hr 11 min\n",
      "e79de561c dice : 0.92552, 095bf7a1f dice : 0.93636, 1e2425f28 dice : 0.93827\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  40*    | 0.931  0.017  0.922  0.997  | 0.009  0.000   | 16 hr 35 min\n",
      "e79de561c dice : 0.91945, 095bf7a1f dice : 0.93886, 1e2425f28 dice : 0.93535\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00100  41*    | 0.932  0.017  0.918  0.998  | 0.010  0.000   | 16 hr 58 min\n",
      "e79de561c dice : 0.92138, 095bf7a1f dice : 0.93460, 1e2425f28 dice : 0.93929\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  42*    | 0.934  0.017  0.934  0.997  | 0.009  0.000   | 17 hr 22 min\n",
      "e79de561c dice : 0.92899, 095bf7a1f dice : 0.93707, 1e2425f28 dice : 0.93542\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  43*    | 0.918  0.019  0.891  0.998  | 0.009  0.000   | 17 hr 45 min\n",
      "e79de561c dice : 0.89931, 095bf7a1f dice : 0.92350, 1e2425f28 dice : 0.93125\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  44*    | 0.928  0.018  0.905  0.998  | 0.009  0.000   | 18 hr 09 min\n",
      "e79de561c dice : 0.90945, 095bf7a1f dice : 0.93550, 1e2425f28 dice : 0.93889\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  45*    | 0.898  0.025  0.844  0.999  | 0.008  0.000   | 18 hr 32 min\n",
      "e79de561c dice : 0.83682, 095bf7a1f dice : 0.92551, 1e2425f28 dice : 0.93227\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  46*    | 0.931  0.017  0.922  0.997  | 0.008  0.000   | 18 hr 56 min\n",
      "e79de561c dice : 0.91644, 095bf7a1f dice : 0.93469, 1e2425f28 dice : 0.94094\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  47*    | 0.931  0.017  0.913  0.998  | 0.007  0.000   | 19 hr 19 min\n",
      "e79de561c dice : 0.91895, 095bf7a1f dice : 0.93509, 1e2425f28 dice : 0.93923\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  48*    | 0.934  0.016  0.925  0.997  | 0.007  0.000   | 19 hr 43 min\n",
      "e79de561c dice : 0.92790, 095bf7a1f dice : 0.93692, 1e2425f28 dice : 0.93780\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  49*    | 0.934  0.016  0.927  0.997  | 0.007  0.000   | 20 hr 06 min\n",
      "e79de561c dice : 0.92531, 095bf7a1f dice : 0.93793, 1e2425f28 dice : 0.93810\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  50*    | 0.926  0.018  0.903  0.998  | 0.007  0.000   | 20 hr 29 min\n",
      "e79de561c dice : 0.90740, 095bf7a1f dice : 0.93412, 1e2425f28 dice : 0.93741\n",
      "================================================================================\n",
      "\n",
      "fold = 3\n",
      "train_dataset : \n",
      "\tlen  = 18675\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 184\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 442\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 108\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 8 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.938  0.012  0.915  0.999  | 0.120  0.000   |  0 hr 23 min\n",
      "2f6ecfcdf dice : 0.93659, 8242609fa dice : 0.94535, b2dc8411c dice : 0.93115\n",
      "\n",
      " saved best models, dice:0.93770\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.947  0.010  0.944  0.998  | 0.017  0.000   |  0 hr 48 min\n",
      "2f6ecfcdf dice : 0.94766, 8242609fa dice : 0.94862, b2dc8411c dice : 0.94401\n",
      "\n",
      " saved best models, dice:0.94676\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.902  0.018  0.828  1.000  | 0.015  0.000   |  1 hr 12 min\n",
      "2f6ecfcdf dice : 0.89691, 8242609fa dice : 0.92253, b2dc8411c dice : 0.88633\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.944  0.010  0.926  0.999  | 0.014  0.000   |  1 hr 38 min\n",
      "2f6ecfcdf dice : 0.94231, 8242609fa dice : 0.95419, b2dc8411c dice : 0.93452\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.928  0.011  0.874  1.000  | 0.013  0.000   |  2 hr 06 min\n",
      "2f6ecfcdf dice : 0.92372, 8242609fa dice : 0.94065, b2dc8411c dice : 0.91851\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.952  0.008  0.933  0.999  | 0.012  0.000   |  2 hr 33 min\n",
      "2f6ecfcdf dice : 0.95169, 8242609fa dice : 0.95520, b2dc8411c dice : 0.94792\n",
      "\n",
      " saved best models, dice:0.95160\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.947  0.008  0.920  0.999  | 0.011  0.000   |  3 hr 00 min\n",
      "2f6ecfcdf dice : 0.94614, 8242609fa dice : 0.95233, b2dc8411c dice : 0.94224\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.950  0.008  0.927  0.999  | 0.010  0.000   |  3 hr 27 min\n",
      "2f6ecfcdf dice : 0.94957, 8242609fa dice : 0.95243, b2dc8411c dice : 0.94673\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.951  0.008  0.929  0.999  | 0.010  0.000   |  3 hr 54 min\n",
      "2f6ecfcdf dice : 0.94952, 8242609fa dice : 0.95351, b2dc8411c dice : 0.94941\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.952  0.008  0.930  0.999  | 0.009  0.000   |  4 hr 21 min\n",
      "2f6ecfcdf dice : 0.95046, 8242609fa dice : 0.95472, b2dc8411c dice : 0.94993\n",
      "\n",
      " saved best models, dice:0.95170\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.951  0.008  0.928  0.999  | 0.009  0.000   |  4 hr 48 min\n",
      "2f6ecfcdf dice : 0.95014, 8242609fa dice : 0.95418, b2dc8411c dice : 0.94876\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.952  0.008  0.930  0.999  | 0.009  0.000   |  5 hr 15 min\n",
      "2f6ecfcdf dice : 0.95062, 8242609fa dice : 0.95513, b2dc8411c dice : 0.94906\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.953  0.008  0.939  0.999  | 0.009  0.000   |  5 hr 42 min\n",
      "2f6ecfcdf dice : 0.95154, 8242609fa dice : 0.95561, b2dc8411c dice : 0.95201\n",
      "\n",
      " saved best models, dice:0.95305\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.946  0.009  0.912  0.999  | 0.009  0.000   |  6 hr 08 min\n",
      "2f6ecfcdf dice : 0.94506, 8242609fa dice : 0.95113, b2dc8411c dice : 0.94126\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.949  0.008  0.921  0.999  | 0.010  0.000   |  6 hr 35 min\n",
      "2f6ecfcdf dice : 0.95012, 8242609fa dice : 0.95389, b2dc8411c dice : 0.94174\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  16*    | 0.949  0.009  0.927  0.999  | 0.010  0.000   |  7 hr 02 min\n",
      "2f6ecfcdf dice : 0.94682, 8242609fa dice : 0.95468, b2dc8411c dice : 0.94486\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do valid...\n",
      "0.00065  17*    | 0.952  0.008  0.934  0.999  | 0.011  0.000   |  7 hr 29 min\n",
      "2f6ecfcdf dice : 0.95171, 8242609fa dice : 0.95641, b2dc8411c dice : 0.94779\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  18*    | 0.953  0.008  0.940  0.999  | 0.012  0.000   |  7 hr 55 min\n",
      "2f6ecfcdf dice : 0.94828, 8242609fa dice : 0.95611, b2dc8411c dice : 0.95426\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  19*    | 0.953  0.008  0.939  0.999  | 0.011  0.000   |  8 hr 22 min\n",
      "2f6ecfcdf dice : 0.95136, 8242609fa dice : 0.95752, b2dc8411c dice : 0.94907\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  20*    | 0.923  0.012  0.867  1.000  | 0.012  0.000   |  8 hr 48 min\n",
      "2f6ecfcdf dice : 0.91991, 8242609fa dice : 0.93456, b2dc8411c dice : 0.91504\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00100  21*    | 0.941  0.010  0.903  0.999  | 0.012  0.000   |  9 hr 15 min\n",
      "2f6ecfcdf dice : 0.93986, 8242609fa dice : 0.94769, b2dc8411c dice : 0.93545\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  22*    | 0.949  0.008  0.928  0.999  | 0.012  0.000   |  9 hr 41 min\n",
      "2f6ecfcdf dice : 0.94773, 8242609fa dice : 0.95386, b2dc8411c dice : 0.94678\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  23*    | 0.953  0.008  0.958  0.998  | 0.011  0.000   | 10 hr 08 min\n",
      "2f6ecfcdf dice : 0.95208, 8242609fa dice : 0.95378, b2dc8411c dice : 0.95306\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  24*    | 0.946  0.009  0.918  0.999  | 0.011  0.000   | 10 hr 34 min\n",
      "2f6ecfcdf dice : 0.94241, 8242609fa dice : 0.95506, b2dc8411c dice : 0.94118\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  25*    | 0.951  0.008  0.942  0.999  | 0.010  0.000   | 11 hr 00 min\n",
      "2f6ecfcdf dice : 0.95090, 8242609fa dice : 0.95387, b2dc8411c dice : 0.94865\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  26*    | 0.947  0.009  0.926  0.999  | 0.009  0.000   | 11 hr 27 min\n",
      "2f6ecfcdf dice : 0.94564, 8242609fa dice : 0.94999, b2dc8411c dice : 0.94646\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  27*    | 0.949  0.008  0.926  0.999  | 0.009  0.000   | 11 hr 52 min\n",
      "2f6ecfcdf dice : 0.94841, 8242609fa dice : 0.95040, b2dc8411c dice : 0.94820\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  28*    | 0.952  0.008  0.936  0.999  | 0.009  0.000   | 12 hr 18 min\n",
      "2f6ecfcdf dice : 0.95202, 8242609fa dice : 0.95245, b2dc8411c dice : 0.95291\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  29*    | 0.952  0.008  0.933  0.999  | 0.008  0.000   | 12 hr 44 min\n",
      "2f6ecfcdf dice : 0.95131, 8242609fa dice : 0.95305, b2dc8411c dice : 0.95303\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  30*    | 0.950  0.008  0.927  0.999  | 0.008  0.000   | 13 hr 10 min\n",
      "2f6ecfcdf dice : 0.95004, 8242609fa dice : 0.95096, b2dc8411c dice : 0.94983\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  31*    | 0.951  0.008  0.928  0.999  | 0.008  0.000   | 13 hr 35 min\n",
      "2f6ecfcdf dice : 0.94994, 8242609fa dice : 0.95146, b2dc8411c dice : 0.95011\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  32*    | 0.951  0.008  0.930  0.999  | 0.008  0.000   | 14 hr 01 min\n",
      "2f6ecfcdf dice : 0.95085, 8242609fa dice : 0.95172, b2dc8411c dice : 0.95024\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  33*    | 0.949  0.009  0.923  0.999  | 0.008  0.000   | 14 hr 26 min\n",
      "2f6ecfcdf dice : 0.94892, 8242609fa dice : 0.94902, b2dc8411c dice : 0.94878\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  34*    | 0.952  0.008  0.932  0.999  | 0.008  0.000   | 14 hr 52 min\n",
      "2f6ecfcdf dice : 0.95189, 8242609fa dice : 0.95295, b2dc8411c dice : 0.95203\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  35*    | 0.952  0.008  0.938  0.999  | 0.008  0.000   | 15 hr 17 min\n",
      "2f6ecfcdf dice : 0.95108, 8242609fa dice : 0.95192, b2dc8411c dice : 0.95205\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  36*    | 0.954  0.008  0.940  0.999  | 0.009  0.000   | 15 hr 42 min\n",
      "2f6ecfcdf dice : 0.95161, 8242609fa dice : 0.95645, b2dc8411c dice : 0.95254\n",
      "\n",
      " saved best models, dice:0.95353\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  37*    | 0.946  0.009  0.919  0.999  | 0.009  0.000   | 16 hr 08 min\n",
      "2f6ecfcdf dice : 0.94533, 8242609fa dice : 0.94973, b2dc8411c dice : 0.94306\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  38*    | 0.941  0.010  0.907  0.999  | 0.010  0.000   | 16 hr 33 min\n",
      "2f6ecfcdf dice : 0.93993, 8242609fa dice : 0.94667, b2dc8411c dice : 0.93787\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  39*    | 0.950  0.009  0.927  0.999  | 0.010  0.000   | 16 hr 59 min\n",
      "2f6ecfcdf dice : 0.94685, 8242609fa dice : 0.95170, b2dc8411c dice : 0.95014\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  40*    | 0.951  0.008  0.949  0.998  | 0.010  0.000   | 17 hr 24 min\n",
      "2f6ecfcdf dice : 0.94630, 8242609fa dice : 0.95163, b2dc8411c dice : 0.95391\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00100  41*    | 0.946  0.009  0.918  0.999  | 0.010  0.000   | 17 hr 49 min\n",
      "2f6ecfcdf dice : 0.94381, 8242609fa dice : 0.95289, b2dc8411c dice : 0.94172\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  42*    | 0.946  0.009  0.919  0.999  | 0.010  0.000   | 18 hr 14 min\n",
      "2f6ecfcdf dice : 0.94366, 8242609fa dice : 0.94746, b2dc8411c dice : 0.94797\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  43*    | 0.948  0.009  0.926  0.999  | 0.010  0.000   | 18 hr 40 min\n",
      "2f6ecfcdf dice : 0.94835, 8242609fa dice : 0.94951, b2dc8411c dice : 0.94574\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  44*    | 0.951  0.008  0.941  0.999  | 0.009  0.000   | 19 hr 05 min\n",
      "2f6ecfcdf dice : 0.94930, 8242609fa dice : 0.95418, b2dc8411c dice : 0.95063\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  45*    | 0.943  0.009  0.911  0.999  | 0.009  0.000   | 19 hr 30 min\n",
      "2f6ecfcdf dice : 0.94146, 8242609fa dice : 0.94794, b2dc8411c dice : 0.94098\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  46*    | 0.946  0.009  0.921  0.999  | 0.008  0.000   | 19 hr 55 min\n",
      "2f6ecfcdf dice : 0.94770, 8242609fa dice : 0.94594, b2dc8411c dice : 0.94342\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  47*    | 0.951  0.009  0.937  0.999  | 0.008  0.000   | 20 hr 21 min\n",
      "2f6ecfcdf dice : 0.95352, 8242609fa dice : 0.95187, b2dc8411c dice : 0.94858\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  48*    | 0.939  0.010  0.904  0.999  | 0.008  0.000   | 20 hr 46 min\n",
      "2f6ecfcdf dice : 0.94094, 8242609fa dice : 0.94239, b2dc8411c dice : 0.93360\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  49*    | 0.948  0.010  0.924  0.999  | 0.007  0.000   | 21 hr 11 min\n",
      "2f6ecfcdf dice : 0.94959, 8242609fa dice : 0.94811, b2dc8411c dice : 0.94526\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do valid...\n",
      "0.00003  50*    | 0.946  0.009  0.919  0.999  | 0.007  0.000   | 21 hr 36 min\n",
      "2f6ecfcdf dice : 0.94735, 8242609fa dice : 0.94748, b2dc8411c dice : 0.94368\n",
      "================================================================================\n",
      "\n",
      "fold = 4\n",
      "train_dataset : \n",
      "\tlen  = 16816\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 168\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 467\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 551\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 8 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.903  0.016  0.866  0.998  | 0.110  0.000   |  0 hr 23 min\n",
      "54f2eec69 dice : 0.91309, 26dc41664 dice : 0.93349, c68fe75ea dice : 0.86164\n",
      "\n",
      " saved best models, dice:0.90274\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.904  0.016  0.891  0.998  | 0.017  0.000   |  0 hr 47 min\n",
      "54f2eec69 dice : 0.91963, 26dc41664 dice : 0.93256, c68fe75ea dice : 0.86012\n",
      "\n",
      " saved best models, dice:0.90411\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.918  0.014  0.905  0.998  | 0.014  0.000   |  1 hr 11 min\n",
      "54f2eec69 dice : 0.93079, 26dc41664 dice : 0.94508, c68fe75ea dice : 0.87838\n",
      "\n",
      " saved best models, dice:0.91808\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.916  0.014  0.905  0.998  | 0.013  0.000   |  1 hr 35 min\n",
      "54f2eec69 dice : 0.92580, 26dc41664 dice : 0.94804, c68fe75ea dice : 0.87458\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.916  0.012  0.917  0.998  | 0.013  0.000   |  1 hr 59 min\n",
      "54f2eec69 dice : 0.92805, 26dc41664 dice : 0.94755, c68fe75ea dice : 0.87098\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.907  0.014  0.872  0.998  | 0.011  0.000   |  2 hr 23 min\n",
      "54f2eec69 dice : 0.91671, 26dc41664 dice : 0.93541, c68fe75ea dice : 0.86910\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.910  0.017  0.886  0.998  | 0.011  0.000   |  2 hr 47 min\n",
      "54f2eec69 dice : 0.91506, 26dc41664 dice : 0.94618, c68fe75ea dice : 0.86776\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.916  0.014  0.897  0.998  | 0.010  0.000   |  3 hr 10 min\n",
      "54f2eec69 dice : 0.92658, 26dc41664 dice : 0.94672, c68fe75ea dice : 0.87529\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.914  0.014  0.891  0.998  | 0.009  0.000   |  3 hr 34 min\n",
      "54f2eec69 dice : 0.92712, 26dc41664 dice : 0.94537, c68fe75ea dice : 0.87088\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.918  0.013  0.905  0.998  | 0.009  0.000   |  3 hr 58 min\n",
      "54f2eec69 dice : 0.93127, 26dc41664 dice : 0.94928, c68fe75ea dice : 0.87473\n",
      "\n",
      " saved best models, dice:0.91843\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.919  0.013  0.911  0.998  | 0.009  0.000   |  4 hr 21 min\n",
      "54f2eec69 dice : 0.93273, 26dc41664 dice : 0.95059, c68fe75ea dice : 0.87263\n",
      "\n",
      " saved best models, dice:0.91865\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.921  0.013  0.915  0.998  | 0.009  0.000   |  4 hr 45 min\n",
      "54f2eec69 dice : 0.93370, 26dc41664 dice : 0.95067, c68fe75ea dice : 0.87944\n",
      "\n",
      " saved best models, dice:0.92127\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.920  0.013  0.913  0.998  | 0.009  0.000   |  5 hr 08 min\n",
      "54f2eec69 dice : 0.93028, 26dc41664 dice : 0.95023, c68fe75ea dice : 0.88005\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.919  0.014  0.916  0.998  | 0.009  0.000   |  5 hr 43 min\n",
      "54f2eec69 dice : 0.93038, 26dc41664 dice : 0.95080, c68fe75ea dice : 0.87631\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.917  0.014  0.903  0.998  | 0.010  0.000   |  6 hr 07 min\n",
      "54f2eec69 dice : 0.93204, 26dc41664 dice : 0.94808, c68fe75ea dice : 0.87234\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  16*    | 0.911  0.016  0.892  0.998  | 0.010  0.000   |  6 hr 29 min\n",
      "54f2eec69 dice : 0.92460, 26dc41664 dice : 0.94428, c68fe75ea dice : 0.86550\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  17*    | 0.893  0.016  0.927  0.996  | 0.011  0.000   |  6 hr 52 min\n",
      "54f2eec69 dice : 0.90292, 26dc41664 dice : 0.93486, c68fe75ea dice : 0.83982\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  18*    | 0.909  0.014  0.928  0.997  | 0.011  0.000   |  7 hr 15 min\n",
      "54f2eec69 dice : 0.92673, 26dc41664 dice : 0.94636, c68fe75ea dice : 0.85302\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  19*    | 0.912  0.015  0.900  0.998  | 0.012  0.000   |  7 hr 41 min\n",
      "54f2eec69 dice : 0.92314, 26dc41664 dice : 0.94270, c68fe75ea dice : 0.87012\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  20*    | 0.911  0.015  0.894  0.998  | 0.011  0.000   |  8 hr 03 min\n",
      "54f2eec69 dice : 0.92482, 26dc41664 dice : 0.94262, c68fe75ea dice : 0.86530\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00100  21*    | 0.904  0.016  0.869  0.998  | 0.012  0.000   |  8 hr 26 min\n",
      "54f2eec69 dice : 0.91684, 26dc41664 dice : 0.93400, c68fe75ea dice : 0.86248\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  22*    | 0.912  0.015  0.896  0.998  | 0.011  0.000   |  8 hr 49 min\n",
      "54f2eec69 dice : 0.92803, 26dc41664 dice : 0.94259, c68fe75ea dice : 0.86622\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  23*    | 0.915  0.014  0.896  0.998  | 0.011  0.000   |  9 hr 11 min\n",
      "54f2eec69 dice : 0.92713, 26dc41664 dice : 0.94166, c68fe75ea dice : 0.87659\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  24*    | 0.917  0.012  0.930  0.997  | 0.010  0.000   |  9 hr 34 min\n",
      "54f2eec69 dice : 0.93015, 26dc41664 dice : 0.94704, c68fe75ea dice : 0.87425\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  25*    | 0.916  0.014  0.900  0.998  | 0.010  0.000   |  9 hr 57 min\n",
      "54f2eec69 dice : 0.92897, 26dc41664 dice : 0.94030, c68fe75ea dice : 0.87842\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  26*    | 0.913  0.014  0.907  0.998  | 0.010  0.000   | 10 hr 21 min\n",
      "54f2eec69 dice : 0.92739, 26dc41664 dice : 0.94119, c68fe75ea dice : 0.87070\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  27*    | 0.914  0.015  0.896  0.998  | 0.009  0.000   | 10 hr 44 min\n",
      "54f2eec69 dice : 0.92373, 26dc41664 dice : 0.94423, c68fe75ea dice : 0.87281\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  28*    | 0.919  0.014  0.906  0.998  | 0.008  0.000   | 11 hr 08 min\n",
      "54f2eec69 dice : 0.92723, 26dc41664 dice : 0.94653, c68fe75ea dice : 0.88325\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  29*    | 0.920  0.014  0.912  0.998  | 0.008  0.000   | 11 hr 34 min\n",
      "54f2eec69 dice : 0.92990, 26dc41664 dice : 0.94978, c68fe75ea dice : 0.87929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "do valid...\n",
      "0.00003  30*    | 0.920  0.014  0.912  0.998  | 0.008  0.000   | 11 hr 58 min\n",
      "54f2eec69 dice : 0.93087, 26dc41664 dice : 0.94919, c68fe75ea dice : 0.88081\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  31*    | 0.920  0.014  0.912  0.998  | 0.008  0.000   | 12 hr 24 min\n",
      "54f2eec69 dice : 0.93034, 26dc41664 dice : 0.94931, c68fe75ea dice : 0.88127\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set seed\n",
    "    print('no set seed') if args.seed ==-1 else set_seeds(seed=args.seed)\n",
    "    run_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "australian-composer",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# #0.9333, deterministic하게 결정시킨것임.\n",
    "\n",
    "# b9a3865fc dice : 0.94369, 0486052bb dice : 0.95022, afa5e8098 dice : 0.90132\n",
    "# saved best models, dice:0.93174\n",
    "\n",
    "# aaa6a05cc dice : 0.90918, cb2d976f4 dice : 0.94353, 4ef6695ce dice : 0.93610\n",
    "# saved best models, dice:0.92960\n",
    "\n",
    "# e79de561c dice : 0.93229, 095bf7a1f dice : 0.93469, 1e2425f28 dice : 0.93323\n",
    "# saved best models, dice:0.93340\n",
    "\n",
    "# 2f6ecfcdf dice : 0.95216, 8242609fa dice : 0.95276, b2dc8411c dice : 0.94840\n",
    "# saved best models, dice:0.95111\n",
    " \n",
    "# 54f2eec69 dice : 0.92800, 26dc41664 dice : 0.94386, c68fe75ea dice : 0.88966\n",
    "# saved best models, dice:0.92050\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-trash",
   "metadata": {},
   "source": [
    "# Train with albumentation augment version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-typing",
   "metadata": {},
   "source": [
    "albumentation으로 augmentation 실험할 경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "significant-pacific",
   "metadata": {
    "code_folding": [
     32,
     38,
     77
    ]
   },
   "outputs": [],
   "source": [
    "# ------- image fold train version, my augment ------- #\n",
    "\n",
    "# augmentation\n",
    "def train_augment():\n",
    "\n",
    "    return A.Compose([\n",
    "            A.OneOf([\n",
    "                A.RandomCrop(args.image_size,args.crop_size),\n",
    "                A.RandomResizedCrop(args.image_size,args.crop_size)\n",
    "             ], p=1),\n",
    "            A.OneOf([\n",
    "                #A.RandomContrast(),\n",
    "                #A.RandomBrightness(),\n",
    "                A.RandomGamma(),\n",
    "                A.RandomBrightnessContrast()\n",
    "                ], p=0.5),\n",
    "            A.OneOf([\n",
    "                A.CLAHE(clip_limit=2),\n",
    "                A.HueSaturationValue(10,15,10),\n",
    "                A.ChannelShuffle(),\n",
    "                A.InvertImg()\n",
    "                ], p=1),\n",
    "            A.OneOf([\n",
    "                A.HorizontalFlip(),\n",
    "                A.VerticalFlip(),\n",
    "                A.RandomRotate90(),\n",
    "                A.ShiftScaleRotate()\n",
    "            ], p = 0.5 ),\n",
    "        \n",
    "        #A.Resize(512, 512),\n",
    "        ToTensor()\n",
    "    ],p=1.)\n",
    "def val_augment():\n",
    "\n",
    "    return A.Compose([\n",
    "        ToTensor()\n",
    "    ],p=1.)\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2, valid_probability3 = [],[],[]\n",
    "    valid_mask, valid_mask2, valid_mask3 = [],[],[]\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, (image,mask) in enumerate(valid_loader):\n",
    "            #mask  = batch['mask']\n",
    "            image = image.to(device)\n",
    "            \n",
    "            if args.clf_head:\n",
    "                logit, _ = net(image) # seg, clf\n",
    "            else:\n",
    "                logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "                \n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "    #assert(valid_num == len(valid_loader.dataset)) # drop last True이면 assert되는거임\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    if args.loss =='bce':\n",
    "        loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "    elif args.loss =='lovasz':\n",
    "        loss = 0\n",
    "    \n",
    "    # mean loss, dice ..\n",
    "    dice = np_dice_score(probability, mask)\n",
    "    tp, tn = np_accuracy(probability, mask)\n",
    "\n",
    "    return [dice, loss,  tp, tn]\n",
    "\n",
    "def run_train(args):\n",
    "    out_dir = data_dir + f'/result/{args.dir}_{args.encoder}_{args.image_size}'\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint','train','valid'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "    #backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.train.txt',mode='a')\n",
    "\n",
    "    # my log argument\n",
    "    print_args(args, log)\n",
    "\n",
    "    log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "    log.write('** dataset setting **\\n')\n",
    "    #-----------dataset split --------------------#\n",
    "    tile_id = []\n",
    "    image_dir_ = f'{args.dataset}'#'0.25_320_160_train'\n",
    "    image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "    \n",
    "    image_dir_val_ = f'{args.val_dataset}'#'0.25_320_320_val'\n",
    "    image_dir_val=[image_dir_val_, ]\n",
    "    \n",
    "    for i in range(len(image_dir)):\n",
    "        df = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir[i]) )\n",
    "\n",
    "    for i in range(len(image_dir_val)):\n",
    "        df2 = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir_val[i]) )\n",
    "    df2['img_id'] = df2['tile_id'].apply(lambda x: x.split('/')[-2])\n",
    "        \n",
    "    kf = KFold(n_splits=args.n_fold, random_state=args.seed, shuffle=True)\n",
    "    all_dice = []\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        if not args.all_fold_train:\n",
    "            if n_fold != args.fold:\n",
    "                print(f'{n_fold} fold pass')\n",
    "                continue\n",
    "        if n_fold in [3, 4]:\n",
    "            print(n_fold,'fold pass')\n",
    "            continue\n",
    "        train_df = df[df['fold']!= n_fold].reset_index(drop=True)\n",
    "        val_df = df2[df2['fold']== n_fold].reset_index(drop=True).copy()\n",
    "        \n",
    "        # validation loader 3개 만들기 위함\n",
    "        unique_value = val_df['tile_id'].apply(lambda x: x.split('/')[-2]).unique() #[valid_id1, valid_id2, valid_id3 ]\n",
    "        val_img_id1 = unique_value[0] ; val_img_id2 = unique_value[1] ; val_img_id3= unique_value[2]\n",
    "        val_df1= val_df[val_df['img_id']==val_img_id1].reset_index(drop=True)\n",
    "        val_df2= val_df[val_df['img_id']==val_img_id2].reset_index(drop=True)\n",
    "        val_df3= val_df[val_df['img_id']==val_img_id3].reset_index(drop=True)\n",
    "        #####################################################\n",
    "        train_dataset = HuDataset(\n",
    "            df = train_df,\n",
    "            augment = train_augment()\n",
    "        )\n",
    "        train_loader  = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 8,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "        # val loader1\n",
    "        valid_dataset1 = HuDataset(\n",
    "            df = val_df1,\n",
    "            augment = val_augment()\n",
    "        )\n",
    "        valid_loader1 = DataLoader(\n",
    "            valid_dataset1,\n",
    "            sampler = SequentialSampler(valid_dataset1),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "        # val loader2\n",
    "        valid_dataset2 = HuDataset(\n",
    "            df = val_df2,\n",
    "            augment = val_augment()\n",
    "        )\n",
    "        \n",
    "        valid_loader2 = DataLoader(\n",
    "            valid_dataset2,\n",
    "            sampler = SequentialSampler(valid_dataset2),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "        # val loader3\n",
    "        valid_dataset3 = HuDataset(\n",
    "            df = val_df3,\n",
    "            augment = val_augment()\n",
    "        )\n",
    "        valid_loader3 = DataLoader(\n",
    "            valid_dataset3,\n",
    "            sampler = SequentialSampler(valid_dataset3),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "        log.write('fold = %s\\n'%str(n_fold))\n",
    "        log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "        log.write('valid_dataset1 : \\n%s\\n'%(valid_dataset1))\n",
    "        log.write('valid_dataset2 : \\n%s\\n'%(valid_dataset2))\n",
    "        log.write('valid_dataset3 : \\n%s\\n'%(valid_dataset3))\n",
    "        log.write('\\n')\n",
    "\n",
    "        # ------------------------\n",
    "        #  Model\n",
    "        # ------------------------\n",
    "        log.write('** net setting **\\n')\n",
    "\n",
    "        scaler = GradScaler()\n",
    "        net = SegModel() \n",
    "        net = net.to(device)\n",
    "        \n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        if args.opt =='adamw':\n",
    "            optimizer = torch.optim.AdamW(net.parameters(), lr = args.start_lr)\n",
    "\n",
    "        elif args.opt =='radam_look':\n",
    "            optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=args.start_lr), alpha=0.5, k=5)\n",
    "        if optimizer == None:\n",
    "            assert False, 'no have optimizer'\n",
    "        \n",
    "        # ------------------------\n",
    "        #  scheduler\n",
    "        # ------------------------\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "\n",
    "        log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "        #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "        log.write('\\n')\n",
    "\n",
    "        ## start training here! ##############################################\n",
    "        #array([0.57142857, 0.42857143])\n",
    "        log.write('** start training here! **\\n')\n",
    "        log.write('   is_mixed_precision = %s \\n'%str(args.amp))\n",
    "        log.write('   batch_size = %d \\n'%(args.batch_size))\n",
    "        log.write('             |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n",
    "        log.write('rate  epoch  | dice   loss   tp     tn     | loss           | time           \\n')\n",
    "        log.write('-------------------------------------------------------------------------------------\\n')\n",
    "                  #0.00100   0.50  0.80 | 0.891  0.020  0.000  0.000  | 0.000  0.000   |  0 hr 02 min\n",
    "\n",
    "        def message(mode='print'):\n",
    "            if mode==('print'):\n",
    "                asterisk = ' '\n",
    "                loss = batch_loss\n",
    "            if mode==('log'):\n",
    "                asterisk = '*'\n",
    "                loss = train_loss\n",
    "\n",
    "            text = \\\n",
    "                '%0.5f  %s%s    | '%(rate, epoch, asterisk,) +\\\n",
    "                '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n",
    "                '%4.3f  %4.3f   | '%(*loss,) +\\\n",
    "                '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "\n",
    "            return text\n",
    "\n",
    "        #----\n",
    "        valid_loss = np.zeros(4,np.float32)\n",
    "        train_loss = np.zeros(2,np.float32)\n",
    "        batch_loss = np.zeros_like(train_loss)\n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = 0\n",
    "        loss = torch.FloatTensor([0]).sum()\n",
    "\n",
    "\n",
    "        start_timer = timer()\n",
    "        rate = 0\n",
    "        best_dice = 0\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            #print('\\r',end='',flush=True)\n",
    "            #log.write(message(mode='log')+'\\n')\n",
    "            # training\n",
    "            for t, (image, mask) in enumerate(train_loader):\n",
    "                # learning rate schduler -------------\n",
    "                #adjust_learning_rate(optimizer, schduler(iteration))\n",
    "                rate = get_learning_rate(optimizer)\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                #batch_size = len(batch['index'])\n",
    "                net.train()\n",
    "\n",
    "                if args.amp:\n",
    "                    #image = image.half()\n",
    "                    with autocast():\n",
    "                        mask  = mask.to(device)\n",
    "                        image = image.to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        #logit = data_parallel(net, image)\n",
    "                        if args.clf_head:\n",
    "                            logit, logit2 = net(image) # seg logit, clf logit\n",
    "                        else:\n",
    "                            logit = net(image)\n",
    "                        if args.loss == 'bce':\n",
    "                            if args.label_smoothing:\n",
    "                                loss = LabelSmoothing()(logit, mask)\n",
    "                            else:\n",
    "                                loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                            if args.clf_head:\n",
    "                                loss += args.clf_alpha *nn.BCEWithLogitsLoss()(logit2, (mask.sum(dim=(2,3))>0).float() )\n",
    "                        elif args.loss =='lovasz':\n",
    "                            #loss = LovaszHingeLoss()(logit, mask)\n",
    "                            loss = symmetric_lovasz(logit, mask)\n",
    "                            \n",
    "                        elif args.loss == 'bce_dice':\n",
    "                            loss = DiceBCELoss()(logit, mask)\n",
    "                        elif args.loss == 'dice':\n",
    "                            loss = DiceLoss()(logit, mask)\n",
    "                            \n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else :\n",
    "                    mask  = batch['mask'].to(device)\n",
    "                    image = batch['image'].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    #logit = data_parallel(net, image)\n",
    "                    logit = net(image)\n",
    "                    if args.loss == 'bce':\n",
    "                        loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                    elif args.loss =='lovasz':\n",
    "                        loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # print statistics  --------\n",
    "\n",
    "                batch_loss = np.array([ loss.item(), 0 ])\n",
    "                sum_train_loss += batch_loss\n",
    "                sum_train += 1\n",
    "\n",
    "                #print('\\r',end='',flush=True)\n",
    "                #print(message(mode='print'), end='',flush=True)\n",
    "            \n",
    "\n",
    "            # train loss\n",
    "            train_loss = sum_train_loss/(sum_train+1e-12)\n",
    "            sum_train_loss[...] = 0\n",
    "            sum_train = 0\n",
    "\n",
    "            # scheudler\n",
    "            valid_loss1 = do_valid(net, valid_loader1) #\n",
    "            valid_loss2 = do_valid(net, valid_loader2)\n",
    "            valid_loss3 = do_valid(net, valid_loader3)\n",
    "            valid_loss = (np.array(valid_loss1) + np.array(valid_loss2) + np.array(valid_loss3))/3\n",
    "            \n",
    "            log.write(message(mode='log')+'\\n')\n",
    "            log.write(f'{val_img_id1} dice : {valid_loss1[0]:.5f}, {val_img_id2} dice : {valid_loss2[0]:.5f}, {val_img_id3} dice : {valid_loss3[0]:.5f}\\n')\n",
    "            \n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(valid_loss[0])\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "            # saved models\n",
    "            #if valid_loss[0] > best_dice:\n",
    "            if valid_loss[0] > best_dice:\n",
    "                best_dice = valid_loss[0]\n",
    "                log.write(f'\\n saved best models, dice:{best_dice:.5f}\\n')\n",
    "                torch.save({\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                }, out_dir + f'/checkpoint/{n_fold}fold_{epoch}epoch_{best_dice:.4f}_model.pth')\n",
    "            \n",
    "            log.write('='*80+'\\n')\n",
    "\n",
    "        log.write('\\n')\n",
    "        \n",
    "        all_dice.append(best_dice)\n",
    "    \n",
    "    print(f'all dice score : {sum(all_dice)/len(all_dice) : .4f}')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "available-canal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__module__       : __main__\n",
      "amp              : True\n",
      "gpu              : 0, 1, 2, 3\n",
      "encoder          : b4\n",
      "decoder          : unet\n",
      "diff_arch        : True\n",
      "encoders         : ['efficientnet-b4', 'efficientnet-b4', 'resnet34', 'xception', 'dpn68']\n",
      "decoders         : ['unet', 'fpn', 'upp', 'unet', 'upp']\n",
      "batch_size       : 64\n",
      "weight_decay     : 1e-06\n",
      "epochs           : 1\n",
      "n_fold           : 5\n",
      "fold             : 0\n",
      "all_fold_train   : True\n",
      "image_size       : 512\n",
      "crop_size        : 512\n",
      "tile_size        : 640\n",
      "tile_step        : 320\n",
      "tile_scale       : 0.5\n",
      "dataset          : 0.5_640_320_train_fold\n",
      "val_dataset      : 0.5_640_640_val_fold\n",
      "dir              : 1_['efficientnet-b4', 'efficientnet-b4', 'resnet34', 'xception', 'dpn68']_['unet', 'fpn', 'upp', 'unet', 'upp']_512_640_320_0.5\n",
      "T_max            : 10\n",
      "opt              : radam_look\n",
      "scheduler        : CosineAnnealingLR\n",
      "loss             : bce\n",
      "factor           : 0.4\n",
      "patience         : 3\n",
      "eps              : 1e-06\n",
      "decay_epoch      : [4, 8, 12]\n",
      "T_0              : 4\n",
      "start_lr         : 0.001\n",
      "min_lr           : 1e-06\n",
      "clf_head         : False\n",
      "label_smoothing  : False\n",
      "multi_gpu        : True\n",
      "clf_alpha        : 0.3\n",
      "smoothing        : 0.1\n",
      "dice_smoothing   : 1\n",
      "num_workers      : 8\n",
      "seed             : 42\n",
      "__dict__         : <attribute '__dict__' of 'args' objects>\n",
      "__weakref__      : <attribute '__weakref__' of 'args' objects>\n",
      "__doc__          : None\n",
      "\tout_dir  = /home/jeonghokim/competition/HubMap/data//result/1_['efficientnet-b4', 'efficientnet-b4', 'resnet34', 'xception', 'dpn68']_['unet', 'fpn', 'upp', 'unet', 'upp']_512_640_320_0.5_b4_512\n",
      "\n",
      "** dataset setting **\n",
      "fold = 0\n",
      "train_dataset : \n",
      "\tlen  = 16331\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 442\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 208\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 670\n",
      "\n",
      "\n",
      "** net setting **\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-059aaf3279a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# set seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no set seed'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mset_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b6bdf2f2403c>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# ------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set seed\n",
    "    print('no set seed') if args.seed ==-1 else set_seeds(seed=args.seed)\n",
    "    run_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-embassy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minimal-sociology",
   "metadata": {
    "code_folding": [
     0,
     3,
     30,
     76,
     113
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------- tile shuffle fold train version(use X) ----------#\n",
    "\"\"\"\n",
    "# augmentation\n",
    "def train_augment(record):\n",
    "    image = record['image']\n",
    "    mask  = record['mask']\n",
    "    \n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask : do_random_rotate_crop(image, mask, size=args.image_size, mag=45),\n",
    "        lambda image, mask : do_random_scale_crop(image, mask, size=args.image_size, mag=0.075),\n",
    "        lambda image, mask : do_random_crop(image, mask, size=args.image_size),\n",
    "    ],1): image, mask = fn(image, mask)\n",
    "\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask : (image, mask),\n",
    "        lambda image, mask : do_random_contast(image, mask, mag=0.8),\n",
    "        lambda image, mask : do_random_gain(image, mask, mag=0.9),\n",
    "        #lambda image, mask : do_random_hsv(image, mask, mag=[0.1, 0.2, 0]),\n",
    "        lambda image, mask : do_random_noise(image, mask, mag=0.1),\n",
    "    ],2): image, mask =  fn(image, mask)\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    image, mask = do_random_hsv(image, mask, mag=[0.1, 0.2, 0])\n",
    "    image, mask = do_random_flip_transpose(image, mask)\n",
    "\n",
    "    record['mask'] = mask\n",
    "    record['image'] = image\n",
    "    return record\n",
    "\n",
    "# validation\n",
    "def do_valid2(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2 = [],[]\n",
    "    valid_mask = []\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            batch_size = len(batch['index'])\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "\n",
    "            logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "            \n",
    "            # loss\n",
    "            if args.loss =='bce':\n",
    "                #loss += criterion_binary_cross_entropy(probability.data.cpu(), mask.data.cpu()).item()\n",
    "                loss += np_binary_cross_entropy_loss(probability.cpu().numpy(), mask.cpu().numpy())\n",
    "            elif args.loss =='lovasz':\n",
    "                loss += symmetric_lovasz(probability.data.cpu(), mask.data.cpu()).item()\n",
    "                \n",
    "            # dice\n",
    "            dice += dice_score(probability.data.cpu(), mask.data.cpu(), threshold = 0.5).item()\n",
    "            tp_, tn_ = torch_accuracy(probability.data.cpu(), mask.data.cpu(), threshold = 0.5)\n",
    "            tp+=tp_.item() ; tn += tn_.item()\n",
    "            # numpy\n",
    "            #dice2 += np_dice_score(probability.data.cpu().numpy(), mask.data.cpu().numpy())\n",
    "            #loss2 += np_binary_cross_entropy_loss(probability.cpu().numpy(), mask.cpu().numpy())\n",
    "\n",
    "            #valid_num += batch_size\n",
    "\n",
    "    #assert(valid_num == len(valid_loader.dataset)) # drop last True이면 assert되는거임\n",
    "    \n",
    "    # mean loss, dice ..\n",
    "    loss = loss/len(valid_loader)\n",
    "    dice = dice/len(valid_loader)\n",
    "    tp = tp/len(valid_loader) ; tn = tn/len(valid_loader)\n",
    "\n",
    "    return [dice, loss,  tp, tn]\n",
    "# append 버전\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2 = [],[]\n",
    "    valid_mask = []\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            batch_size = len(batch['index'])\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "\n",
    "            logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "            \n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "\n",
    "    \n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    if args.loss =='bce':\n",
    "        loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "    elif args.loss =='lovasz':\n",
    "        loss = symmetric_lovasz(probability, mask)\n",
    "    \n",
    "    # mean loss, dice ..\n",
    "    dice = np_dice_score(probability, mask)\n",
    "    tp, tn = np_accuracy(probability, mask)\n",
    "    return [dice, loss,  tp, tn]\n",
    "\n",
    "def run_train(args):\n",
    "    out_dir = data_dir + f'/result/{args.dir}_fold{args.fold}_{args.encoder}_{args.image_size}'\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint','train','valid','backup'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "    #backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.train.txt',mode='a')\n",
    "\n",
    "    # my log argument\n",
    "    print_args(args, log)\n",
    "\n",
    "    log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "    log.write('** dataset setting **\\n')\n",
    "    #-----------dataset split --------------------#\n",
    "    tile_id = []\n",
    "    image_dir_ = f'{args.dataset}'#'0.25_480_240_train'\n",
    "    image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "\n",
    "    for i in range(len(image_dir)):\n",
    "        df = pd.read_csv(data_dir + '/tile/%s/image_id.csv'% (image_dir[i]) )\n",
    "        tile_id += ('%s/'%(image_dir[i]) + df.tile_id).tolist()\n",
    "\n",
    "    kf = KFold(n_splits=args.n_fold, random_state=args.seed, shuffle=True)\n",
    "    all_dice = []\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(tile_id)):\n",
    "        if not args.all_fold_train:\n",
    "            if n_fold != args.fold:\n",
    "                print(f'{n_fold} fold pass')\n",
    "                continue\n",
    "\n",
    "        #####################################################\n",
    "        train_dataset = HuDataset(\n",
    "            tile_id = df.loc[trn_idx]['tile_id'].tolist(),\n",
    "            augment = train_augment\n",
    "        )\n",
    "        train_loader  = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 8,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "\n",
    "        valid_dataset = HuDataset(\n",
    "            tile_id = df.loc[val_idx]['tile_id'].tolist()\n",
    "            ,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler = SequentialSampler(valid_dataset),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        log.write('fold = %s\\n'%str(n_fold))\n",
    "        log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "        log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n",
    "        log.write('\\n')\n",
    "\n",
    "        # ------------------------\n",
    "        #  Model\n",
    "        # ------------------------\n",
    "        log.write('** net setting **\\n')\n",
    "\n",
    "        scaler = GradScaler()\n",
    "        net = SegModel() \n",
    "        net = net.to(device)\n",
    "        \n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        if args.opt =='adamw':\n",
    "            optimizer = torch.optim.AdamW(net.parameters(), lr = args.start_lr)\n",
    "\n",
    "        elif args.opt =='radam_look':\n",
    "            optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=args.start_lr), alpha=0.5, k=5)\n",
    "        if optimizer == None:\n",
    "            assert False, 'no have optimizer'\n",
    "        \n",
    "        # ------------------------\n",
    "        #  scheduler\n",
    "        # ------------------------\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "\n",
    "        log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "        #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "        log.write('\\n')\n",
    "\n",
    "        ## start training here! ##############################################\n",
    "        #array([0.57142857, 0.42857143])\n",
    "        log.write('** start training here! **\\n')\n",
    "        log.write('   is_mixed_precision = %s \\n'%str(args.amp))\n",
    "        log.write('   batch_size = %d \\n'%(args.batch_size))\n",
    "        log.write('             |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n",
    "        log.write('rate  epoch  | dice   loss   tp     tn     | loss           | time           \\n')\n",
    "        log.write('-------------------------------------------------------------------------------------\\n')\n",
    "                  #0.00100   0.50  0.80 | 0.891  0.020  0.000  0.000  | 0.000  0.000   |  0 hr 02 min\n",
    "\n",
    "        def message(mode='print'):\n",
    "            if mode==('print'):\n",
    "                asterisk = ' '\n",
    "                loss = batch_loss\n",
    "            if mode==('log'):\n",
    "                asterisk = '*'\n",
    "                loss = train_loss\n",
    "\n",
    "            text = \\\n",
    "                '%0.5f  %s%s    | '%(rate, epoch, asterisk,) +\\\n",
    "                '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n",
    "                '%4.3f  %4.3f   | '%(*loss,) +\\\n",
    "                '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "\n",
    "            return text\n",
    "\n",
    "        #----\n",
    "        valid_loss = np.zeros(4,np.float32)\n",
    "        train_loss = np.zeros(2,np.float32)\n",
    "        batch_loss = np.zeros_like(train_loss)\n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = 0\n",
    "        loss = torch.FloatTensor([0]).sum()\n",
    "\n",
    "\n",
    "        start_timer = timer()\n",
    "        rate = 0\n",
    "        best_dice = 0\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            #print('\\r',end='',flush=True)\n",
    "            #log.write(message(mode='log')+'\\n')\n",
    "            # training\n",
    "            for t, batch in enumerate(train_loader):\n",
    "\n",
    "                # learning rate schduler -------------\n",
    "                #adjust_learning_rate(optimizer, schduler(iteration))\n",
    "                rate = get_learning_rate(optimizer)\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                batch_size = len(batch['index'])\n",
    "                net.train()\n",
    "\n",
    "                if args.amp:\n",
    "                    #image = image.half()\n",
    "                    with autocast():\n",
    "                        mask  = batch['mask'].to(device)\n",
    "                        image = batch['image'].to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        #logit = data_parallel(net, image)\n",
    "                        logit = net(image)\n",
    "                        if args.loss == 'bce':\n",
    "                            loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                        elif args.loss =='lovasz':\n",
    "                            loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else :\n",
    "                    mask  = batch['mask'].to(device)\n",
    "                    image = batch['image'].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    #logit = data_parallel(net, image)\n",
    "                    logit = net(image)\n",
    "                    if args.loss == 'bce':\n",
    "                        loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                    elif args.loss =='lovasz':\n",
    "                        loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # print statistics  --------\n",
    "\n",
    "                batch_loss = np.array([ loss.item(), 0 ])\n",
    "                sum_train_loss += batch_loss\n",
    "                sum_train += 1\n",
    "\n",
    "                #print('\\r',end='',flush=True)\n",
    "                #print(message(mode='print'), end='',flush=True)\n",
    "\n",
    "            # train loss\n",
    "            train_loss = sum_train_loss/(sum_train+1e-12)\n",
    "            sum_train_loss[...] = 0\n",
    "            sum_train = 0\n",
    "\n",
    "            # scheudler\n",
    "            valid_loss = do_valid(net, valid_loader) #\n",
    "            log.write(message(mode='log')+'\\n')\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(valid_loss[0])\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "            # saved models\n",
    "            #if valid_loss[0] > best_dice:\n",
    "            if valid_loss[0] > best_dice:\n",
    "                best_dice = valid_loss[0]\n",
    "                print(f'\\n saved best models, dice:{best_dice:.5f}')\n",
    "                torch.save({\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                }, out_dir + f'/checkpoint/{n_fold}fold_{epoch}epoch_{best_dice:.4f}_model.pth')\n",
    "\n",
    "        log.write('\\n')\n",
    "        all_dice.append(best_dice)\n",
    "    \n",
    "    print(f'all dice score : {sum(all_dice)/len(all_dice) : .4f}')\n",
    "  \n",
    "     \"\"\"   \n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-influence",
   "metadata": {},
   "source": [
    "# validation 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-factory",
   "metadata": {},
   "source": [
    "eval mode : 모델들 불러와서 validation에 해당하는 이미지 예측후 cv측정, threshold별 dice 계산\n",
    "\n",
    "gen_image : validation에 해당하는 이미지 예측후 visualize(저장된 이미지로 확인가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "antique-ready",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    server ='local' # ['kaggle', 'local'] local은 cv측정용도\n",
    "    amp = False\n",
    "    gpu = 1\n",
    "    \n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    n_fold = 5\n",
    "    diff_arch = True\n",
    "    encoders = [\"xception\", \"efficientnet-b4\", \"xception\", \"efficientnet-b4\", \"xception\"]\n",
    "    decoders = [\"unet\", \"fpn\", \"upp\", \"unet\", \"linknet\"]\n",
    "    batch_size=16\n",
    "    #fold=0\n",
    "    mode = 'eval' # ['eval', 'gen_image']\n",
    "    loss = 'bce'\n",
    "    clf_head=False\n",
    "    dataset = '0.5_640_320_train_fold'#'[0.25_256_128_train', '0.25_480_240_train' ]# dataset size\n",
    "    val_dataset = '0.5_640_640_val_fold'\n",
    "    \n",
    "    model_path = [\"./data/result/50_['xception', 'efficientnet-b4', 'xception', 'efficientnet-b4', 'xception']_['unet', 'fpn', 'upp', 'unet', 'linknet']_512_640_320_0.5_b4_512\" +\n",
    "                  \"/checkpoint/\" + x for x in \\\n",
    "                 ['0fold_13epoch_0.9375_xception_unetmodel.pth','1fold_34epoch_0.9367_efficientnet-b4_fpnmodel.pth',\n",
    "                 '2fold_8epoch_0.9372_xception_uppmodel.pth','3fold_36epoch_0.9529_efficientnet-b4_unetmodel.pth',\n",
    "                 '4fold_15epoch_0.9241_xception_linknetmodel.pth']]\n",
    "    \n",
    "    sub = '[visualize][04.05]_0.9337_models'# submission name\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    \n",
    "    tile_size = 640\n",
    "    tile_average_step = 320\n",
    "    tile_scale = 0.25\n",
    "    tile_min_score = 0.25  \n",
    "\n",
    "#assert args.server!='local', 'not implement'\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "round-stranger",
   "metadata": {
    "code_folding": [
     2,
     41,
     189,
     199,
     204,
     214,
     218
    ]
   },
   "outputs": [],
   "source": [
    "all_dice_dict={}\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2, valid_probability3 = [],[],[]\n",
    "    valid_mask, valid_mask2, valid_mask3 = [],[],[]\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "            \n",
    "            if args.clf_head:\n",
    "                logit, _ = net(image) # seg, clf\n",
    "            else:\n",
    "                logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "                \n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "    #assert(valid_num == len(valid_loader.dataset)) # drop last True이면 assert되는거임\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    if args.loss =='bce':\n",
    "        loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "    elif args.loss =='lovasz':\n",
    "        loss = 0\n",
    "    \n",
    "    dice = [np_dice_score2(probability, mask, round(th, 2)) for th in np.arange(0.1, 0.7, 0.05)]\n",
    "    #tp, tn = np_accuracy(probability, mask)\n",
    "\n",
    "    return np.array(dice)#[dice_dict, loss,  tp, tn]\n",
    "\n",
    "\n",
    "def gen_val_image(args):\n",
    "    out_dir = args.model_path[0].split('checkpoint')[0]\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint','train','valid','backup'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.val.txt',mode='a')\n",
    "\n",
    "    # my log argument\n",
    "    print_args(args, log)\n",
    "\n",
    "    submit_dir = out_dir + '/valid/%s-mean'%(args.server)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    #\n",
    "    for fold in range(5):\n",
    "        scaler = GradScaler()\n",
    "        net = SegModel() \n",
    "        net = net.to(device)\n",
    "        state_dict = torch.load(args.model_path[fold], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        net.load_state_dict(state_dict,strict=True)  #True\n",
    "        net = net.eval()\n",
    "\n",
    "        #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "        log.write('\\n')\n",
    "\n",
    "        #----      \n",
    "\n",
    "        # make validation predict images\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "        #\n",
    "        a = pd.read_csv('../hubmap/tile/0.25_320_160_train_fold/image_id_split.csv')\n",
    "        b = a[a['fold']==fold]\n",
    "        valid_image_id = b['tile_id'].apply(lambda x : x.split('/')[-2]).unique()\n",
    "\n",
    "        #\n",
    "        start_timer = timer()\n",
    "        for id in valid_image_id:\n",
    "            image_file = data_dir + '/train/%s.tiff' % id\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "            structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "            mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "            mask  = read_mask(mask_file)\n",
    "\n",
    "            #--- predict here!  ---\n",
    "            tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "            tile_image = tile['tile_image']\n",
    "            tile_image = np.stack(tile_image)[..., ::-1]\n",
    "            tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "            tile_image = tile_image.astype(np.float32)/255\n",
    "            print(tile_image.shape)\n",
    "            tile_probability = []\n",
    "\n",
    "            batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "            for t,m in enumerate(batch):\n",
    "                print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "                m = torch.from_numpy(m).to(device)\n",
    "\n",
    "                p = []\n",
    "                with torch.no_grad():\n",
    "                    logit = net(m)\n",
    "                    p.append(torch.sigmoid(logit))\n",
    "                    if args.server == 'local':\n",
    "                        if 0: #tta here\n",
    "                            #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.cat(p)\n",
    "\n",
    "                tile_probability.append(p.data.cpu().numpy())\n",
    "            print('\\r' , end='',flush=True)\n",
    "            log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "            tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "            height, width = tile['image_small'].shape[:2]\n",
    "            probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                                  tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                                  aggregate='mean')\n",
    "            #\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            overlay = np.dstack([\n",
    "                np.zeros_like(truth),\n",
    "                probability, #green\n",
    "                truth, #red\n",
    "            ])\n",
    "\n",
    "            image_small = tile['image_small'].astype(np.float32)/255\n",
    "            #predict = (probability>thres).astype(np.float32)\n",
    "            overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "            overlay2 = image_small.copy()\n",
    "            overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "            overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "            overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "            if 1:\n",
    "                #cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "                cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "def eval_image(args):\n",
    "\n",
    "    #-----------dataset split --------------------#\n",
    "    tile_id = []\n",
    "    image_dir_ = f'{args.dataset}'#'0.25_320_160_train'\n",
    "    image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "    \n",
    "    image_dir_val_ = f'{args.val_dataset}'#'0.25_320_320_val'\n",
    "    image_dir_val=[image_dir_val_, ]\n",
    "    \n",
    "    for i in range(len(image_dir)):\n",
    "        df = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir[i]) )\n",
    "\n",
    "    for i in range(len(image_dir_val)):\n",
    "        df2 = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir_val[i]) )\n",
    "    df2['img_id'] = df2['tile_id'].apply(lambda x: x.split('/')[-2])\n",
    "        \n",
    "    all_dice = []\n",
    "    for n_fold in range(5):\n",
    "\n",
    "        train_df = df[df['fold']!= n_fold].reset_index(drop=True)\n",
    "        val_df = df2[df2['fold']== n_fold].reset_index(drop=True).copy()\n",
    "        \n",
    "        # validation loader 3개 만들기 위함\n",
    "        unique_value = val_df['tile_id'].apply(lambda x: x.split('/')[-2]).unique() #[valid_id1, valid_id2, valid_id3 ]\n",
    "        val_img_id1 = unique_value[0] ; val_img_id2 = unique_value[1] ; val_img_id3= unique_value[2]\n",
    "        val_df1= val_df[val_df['img_id']==val_img_id1].reset_index(drop=True)\n",
    "        val_df2= val_df[val_df['img_id']==val_img_id2].reset_index(drop=True)\n",
    "        val_df3= val_df[val_df['img_id']==val_img_id3].reset_index(drop=True)\n",
    "        #####################################################\n",
    "        # val loader1\n",
    "        valid_dataset1 = HuDataset(\n",
    "            df = val_df1\n",
    "            ,\n",
    "        )\n",
    "        valid_loader1 = DataLoader(\n",
    "            valid_dataset1,\n",
    "            sampler = SequentialSampler(valid_dataset1),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader2\n",
    "        valid_dataset2 = HuDataset(\n",
    "            df = val_df2\n",
    "            ,\n",
    "        )\n",
    "        \n",
    "        valid_loader2 = DataLoader(\n",
    "            valid_dataset2,\n",
    "            sampler = SequentialSampler(valid_dataset2),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader3\n",
    "        valid_dataset3 = HuDataset(\n",
    "            df = val_df3\n",
    "            ,\n",
    "        )\n",
    "        valid_loader3 = DataLoader(\n",
    "            valid_dataset3,\n",
    "            sampler = SequentialSampler(valid_dataset3),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # ------------------------\n",
    "        #  Model\n",
    "        # ------------------------\n",
    "\n",
    "        scaler = GradScaler()\n",
    "        models = SegModel() \n",
    "        net = models[n_fold].to(device)\n",
    "        state_dict = torch.load(args.model_path[n_fold], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        # 병렬처리를 했으면 앞에 module이 붙으므로 키를 바꿔줘야 한다. \n",
    "        for key in list(state_dict.keys()):\n",
    "            if \"module.\" in key:\n",
    "                state_dict[key.replace(\"module.\", \"\")] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        net.load_state_dict(state_dict,strict=True)  #True\n",
    "        net = net.eval()\n",
    "        \n",
    "        print(\"model load success!!!\")\n",
    "        # scheudler\n",
    "        valid_loss1 = do_valid(net, valid_loader1) #\n",
    "        valid_loss2 = do_valid(net, valid_loader2)\n",
    "        valid_loss3 = do_valid(net, valid_loader3)\n",
    "        valid_loss = (valid_loss1 + valid_loss2 + valid_loss3)/3\n",
    "        \n",
    "        all_dice.append(valid_loss)\n",
    "        \n",
    "    dice = sum(all_dice)/len(all_dice)\n",
    "    for n, th in enumerate(np.arange(0.1, 0.7, 0.05)):\n",
    "        th = round(th, 2)\n",
    "        print(f'th:{th}, dice score : {dice[n] : .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "documented-enforcement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "th:0.1, dice score :  0.9089\n",
      "th:0.15, dice score :  0.9189\n",
      "th:0.2, dice score :  0.9257\n",
      "th:0.25, dice score :  0.9304\n",
      "th:0.3, dice score :  0.9338\n",
      "th:0.35, dice score :  0.9361\n",
      "th:0.4, dice score :  0.9374\n",
      "th:0.45, dice score :  0.9379\n",
      "th:0.5, dice score :  0.9377\n",
      "th:0.55, dice score :  0.9367\n",
      "th:0.6, dice score :  0.9347\n",
      "th:0.65, dice score :  0.9316\n"
     ]
    }
   ],
   "source": [
    "\"\"\"red is real\"\"\"\n",
    "if 1: #normal\n",
    "    if __name__ == '__main__':\n",
    "        if args.mode == 'eval':\n",
    "            eval_image(args)\n",
    "        elif args.mode =='gen_image':\n",
    "            gen_val_image(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-hydrogen",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "varied-feeling",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    server ='kaggle' # ['kaggle', 'local'] local은 cv측정용도\n",
    "    amp = False\n",
    "    gpu = 0\n",
    "    \n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    \n",
    "    diff_arch = True\n",
    "    encoders = [\"xception\", \"efficientnet-b4\", \"xception\", \"efficientnet-b4\", \"xception\"]\n",
    "    decoders = [\"unet\", \"fpn\", \"upp\", \"unet\", \"linknet\"]\n",
    "    n_fold = 5\n",
    "    batch_size=64\n",
    "    clf_head=False\n",
    "    \n",
    "    threshold = 0.45\n",
    "    \n",
    "    model_path = '../hubmap/result/'\n",
    "\n",
    "    en_model_path = [\"./data/result/50_['xception', 'efficientnet-b4', 'xception', 'efficientnet-b4', 'xception']_['unet', 'fpn', 'upp', 'unet', 'linknet']_512_640_320_0.5_b4_512\" +\n",
    "                  \"/checkpoint/\" + x for x in \\\n",
    "                 ['0fold_13epoch_0.9375_xception_unetmodel.pth','1fold_34epoch_0.9367_efficientnet-b4_fpnmodel.pth',\n",
    "                 '2fold_8epoch_0.9372_xception_uppmodel.pth','3fold_36epoch_0.9529_efficientnet-b4_unetmodel.pth',\n",
    "                 '4fold_15epoch_0.9241_xception_linknetmodel.pth']]\n",
    "    sub = '30epoch_imagefold_0.9338_320_160'# submission name\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    \n",
    "    tile_size = 640\n",
    "    tile_average_step = 320\n",
    "    tile_scale = 0.5\n",
    "    tile_min_score = 0.25  \n",
    "\n",
    "assert args.server!='local', 'not implement'\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "horizontal-booking",
   "metadata": {
    "code_folding": [
     4,
     26,
     257,
     353,
     363
    ]
   },
   "outputs": [],
   "source": [
    "thres = args.threshold\n",
    "\n",
    "prob = []\n",
    "\n",
    "def mask_to_csv(image_id, submit_dir):\n",
    "\n",
    "    predicted = []\n",
    "    for id in image_id:\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        predict_file = submit_dir + '/%s.predict.png' % id\n",
    "        # predict = cv2.imread(predict_file, cv2.IMREAD_GRAYSCALE)\n",
    "        predict = np.array(Image.open(predict_file))\n",
    "        predict = cv2.resize(predict, dsize=(width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        predict = (predict > 128).astype(np.uint8) * 255\n",
    "\n",
    "        p = rle_encode(predict)\n",
    "        predicted.append(p)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = image_id\n",
    "    df['predicted'] = predicted\n",
    "    return df\n",
    "\n",
    "def run_submit(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.model_path.split('checkpoint')[0]\n",
    "    initial_checkpoint = out_dir + '/checkpoint' + args.model_path.split('checkpoint')[1]\n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-mean-thres(%s)'%(server, initial_checkpoint[-18:-4],thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    # ----- model -------\n",
    "    net = SegModel() \n",
    "    net.to(device)\n",
    "    state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "    net.load_state_dict(state_dict,strict=True)  #True\n",
    "    net = net.eval()\n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        if server == 'local':\n",
    "            image_file = data_dir + '/train/%s.tiff' % id\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "            structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "            mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "            mask  = read_mask(mask_file)\n",
    "\n",
    "        if server == 'kaggle':\n",
    "            image_file = data_dir + '/test/%s.tiff' % id\n",
    "            json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "            structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "            mask = None\n",
    "\n",
    "\n",
    "        #--- predict here!  ---\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        tile_image = tile['tile_image']\n",
    "        tile_image = np.stack(tile_image)[..., ::-1]\n",
    "        tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "        tile_image = tile_image.astype(np.float32)/255\n",
    "        print(tile_image.shape)\n",
    "        tile_probability = []\n",
    "        \n",
    "        batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "        for t,m in enumerate(batch):\n",
    "            print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "            m = torch.from_numpy(m).to(device)\n",
    "\n",
    "            p = []\n",
    "            with torch.no_grad():\n",
    "                logit = net(m)\n",
    "                p.append(torch.sigmoid(logit))\n",
    "\n",
    "                #---\n",
    "                if server == 'kaggle':\n",
    "                    if 1: #tta here\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.stack(p).mean(0)\n",
    "                if server == 'local':\n",
    "                    if 0: #tta here\n",
    "                        #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.cat(p)\n",
    "                    #p = torch.stack(p)\n",
    "\n",
    "            tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "        print('\\r' , end='',flush=True)\n",
    "        log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "        tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "        height, width = tile['image_small'].shape[:2]\n",
    "        probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                              tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                              aggregate='mean')\n",
    "        \n",
    "\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = (probability>thres).astype(np.float32)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir + args.sub+'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0\n",
    "    \n",
    "def run_submit_ensemble(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.en_model_path[0].split('checkpoint')[0]\n",
    "    \n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-thres(%s)'%(server, args.sub,thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        fold_prob = []\n",
    "        models = SegModel()\n",
    "        for i, m_p in enumerate(args.en_model_path):\n",
    "            initial_checkpoint = m_p\n",
    "            # ----- model -------\n",
    "            net = models[i]\n",
    "            net.to(device)\n",
    "            state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "            for key in list(state_dict.keys()):\n",
    "                if \"module.\" in key:\n",
    "                    state_dict[key.replace(\"module.\", \"\")] = state_dict[key]\n",
    "                    del state_dict[key]\n",
    "            net.load_state_dict(state_dict,strict=True)  #True\n",
    "            net = net.eval()\n",
    "            print(\"model load success!!!\")\n",
    "            if server == 'local':\n",
    "                image_file = data_dir + '/train/%s.tiff' % id\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "                structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "                mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "                mask  = read_mask(mask_file)\n",
    "\n",
    "            if server == 'kaggle':\n",
    "                image_file = data_dir + '/test/%s.tiff' % id\n",
    "                json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "                structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "                mask = None\n",
    "\n",
    "\n",
    "            #--- predict here!  ---\n",
    "            tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "            tile_image = tile['tile_image']\n",
    "            tile_image = np.stack(tile_image)[..., ::-1]\n",
    "            tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "            tile_image = tile_image.astype(np.float32)/255\n",
    "            print(tile_image.shape)\n",
    "            tile_probability = []\n",
    "\n",
    "            batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "            for t,m in enumerate(batch):\n",
    "                print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "                m = torch.from_numpy(m).to(device)\n",
    "\n",
    "                p = []\n",
    "                with torch.no_grad():\n",
    "                    logit = net(m)\n",
    "                    p.append(torch.sigmoid(logit))\n",
    "\n",
    "                    #---\n",
    "                    if server == 'kaggle':\n",
    "                        if 1: #tta here\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.stack(p).mean(0)\n",
    "                    if server == 'local':\n",
    "                        if 0: #tta here\n",
    "                            #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.cat(p)\n",
    "                        #p = torch.stack(p)\n",
    "\n",
    "                tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "            print('\\r' , end='',flush=True)\n",
    "            log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "            tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "            height, width = tile['image_small'].shape[:2]\n",
    "            probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                                  tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                                  aggregate='mean')\n",
    "\n",
    "            fold_prob.append(probability)\n",
    "        \n",
    "        probability = sum(fold_prob)/len(args.en_model_path)\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = (probability>thres).astype(np.float32)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir +'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "brazilian-pennsylvania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_size = 640 \n",
      "tile_average_step = 320 \n",
      "tile_scale = 0.500000 \n",
      "tile_min_score = 0.250000 \n",
      "\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    1 min 25 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    3 min 01 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    5 min 45 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    7 min 26 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    8 min 49 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   10 min 39 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   11 min 13 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   12 min 12 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   13 min 06 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   13 min 45 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   16 min 44 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   19 min 03 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   22 min 50 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   25 min 19 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   26 min 44 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   31 min 37 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   35 min 23 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   41 min 10 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   45 min 11 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   48 min 24 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   51 min 50 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   54 min 05 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   57 min 47 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   60 min 09 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   62 min 08 secc\n",
      "          id                                          predicted\n",
      "0  2ec3f1bb9  60762289 44 60786279 44 60810257 64 60834247 6...\n",
      "1  3589adb90  68600108 39 68629540 41 68658950 71 68688382 7...\n",
      "2  57512b7f1  329019035 34 329052275 34 329085507 54 3291187...\n",
      "3  aa05346ff  59860739 370 59891459 370 59922177 372 5995289...\n",
      "4  d488c759a  548482197 46 548528857 46 548575509 64 5486221...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 0: #normal\n",
    "    if __name__ == '__main__':\n",
    "        run_submit(args)\n",
    "elif 1:# ensemble\n",
    "    if __name__ == '__main__':\n",
    "        run_submit_ensemble(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-cornell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-fields",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubmap",
   "language": "python",
   "name": "hubmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
