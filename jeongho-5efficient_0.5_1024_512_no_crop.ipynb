{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indonesian-astrology",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ------------Library--------------#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.autograd import Variable\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import tifffile as tiff\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from sklearn.model_selection import KFold\n",
    "# loss\n",
    "#from lovasz import lovasz_hinge\n",
    "#from losses_pytorch.lovasz_loss import LovaszSoftmax\n",
    "PI  = np.pi\n",
    "INF = np.inf\n",
    "EPS = 1e-12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "upset-paint",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    amp = True\n",
    "    gpu = '0,1,7'\n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    diff_arch = True\n",
    "    encoders = [\"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\"]\n",
    "    decoders = [\"unet\", \"unet\", \"unet\", \"unet\", \"unet\"]\n",
    "    \n",
    "    batch_size=16\n",
    "    weight_decay=1e-6\n",
    "    epochs=15\n",
    "    n_fold=5\n",
    "    fold=0 # [0, 1, 2, 3, 4]\n",
    "    all_fold_train = True # all fold training\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    image_size=1024 # crop size\n",
    "    crop_size=image_size\n",
    "    \n",
    "    tile_size = 1024\n",
    "    tile_step = 512\n",
    "    tile_scale = 0.5\n",
    "    dataset = f'{tile_scale}_{tile_size}_{tile_step}_train_fold'#'0.25_320_160_train_fold'\n",
    "    val_dataset = f'{tile_scale}_{tile_size}_{tile_size}_val_fold'\n",
    "    if diff_arch:\n",
    "        dir = f'{epochs}_{encoders}_{decoders}_{image_size}_{tile_size}_{tile_step}_{tile_scale}'\n",
    "    else:\n",
    "        dir = f'{epochs}_{encoder}_{decoder}_{image_size}_{tile_size}_{tile_step}_{tile_scale}' \n",
    "    # ---- optimizer, scheduler .. ---- #\n",
    "    T_max=10 # CosineAnnealingLR\n",
    "    opt =  'radam_look' # [adamw, radam_look]\n",
    "    scheduler='CosineAnnealingLR' #'MultiStepLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    loss = 'bce' # [lovasz, bce, bce_dice, dice]\n",
    "    factor=0.4 # ReduceLROnPlateau, MultiStepLR\n",
    "    patience=3 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    \n",
    "    decay_epoch = [4, 8, 12]\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "    #encoder_lr=4e-4\n",
    "    #decoder_lr=4e-4\n",
    "    start_lr = 1e-3\n",
    "    min_lr=1e-6\n",
    "    #----------------------------------#\n",
    "    \n",
    "    \n",
    "    # ----- 여러 시도 ------#\n",
    "    clf_head=False # encoder에 classfication head 붙일지 여부\n",
    "    label_smoothing = False # label smoothing 여부\n",
    "    multi_gpu=True if len(gpu)>1 else False # multi gpu 사용\n",
    "    clf_alpha = 0.3 # classification head 의 loss 비율\n",
    "    smoothing = 0.1 # label smoothing factor\n",
    "    dice_smoothing = 1 # dice loss 사용시 하이퍼 파라미터\n",
    "    \n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=42\n",
    "    \n",
    "data_dir = '/home/jeonghokim/competition/HubMap/data/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-product",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continental-community",
   "metadata": {
    "code_folding": [
     0,
     2,
     13,
     24,
     42,
     56,
     73,
     87,
     108,
     125,
     133,
     146,
     192,
     230,
     234,
     249
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#-------evaluation metric, loss---------#\n",
    "###################################\n",
    "def np_binary_cross_entropy_loss(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    #---\n",
    "    logp = -np.log(np.clip(p,1e-6,1))\n",
    "    logn = -np.log(np.clip(1-p,1e-6,1))\n",
    "    loss = t*logp +(1-t)*logn\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def np_dice_score(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "def dice_score(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "    https://catalyst-team.github.io/catalyst/_modules/catalyst/dl/utils/criterion/dice.html\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "        targets = (targets > threshold).float()\n",
    "\n",
    "    intersection = torch.sum(targets * outputs)\n",
    "    union = torch.sum(targets) + torch.sum(outputs)\n",
    "    dice = 2 * intersection / (union + eps)\n",
    "\n",
    "    return dice\n",
    "def torch_accuracy(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,):\n",
    "\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "        \n",
    "    tp = torch.sum(targets*outputs)/torch.sum(targets)\n",
    "    tn = torch.sum((1-outputs)*(1-targets))/torch.sum(1-targets)\n",
    "\n",
    "    return tp, tn\n",
    "\n",
    "def np_accuracy(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    tp = (p*t).sum()/((t).sum()+1e-7)\n",
    "    tn = ((1-p)*(1-t)).sum()/(1-t).sum()\n",
    "    return tp, tn\n",
    "\n",
    "def criterion_binary_cross_entropy(logit, mask):\n",
    "    logit = logit.reshape(-1)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logit, mask)\n",
    "    return loss\n",
    "\n",
    "# threshold dice score\n",
    "def np_dice_score2(probability, mask, threshold):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>threshold\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "# --------------------\n",
    "# Loss\n",
    "# --------------------\n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=args.smoothing):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "        \n",
    "        Dice_BCE = BCE*0.6 + dice_loss*0.4\n",
    "        \n",
    "        return Dice_BCE.mean()\n",
    "class DiceLoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=args.dice_smoothing):\n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        inputs = F.sigmoid(inputs)   \n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "                \n",
    "        return dice_loss.mean()\n",
    "    \n",
    "#PyTorch lovasz\n",
    "def symmetric_lovasz(outputs, targets):\n",
    "    return 0.5*(lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1.0 - targets))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#from torch.autograd import Function\n",
    "# copy from: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/LovaszSoftmax/lovasz_loss.py\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "class LovaszSoftmax(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(LovaszSoftmax, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def prob_flatten(self, input, target):\n",
    "        assert input.dim() in [4, 5]\n",
    "        num_class = input.size(1)\n",
    "        if input.dim() == 4:\n",
    "            input = input.permute(0, 2, 3, 1).contiguous()\n",
    "            input_flatten = input.view(-1, num_class)\n",
    "        elif input.dim() == 5:\n",
    "            input = input.permute(0, 2, 3, 4, 1).contiguous()\n",
    "            input_flatten = input.view(-1, num_class)\n",
    "        target_flatten = target.view(-1)\n",
    "        return input_flatten, target_flatten\n",
    "\n",
    "    def lovasz_softmax_flat(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "        losses = []\n",
    "        for c in range(num_classes):\n",
    "            target_c = (targets == c).float()\n",
    "            if num_classes == 1:\n",
    "                input_c = inputs[:, 0]\n",
    "            else:\n",
    "                input_c = inputs[:, c]\n",
    "            loss_c = (torch.autograd.Variable(target_c) - input_c).abs()\n",
    "            loss_c_sorted, loss_index = torch.sort(loss_c, 0, descending=True)\n",
    "            target_c_sorted = target_c[loss_index]\n",
    "            losses.append(torch.dot(loss_c_sorted, torch.autograd.Variable(lovasz_grad(target_c_sorted))))\n",
    "        losses = torch.stack(losses)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            loss = losses\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = losses.sum()\n",
    "        else:\n",
    "            loss = losses.mean()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # print(inputs.shape, targets.shape) # (batch size, class_num, x,y,z), (batch size, 1, x,y,z)\n",
    "        inputs, targets = self.prob_flatten(inputs, targets)\n",
    "        # print(inputs.shape, targets.shape)\n",
    "        losses = self.lovasz_softmax_flat(inputs, targets)\n",
    "        return losses\n",
    "class Lovasz_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lovasz_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        return LovaszSoftmax()(inputs, targets)\n",
    "###################################\n",
    "#-------ELSE function---------#\n",
    "###################################\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "def print_args(args, logger=None):\n",
    "    for k, v in vars(args).items():\n",
    "        if logger is not None:\n",
    "            logger.write('{:<16} : {}\\n'.format(k, v))\n",
    "        else:\n",
    "            print('{:<16} : {}'.format(k, v))\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "###########################\n",
    "#---- label smoothing -----\n",
    "###########################\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = args.smoothing):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = x.float().flatten()\n",
    "        target = target.float() * (1-self.smoothing) + 0.5 * self.smoothing\n",
    "        target = target.flatten()\n",
    "\n",
    "\n",
    "        loss  = F.binary_cross_entropy_with_logits(x, target, reduction='mean')\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contained-information",
   "metadata": {
    "code_folding": [
     0,
     1,
     20,
     27,
     52,
     67,
     79,
     95,
     153,
     196,
     208
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#-------masking & tile & decode---------#\n",
    "def read_tiff(image_file):\n",
    "    \"\"\"\n",
    "    *data size*\n",
    "    e.g.) (3, w, h) or (1,1,3,w,h) or (w, h, 3)  --> transform --> (w, h, 3)\n",
    "    \"\"\"\n",
    "    image = tiff.imread(image_file)\n",
    "    if image.shape[0] == 1:\n",
    "        image = image[0][0]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    elif image.shape[0] == 3:\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    return image\n",
    "\n",
    "def read_mask(mask_file):\n",
    "    mask = np.array(Image.open(mask_file))\n",
    "    return mask\n",
    "\n",
    "def read_json_as_df(json_file):\n",
    "    with open(json_file) as f:\n",
    "        j = json.load(f)\n",
    "    df = pd.json_normalize(j)\n",
    "    return df\n",
    "\n",
    "\n",
    "def draw_strcuture(df, height, width, fill=255, structure=[]):\n",
    "    mask = np.zeros((height, width), np.uint8)\n",
    "    for row in df.values:\n",
    "        type  = row[2]  #geometry.type\n",
    "        coord = row[3]  # geometry.coordinates\n",
    "        name  = row[4]   # properties.classification.name\n",
    "\n",
    "        if structure !=[]:\n",
    "            if not any(s in name for s in structure): continue\n",
    "\n",
    "\n",
    "        if type=='Polygon':\n",
    "            pt = np.array(coord).astype(np.int32)\n",
    "            #cv2.polylines(mask, [coord.reshape((-1, 1, 2))], True, 255, 1)\n",
    "            cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "        if type=='MultiPolygon':\n",
    "            for pt in coord:\n",
    "                pt = np.array(pt).astype(np.int32)\n",
    "                cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# resize, cvtcolor, generate mask\n",
    "# 원하는 object 영역만 따오는 mask\n",
    "def draw_strcuture_from_hue(image, fill=255, scale=1/32): # 0.25/32 default\n",
    "    height, width, _ = image.shape\n",
    "    vv = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = cv2.cvtColor(vv, cv2.COLOR_RGB2HSV)\n",
    "    # image_show('v[0]', v[:,:,0])\n",
    "    # image_show('v[1]', v[:,:,1])\n",
    "    # image_show('v[2]', v[:,:,2])\n",
    "    # cv2.waitKey(0)\n",
    "    mask = (vv[:, :, 1] > 32).astype(np.uint8) # rgb2hsv를 하고나서 1채널에 대해 시행하면 원하는 object만 잘따온다.\n",
    "    mask = mask*fill\n",
    "    mask = cv2.resize(mask, dsize=(width, height), interpolation=cv2.INTER_LINEAR) # 다시 원래사이즈로 복구\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --- rle ---------------------------------\n",
    "def rle_decode(rle, height, width , fill=255):\n",
    "    s = rle.split()\n",
    "    start, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    start -= 1\n",
    "    mask = np.zeros(height*width, dtype=np.uint8)\n",
    "    for i, l in zip(start, length):\n",
    "        mask[i:i+l] = fill\n",
    "    mask = mask.reshape(width,height).T\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def rle_encode(mask):\n",
    "    m = mask.T.flatten()\n",
    "    m = np.concatenate([[0], m, [0]])\n",
    "    run = np.where(m[1:] != m[:-1])[0] + 1\n",
    "    run[1::2] -= run[::2]\n",
    "    rle =  ' '.join(str(r) for r in run)\n",
    "    return rle\n",
    "\n",
    "\n",
    "# --- tile ---------------------------------\n",
    "\"\"\"\n",
    "-결국, tile_image, tile_mask만 가져다가 쓴다.\n",
    "1. scale로 resize를 하고 image size와 step만큼 건너뛰며 이미지를 만든다.\n",
    "2. 이때 일정 영역이 빈마스크면 데이터에서 제외한다.\n",
    "3. 쌓은 image와 mask를 return\n",
    "\"\"\"\n",
    "def to_tile(image, mask, structure, scale, size, step, min_score): \n",
    "    half = size//2\n",
    "    image_small = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR) # defualt는 1/4만큼 w,h를 줄인다.\n",
    "    height, width, _ = image_small.shape\n",
    "\n",
    "    #make score\n",
    "    structure_small = cv2.resize(structure, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = structure_small.astype(np.float32)/255\n",
    "\n",
    "    #make coord\n",
    "    xx = np.linspace(half, width  - half, int(np.ceil((width  - size) / step)))\n",
    "    yy = np.linspace(half, height - half, int(np.ceil((height - size) / step)))\n",
    "    xx = [int(x) for x in xx]\n",
    "    yy = [int(y) for y in yy]\n",
    "\n",
    "    coord  = []\n",
    "    reject = []\n",
    "    for cy in yy:\n",
    "        for cx in xx:\n",
    "            cv = vv[cy - half:cy + half, cx - half:cx + half].mean() # h, w // tiling한 마스크(structure)가 평균 0.25를 안넘으면 버린다.\n",
    "            if cv>min_score: # min_score ,default:0.25, 0.25의 의미?, 타일링 이미지의 1/4는 object여야 한다는 의미?\n",
    "                coord.append([cx,cy,cv])\n",
    "            else:\n",
    "                reject.append([cx,cy,cv])\n",
    "    #-----\n",
    "    if 1: # resize한 image를 tiling 하여 리스트만든다\n",
    "        tile_image = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = image_small[cy - half:cy + half, cx - half:cx + half] # resize한 image에서 indexing만 하는과정\n",
    "            assert (t.shape == (size, size, 3))\n",
    "            tile_image.append(t)\n",
    "\n",
    "    if mask is not None: # mask를 resize하고 tiling하여 리스트 만든다\n",
    "        mask_small = cv2.resize(mask, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        tile_mask = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = mask_small[cy - half:cy + half, cx - half:cx + half]\n",
    "            assert (t.shape == (size, size))\n",
    "            tile_mask.append(t)\n",
    "    else:\n",
    "        mask_small = None\n",
    "        tile_mask  = None\n",
    "\n",
    "    return {\n",
    "        'image_small': image_small,\n",
    "        'mask_small' : mask_small,\n",
    "        'structure_small' : structure_small,\n",
    "        'tile_image' : tile_image,\n",
    "        'tile_mask'  : tile_mask,\n",
    "        'coord'  : coord,\n",
    "        'reject' : reject,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "submission할때 쓰임\n",
    "\"\"\"\n",
    "def to_mask(tile, coord, height, width, scale, size, step, min_score, aggregate='mean'):\n",
    "\n",
    "    half = size//2\n",
    "    mask  = np.zeros((height, width), np.float32)\n",
    "\n",
    "    if 'mean' in aggregate:\n",
    "        w = np.ones((size,size), np.float32)\n",
    "\n",
    "        #if 'sq' in aggregate:\n",
    "        if 1:\n",
    "            #https://stackoverflow.com/questions/17190649/how-to-obtain-a-gaussian-filter-in-python\n",
    "            y,x = np.mgrid[-half:half,-half:half]\n",
    "            y = half-abs(y)\n",
    "            x = half-abs(x)\n",
    "            w = np.minimum(x,y)\n",
    "            w = w/w.max()#*2.5\n",
    "            w = np.minimum(w,1)\n",
    "\n",
    "        #--------------\n",
    "        count = np.zeros((height, width), np.float32)\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask [cy - half:cy + half, cx - half:cx + half] += tile[t]*w\n",
    "            count[cy - half:cy + half, cx - half:cx + half] += w\n",
    "               # see unet paper for \"Overlap-tile strategy for seamless segmentation of arbitrary large images\"\n",
    "        m = (count != 0)\n",
    "        mask[m] /= count[m]\n",
    "\n",
    "    if aggregate=='max':\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask[cy - half:cy + half, cx - half:cx + half] = np.maximum(\n",
    "                mask[cy - half:cy + half, cx - half:cx + half], tile[t] )\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --------------이 아래로 안씀 ------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# --draw ------------------------------------------\n",
    "\"\"\"\n",
    "경계선을 그리게 만든다, 컨투어\n",
    "하지만 안씀\n",
    "\"\"\"\n",
    "def mask_to_inner_contour(mask):\n",
    "    mask = mask>0.5\n",
    "    pad = np.lib.pad(mask, ((1, 1), (1, 1)), 'reflect')\n",
    "    contour = mask & (\n",
    "            (pad[1:-1,1:-1] != pad[:-2,1:-1]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[2:,1:-1])  \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,:-2]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,2:])\n",
    "    )\n",
    "    return contour\n",
    "\n",
    "\n",
    "def draw_contour_overlay(image, mask, color=(0,0,255), thickness=1):\n",
    "    contour =  mask_to_inner_contour(mask)\n",
    "    if thickness==1:\n",
    "        image[contour] = color\n",
    "    else:\n",
    "        r = max(1,thickness//2)\n",
    "        for y,x in np.stack(np.where(contour)).T:\n",
    "            cv2.circle(image, (x,y), r, color, lineType=cv2.LINE_4 )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-external",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intense-metro",
   "metadata": {
    "code_folding": [
     0,
     15,
     137,
     227,
     291
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------ make dataset  new version image fold--------- #\n",
    "#################################\n",
    "\"\"\"\n",
    "- robust validation을 위해 overlap 없는 데이터도 만든다\n",
    "\"\"\"\n",
    "# <todo> make difference scale tile\n",
    "\n",
    "tile_scale = 0.5\n",
    "tile_min_score = 0.25\n",
    "tile_size = 1024#320  # 480 #\n",
    "tile_average_step = 512#160 #240  # 160 #192\n",
    "tile_average_step2 = tile_size\n",
    "\n",
    "#make tile train image\n",
    "# train,tiling (image,mask) png 저장용도\n",
    "def run_make_train_tile():\n",
    "\n",
    "    train_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_train_fold' #nipa2\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "    \n",
    "    df_all = []\n",
    "    \n",
    "    os.makedirs(train_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "        # 1. image 불러오고\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        #mask = rle_decode(encoding, height, width, 255)\n",
    "        # 2. mask, target 불러오고\n",
    "        mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "        mask = read_mask(mask_file)\n",
    "        \n",
    "        # 3. 일정영역,object만 표시한 mask불러오기.\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "        print(id, mask_file)\n",
    "        \n",
    "        # make tile\n",
    "        # 4. 학습할 tile image, mask를 생성한다.\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(train_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            #s = '%s_y%08d_x%08d' % (id, cy, cx)\n",
    "            s = 'y%08d_x%08d' %(cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.png' %(id, s), tile_image)\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.mask.png' %(id, s), tile_mask)\n",
    "\n",
    "\n",
    "        df_image['tile_id']= [f'{train_tile_dir}/{id}/'+ x for x in tile_id]\n",
    "        df_all.append(df_image)\n",
    "    df_all = pd.concat(df_all, 0).reset_index(drop=True)\n",
    "    df_all[['tile_id','cx','cy','cv']].to_csv(train_tile_dir+'/image_id.csv', index=False)\n",
    "#------\n",
    "# maek tile val image\n",
    "def run_make_val_tile():\n",
    "\n",
    "    train_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step2}_val_fold' #nipa2\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "    \n",
    "    df_all = []\n",
    "    \n",
    "    os.makedirs(train_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "        # 1. image 불러오고\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        #mask = rle_decode(encoding, height, width, 255)\n",
    "        # 2. mask, target 불러오고\n",
    "        mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "        mask = read_mask(mask_file)\n",
    "        \n",
    "        # 3. 일정영역,object만 표시한 mask불러오기.\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "        print(id, mask_file)\n",
    "        \n",
    "        # make tile\n",
    "        # 4. 학습할 tile image, mask를 생성한다.\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step2, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(train_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            #s = '%s_y%08d_x%08d' % (id, cy, cx)\n",
    "            s = 'y%08d_x%08d' %(cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.png' %(id, s), tile_image)\n",
    "            cv2.imwrite(train_tile_dir + '/%s/%s.mask.png' %(id, s), tile_mask)\n",
    "\n",
    "\n",
    "        df_image['tile_id']= [f'{train_tile_dir}/{id}/'+ x for x in tile_id]\n",
    "        df_all.append(df_image)\n",
    "    df_all = pd.concat(df_all, 0).reset_index(drop=True)\n",
    "    df_all[['tile_id','cx','cy','cv']].to_csv(train_tile_dir+'/image_id.csv', index=False)\n",
    "\n",
    "    \n",
    "#make tile train image\n",
    "# test tiling image png 저장용도\n",
    "def run_make_test_tile():\n",
    "    #tile_scale = 0.25\n",
    "    #tile_min_score = 0.25\n",
    "    #tile_size = 480#320  # 480 #\n",
    "    #tile_average_step = 240#160 #240  # 160 #192\n",
    "\n",
    "    #test_tile_dir = '/home/ubuntu/gwang/hubmap/etc/tile/0.25_640_320_test'\n",
    "    test_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_test'\n",
    "    #---\n",
    "\n",
    "\n",
    "    os.makedirs(test_tile_dir, exist_ok=True)\n",
    "    assert False, 'todo modify test file'\n",
    "    for id in ['c68fe75ea','afa5e8098',]:\n",
    "        print(id)\n",
    "\n",
    "        # 1. test image load\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "        image = read_tiff(image_file)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        mask = None\n",
    "        # 2. test structure load\n",
    "        structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "        # structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "\n",
    "        # 3. test를 위한 tile image 생성\n",
    "        #make tile\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(test_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            s = 'y%08d_x%08d' % (cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            cv2.imwrite(test_tile_dir + '/%s/%s.png' % (id, s), tile_image)\n",
    "            #image_show('tile_image', tile_image)\n",
    "            #cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        df_image['tile_id']=tile_id\n",
    "        df_image[['tile_id','cx','cy','cv']].to_csv(test_tile_dir+'/%s.csv'%id, index=False)\n",
    "        #------\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "# tile이 아닌 train image의 mask생성\n",
    "def run_make_train_mask():\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        if image.shape[0]==1:\n",
    "            image = image[0][0]\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            image = np.ascontiguousarray(image)\n",
    "            height, width = image.shape[:2]\n",
    "        elif image.shape[0] == 3:\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            image = np.ascontiguousarray(image)\n",
    "            height, width = image.shape[:2]\n",
    "        else:\n",
    "            height, width = image.shape[:2]\n",
    "        mask = rle_decode(encoding, height, width, 255)\n",
    "\n",
    "        cv2.imwrite(data_dir + '/train/%s.mask.png' % id, mask)\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "def run_make_pseudo_tile():\n",
    "\n",
    "    \n",
    "    tile_scale = 0.25\n",
    "    tile_min_score = 0.25\n",
    "    tile_size = 480#320  #480 #\n",
    "    tile_average_step = 240 #160 #240  # 192\n",
    "    #---\n",
    "    pseudo_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_pseudo_0.95'\n",
    "    #df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    #df_pseudo = pd.read_csv('/root/share1/kaggle/2020/hubmap/result/resnet34/fold2/submit-fold-2-resnet34-00010000_model_lb0.837.csv')\n",
    "    df_pseudo = pd.read_csv('../../submission/0.891_submission-fold6-00004000_model_thres-0.9.csv')\n",
    "    \n",
    "    print(df_pseudo)\n",
    "    print(df_pseudo.shape)\n",
    "\n",
    "    os.makedirs(pseudo_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_pseudo)):\n",
    "        id, encoding = df_pseudo.iloc[i]\n",
    "\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        mask = rle_decode(encoding, height, width, 255)\n",
    "\n",
    "        #make tile\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "        #to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        #mask, scale, size, step, min_score\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(pseudo_tile_dir + '/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            s = 'y%08d_x%08d' % (cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(pseudo_tile_dir + '/%s/%s.png' % (id, s), tile_image)\n",
    "            cv2.imwrite(pseudo_tile_dir + '/%s/%s.mask.png' % (id, s), tile_mask)\n",
    "\n",
    "            #image_show('tile_image', tile_image)\n",
    "            #image_show('tile_mask', tile_mask)\n",
    "            #cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        df_image['tile_id']=tile_id\n",
    "        df_image[['tile_id','cx','cy','cv']].to_csv(pseudo_tile_dir+'/%s.csv'%id, index=False)\n",
    "        #------\n",
    "\n",
    "def split_fold():\n",
    "    \n",
    "    df = pd.read_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step}_train_fold/image_id.csv')\n",
    "    df2 = pd.read_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step2}_val_fold/image_id.csv')\n",
    "\n",
    "    a = {0 : '0486052bb', 1 : '095bf7a1f', 2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce', 6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', 10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4', 14 :'e79de561c'}\n",
    "    #\n",
    "    kf = KFold(n_splits=args.n_fold, random_state=args.seed, shuffle=True)\n",
    "    fold_dict={}\n",
    "    for n, (t,v) in enumerate(kf.split(a)):\n",
    "        for f in v:\n",
    "            fold_dict[a[f]] = n\n",
    "\n",
    "    df['fold'] = df['tile_id'].apply(lambda x : x.split('/')[-2])\n",
    "    df['fold'] = df['fold'].apply(lambda x :fold_dict[x])\n",
    "    \n",
    "    df2['fold'] = df2['tile_id'].apply(lambda x : x.split('/')[-2])\n",
    "    df2['fold'] = df2['fold'].apply(lambda x :fold_dict[x])\n",
    "    \n",
    "    df.to_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step}_train_fold/image_id_split.csv', index=False)\n",
    "    df2.to_csv(f'{data_dir}/tile/{tile_scale}_{tile_size}_{tile_average_step2}_val_fold/image_id_split.csv', index=False)\n",
    "    print('saved split fold')\n",
    "    \n",
    "# main #################################################################\n",
    "if 0:\n",
    "    if __name__ == '__main__':\n",
    "        #print('started run make train mask')\n",
    "        # 1.\n",
    "        print('started 1')\n",
    "        run_make_train_mask()\n",
    "        # 2.\n",
    "        print('started 2')\n",
    "        run_make_train_tile()\n",
    "        # 3.\n",
    "        print('started 3')\n",
    "        run_make_val_tile()\n",
    "        \n",
    "        #print('started 3')\n",
    "        #run_make_test_tile()\n",
    "        # 4. if use pseudo datasets\n",
    "        #run_make_pseudo_tile()\n",
    "        \n",
    "        print('split kfold csv')\n",
    "        split_fold()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-thing",
   "metadata": {},
   "source": [
    "# Dataset & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hired-updating",
   "metadata": {
    "code_folding": [
     0,
     6,
     44,
     92,
     130,
     131,
     242
    ]
   },
   "outputs": [],
   "source": [
    "#--------------- Dataset ----------------#\n",
    "##########################################\n",
    "\n",
    "#--------------- \n",
    "# Old version\n",
    "#--------------- \n",
    "def make_image_id_v1(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset_v1(Dataset):\n",
    "    def __init__(self, image_id, image_dir, augment=None):\n",
    "        self.augment = augment\n",
    "        self.image_id = image_id\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        tile_id = []\n",
    "        for i in range(len(image_dir)):\n",
    "            for id in image_id[i]: \n",
    "                df = pd.read_csv(data_dir + '/tile/%s/%s.csv'% (self.image_dir[i],id) )\n",
    "                tile_id += ('%s/%s/'%(self.image_dir[i],id) + df.tile_id).tolist()\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        string += '\\timage_dir = %s\\n'%self.image_dir\n",
    "        string += '\\timage_id  = %s\\n'%str(self.image_id)\n",
    "        string += '\\t          = %d\\n'%sum(len(i) for i in self.image_id)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(data_dir + '/tile/%s.png'%(id), cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(data_dir + '/tile/%s.mask.png'%(id), cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "\n",
    "#--------------- \n",
    "# Old version (simple fold)\n",
    "#--------------- \n",
    "def make_image_id_(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset_(Dataset):\n",
    "    def __init__(self, tile_id, augment=None):\n",
    "        self.augment = augment\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.png', cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment()\n",
    "        \n",
    "        return r\n",
    "\n",
    "#--------------- \n",
    "# New version(image fold)\n",
    "#--------------- \n",
    "def make_image_id(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "        self.augment = augment\n",
    "\n",
    "        #self.tile_id = tile_id\n",
    "        #self.len =len(self.tile_id)\n",
    "        self.df = df\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.df['tile_id'].loc[index]\n",
    "        image = cv2.imread(f'{id}.png', cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(f'{id}.mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        #if self.augment is not None: r = self.augment(image=r['image'], mask=r['mask'])\n",
    "        return r\n",
    "\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    index = []\n",
    "    mask = []\n",
    "    image = []\n",
    "    for r in batch:\n",
    "        index.append(r['index'])\n",
    "        mask.append(r['mask'])\n",
    "        image.append(r['image'])\n",
    "\n",
    "    image = np.stack(image)\n",
    "    image = image[...,::-1]\n",
    "    image = image.transpose(0,3,1,2)\n",
    "    image = np.ascontiguousarray(image)\n",
    "\n",
    "    mask  = np.stack(mask)\n",
    "    mask  = np.ascontiguousarray(mask)\n",
    "\n",
    "    #---\n",
    "    image = torch.from_numpy(image).contiguous().float()\n",
    "    mask  = torch.from_numpy(mask).contiguous().unsqueeze(1)\n",
    "    mask  = (mask>0.5).float()\n",
    "\n",
    "    return {\n",
    "        'index' : index,\n",
    "        'mask' : mask,\n",
    "        'image' : image,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "narrow-schema",
   "metadata": {
    "code_folding": [
     0,
     3,
     19,
     27,
     42,
     82,
     91,
     97,
     103,
     122
    ]
   },
   "outputs": [],
   "source": [
    "#---------- augmentation ---------------------#\n",
    "###############################################\n",
    "#flip\n",
    "def do_random_flip_transpose(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = image.transpose(1,0,2)\n",
    "        mask = mask.transpose(1,0)\n",
    "\n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return image, mask\n",
    "\n",
    "#geometric\n",
    "def do_random_crop(image, mask, size):\n",
    "    height, width = image.shape[:2]\n",
    "    x = np.random.choice(width -size)\n",
    "    y = np.random.choice(height-size)\n",
    "    image = image[y:y+size,x:x+size]\n",
    "    mask  = mask[y:y+size,x:x+size]\n",
    "    return image, mask\n",
    "\n",
    "def do_random_scale_crop(image, mask, size, mag):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    s = 1 + np.random.uniform(-1, 1)*mag\n",
    "    s =  int(s*size)\n",
    "\n",
    "    x = np.random.choice(width -s)\n",
    "    y = np.random.choice(height-s)\n",
    "    image = image[y:y+s,x:x+s]\n",
    "    mask  = mask[y:y+s,x:x+s]\n",
    "    if s!=size:\n",
    "        image = cv2.resize(image, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "        mask  = cv2.resize(mask, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rotate_crop(image, mask, size, mag=30 ):\n",
    "    angle = 1+np.random.uniform(-1, 1)*mag\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    dst = np.array([\n",
    "        [0,0],[size,size], [size,0], [0,size],\n",
    "    ])\n",
    "\n",
    "    c = np.cos(angle/180*2*PI)\n",
    "    s = np.sin(angle/180*2*PI)\n",
    "    src = (dst-size//2)@np.array([[c, -s],[s, c]]).T\n",
    "    src[:,0] -= src[:,0].min()\n",
    "    src[:,1] -= src[:,1].min()\n",
    "\n",
    "    src[:,0] = src[:,0] + np.random.uniform(0,width -src[:,0].max())\n",
    "    src[:,1] = src[:,1] + np.random.uniform(0,height-src[:,1].max())\n",
    "\n",
    "    if 0: #debug\n",
    "        def to_int(f):\n",
    "            return (int(f[0]),int(f[1]))\n",
    "\n",
    "        cv2.line(image, to_int(src[0]), to_int(src[1]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[1]), to_int(src[2]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[2]), to_int(src[3]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[3]), to_int(src[0]), (0,0,1), 16)\n",
    "        image_show_norm('image', image, min=0, max=1)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "    transform = cv2.getAffineTransform(src[:3].astype(np.float32), dst[:3].astype(np.float32))\n",
    "    image = cv2.warpAffine( image, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "    mask  = cv2.warpAffine( mask, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask\n",
    "\n",
    "#warp/elastic deform ...\n",
    "#<todo>\n",
    "\n",
    "#noise\n",
    "def do_random_noise(image, mask, mag=0.1):\n",
    "    height, width = image.shape[:2]\n",
    "    noise = np.random.uniform(-1,1, (height, width,1))*mag\n",
    "    image = image + noise\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "#intensity\n",
    "def do_random_contast(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image * alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_gain(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image ** alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h = hsv[:, :, 0].astype(np.float32)  # hue\n",
    "    s = hsv[:, :, 1].astype(np.float32)  # saturation\n",
    "    v = hsv[:, :, 2].astype(np.float32)  # value\n",
    "    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n",
    "    s =  s*(1 + random.uniform(-1,1)*mag[1])\n",
    "    v =  v*(1 + random.uniform(-1,1)*mag[2])\n",
    "\n",
    "    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n",
    "    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n",
    "    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    image = image.astype(np.float32)/255\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def filter_small(mask, min_size):\n",
    "\n",
    "    m = (mask*255).astype(np.uint8)\n",
    "\n",
    "    num_comp, comp, stat, centroid = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
    "    if num_comp==1: return mask\n",
    "\n",
    "    filtered = np.zeros(comp.shape,dtype=np.uint8)\n",
    "    area = stat[:, -1]\n",
    "    for i in range(1, num_comp):\n",
    "        if area[i] >= min_size:\n",
    "            filtered[comp == i] = 255\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-commander",
   "metadata": {
    "code_folding": [
     0,
     2,
     41,
     117
    ]
   },
   "outputs": [],
   "source": [
    "#---------- optimizer, scheduler ---------------------#\n",
    "############################################\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, alpha=0.5, k=6):\n",
    "\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "\n",
    "        self.slow_weights = [\n",
    "                [p.clone().detach() for p in group['params']]\n",
    "            for group in self.param_groups]\n",
    "\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        self.state = optimizer.state\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        loss = self.optimizer.step()\n",
    "\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(p.data - q.data, alpha=self.alpha )\n",
    "                p.data.copy_(q.data)\n",
    "        return loss\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "#---------- scheduler ---------------------#\n",
    "def get_scheduler(optimizer):\n",
    "    if args.scheduler =='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = args.epochs//args.T_0, T_mult=1, eta_min=0, last_epoch=-1)\n",
    "    elif args.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.min_lr, last_epoch=-1)\n",
    "    elif args.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=args.factor, patience=args.patience, verbose=True, \n",
    "                                      min_lr = args.min_lr, eps=args.eps)\n",
    "    else:\n",
    "        scheduler=None\n",
    "        assert False, 'not implement'\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-relief",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "established-glasgow",
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "class DOWNBLOCK(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DOWNBLOCK, self).__init__()\n",
    "        self.down_conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.down_bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.down_bn1(self.down_conv1(x)))\n",
    "        return x\n",
    "    \n",
    "def SegModel():\n",
    "    models = []\n",
    "    # 다른 모덷들일때\n",
    "    if args.diff_arch:\n",
    "        for i in range(args.n_fold):\n",
    "            en_name = args.encoders[i]\n",
    "            de_name = args.decoders[i]\n",
    "            # decoder별로 로드\n",
    "            if de_name.lower() == \"unet\":\n",
    "                if args.clf_head:\n",
    "                    print('classification head')\n",
    "                    aux_params=dict(\n",
    "                        pooling='avg',             # one of 'avg', 'max'\n",
    "                        dropout=0.5,               # dropout ratio, default is None\n",
    "                        activation='sigmoid',      # activation function, default is None\n",
    "                        classes=1,                 # define number of output labels\n",
    "                    )\n",
    "                    model = smp.Unet(\n",
    "                        encoder_name=en_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                        in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                        classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                        aux_params=aux_params\n",
    "                        )\n",
    "                else:\n",
    "                    model = smp.Unet(\n",
    "                        encoder_name=en_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                        in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                        classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                        )\n",
    "            elif de_name.lower() == \"fpn\":\n",
    "                model = smp.FPN(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            elif de_name.lower() == \"upp\":\n",
    "                model = smp.UnetPlusPlus(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            elif de_name.lower() == \"linknet\":\n",
    "                model = smp.Linknet(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            models.append(model)\n",
    "                \n",
    "        \n",
    "    # 같은 모델 일 때 5개 복사\n",
    "    else:\n",
    "        if args.encoder in ['b0','b1','b2','b3','b4','b5','b6','b7']:\n",
    "            encoder_name_ = f'efficientnet-{args.encoder}' #'timm-efficientnet-b4'\n",
    "            print('encoder : ', encoder_name_)\n",
    "        else:\n",
    "            encoder_name_ = args.encoder\n",
    "        if args.decoder =='fpn':\n",
    "            print('fpn loaded')\n",
    "            model = smp.FPN(\n",
    "                encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                )\n",
    "        elif args.decoder =='unet':\n",
    "            print('unet loaded')\n",
    "            if args.clf_head:\n",
    "                print('classification head')\n",
    "                aux_params=dict(\n",
    "                    pooling='avg',             # one of 'avg', 'max'\n",
    "                    dropout=0.5,               # dropout ratio, default is None\n",
    "                    activation='sigmoid',      # activation function, default is None\n",
    "                    classes=1,                 # define number of output labels\n",
    "                )\n",
    "                model = smp.Unet(\n",
    "                    encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                    aux_params=aux_params\n",
    "                    )\n",
    "            else:\n",
    "                model = smp.Unet(\n",
    "                    encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "                #list_ = [DOWNBLOCK(), model, nn.Upsample(size=640, mode='bilinear', align_corners=True)]\n",
    "                #model = nn.Sequential(*list_)\n",
    "        if args.encoder=='R50':\n",
    "            if args.decoder=='ViT':\n",
    "                vit_name='R50-ViT-B_16'\n",
    "                config_vit = CONFIGS[vit_name]\n",
    "                config_vit.n_classes = 1\n",
    "                config_vit.n_skip = 3\n",
    "                if vit_name.find('R50') != -1:\n",
    "                    config_vit.patches.grid = (int(args.image_size / 16), int(args.image_size / 16))\n",
    "                model = VisionTransformer(config_vit, img_size=args.image_size, num_classes=config_vit.n_classes) \n",
    "        for i in range(args.n_fold):\n",
    "            models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-participant",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inappropriate-curve",
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ------- image fold train version ------- #\n",
    "\n",
    "# augmentation\n",
    "#\"\"\"현재 crop 없는상태\"\"\"\n",
    "def train_augment(record):\n",
    "    image = record['image']\n",
    "    mask  = record['mask']\n",
    "    \n",
    "#     for fn in np.random.choice([\n",
    "#         lambda image, mask : do_random_rotate_crop(image, mask, size=args.crop_size, mag=45),\n",
    "#         lambda image, mask : do_random_scale_crop(image, mask, size=args.crop_size, mag=0.075),\n",
    "#         lambda image, mask : do_random_crop(image, mask, size=args.crop_size),\n",
    "#     ],1): image, mask = fn(image, mask)\n",
    "\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask : (image, mask),\n",
    "        lambda image, mask : do_random_contast(image, mask, mag=0.8),\n",
    "        lambda image, mask : do_random_gain(image, mask, mag=0.9),\n",
    "        #lambda image, mask : do_random_hsv(image, mask, mag=[0.1, 0.2, 0]),\n",
    "        lambda image, mask : do_random_noise(image, mask, mag=0.1),\n",
    "    ],2): image, mask =  fn(image, mask)\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    image, mask = do_random_hsv(image, mask, mag=[0.1, 0.2, 0])\n",
    "    image, mask = do_random_flip_transpose(image, mask)\n",
    "\n",
    "    record['mask'] = mask\n",
    "    record['image'] = image\n",
    "    return record\n",
    "#그냥 데이터 로더 3개 만들어서 이미지별로 각각 계산해서 평균하자..\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2, valid_probability3 = [],[],[]\n",
    "    valid_mask, valid_mask2, valid_mask3 = [],[],[]\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            batch_size = len(batch['index'])\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "            \n",
    "            if args.clf_head:\n",
    "                logit, _ = net(image) # seg, clf\n",
    "            else:\n",
    "                logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "                \n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "    #assert(valid_num == len(valid_loader.dataset)) # drop last True이면 assert되는거임\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    if args.loss =='bce':\n",
    "        loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "    elif args.loss =='lovasz':\n",
    "        loss = 0\n",
    "    \n",
    "    # mean loss, dice ..\n",
    "    dice = np_dice_score(probability, mask)\n",
    "    tp, tn = np_accuracy(probability, mask)\n",
    "\n",
    "    return [dice, loss,  tp, tn]\n",
    "\n",
    "def run_train(args):\n",
    "    out_dir = data_dir + f'/result/{args.dir}_{args.encoder}_{args.image_size}'\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint','train','valid'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "    #backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.train.txt',mode='a')\n",
    "\n",
    "    # my log argument\n",
    "    print_args(args, log)\n",
    "\n",
    "    log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "    log.write('** dataset setting **\\n')\n",
    "    #-----------dataset split --------------------#\n",
    "    tile_id = []\n",
    "    image_dir_ = f'{args.dataset}'#'0.25_320_160_train'\n",
    "    image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "    \n",
    "    image_dir_val_ = f'{args.val_dataset}'#'0.25_320_320_val'\n",
    "    image_dir_val=[image_dir_val_, ]\n",
    "    \n",
    "    for i in range(len(image_dir)):\n",
    "        df = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir[i]) )\n",
    "\n",
    "    for i in range(len(image_dir_val)):\n",
    "        df2 = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir_val[i]) )\n",
    "    df2['img_id'] = df2['tile_id'].apply(lambda x: x.split('/')[-2])\n",
    "        \n",
    "    kf = KFold(n_splits=args.n_fold, random_state=args.seed, shuffle=True)\n",
    "    all_dice = []\n",
    "    models = SegModel()\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        train_df = df[df['fold']!= n_fold].reset_index(drop=True)\n",
    "        val_df = df2[df2['fold']== n_fold].reset_index(drop=True).copy()\n",
    "        \n",
    "        # validation loader 3개 만들기 위함\n",
    "        unique_value = val_df['tile_id'].apply(lambda x: x.split('/')[-2]).unique() #[valid_id1, valid_id2, valid_id3 ]\n",
    "        val_img_id1 = unique_value[0] ; val_img_id2 = unique_value[1] ; val_img_id3= unique_value[2]\n",
    "        val_df1= val_df[val_df['img_id']==val_img_id1].reset_index(drop=True)\n",
    "        val_df2= val_df[val_df['img_id']==val_img_id2].reset_index(drop=True)\n",
    "        val_df3= val_df[val_df['img_id']==val_img_id3].reset_index(drop=True)\n",
    "        #####################################################\n",
    "        train_dataset = HuDataset(\n",
    "            df = train_df,\n",
    "            augment = train_augment\n",
    "        )\n",
    "        train_loader  = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 8,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader1\n",
    "        valid_dataset1 = HuDataset(\n",
    "            df = val_df1\n",
    "            ,\n",
    "        )\n",
    "        valid_loader1 = DataLoader(\n",
    "            valid_dataset1,\n",
    "            sampler = SequentialSampler(valid_dataset1),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader2\n",
    "        valid_dataset2 = HuDataset(\n",
    "            df = val_df2\n",
    "            ,\n",
    "        )\n",
    "        \n",
    "        valid_loader2 = DataLoader(\n",
    "            valid_dataset2,\n",
    "            sampler = SequentialSampler(valid_dataset2),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader3\n",
    "        valid_dataset3 = HuDataset(\n",
    "            df = val_df3\n",
    "            ,\n",
    "        )\n",
    "        valid_loader3 = DataLoader(\n",
    "            valid_dataset3,\n",
    "            sampler = SequentialSampler(valid_dataset3),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        log.write('fold = %s\\n'%str(n_fold))\n",
    "        log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "        log.write('valid_dataset1 : \\n%s\\n'%(valid_dataset1))\n",
    "        log.write('valid_dataset2 : \\n%s\\n'%(valid_dataset2))\n",
    "        log.write('valid_dataset3 : \\n%s\\n'%(valid_dataset3))\n",
    "        log.write('\\n')\n",
    "\n",
    "        # ------------------------\n",
    "        #  Model\n",
    "        # ------------------------\n",
    "        log.write('** net setting **\\n')\n",
    "\n",
    "        scaler = GradScaler()\n",
    "        models = SegModel()\n",
    "        \n",
    "        net = models[n_fold]\n",
    "        net = net.to(device)\n",
    "        if args.multi_gpu:\n",
    "            log.write('multi gpu')\n",
    "            net = nn.DataParallel(net)\n",
    "        \n",
    "        \n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        if args.opt =='adamw':\n",
    "            optimizer = torch.optim.AdamW(net.parameters(), lr = args.start_lr)\n",
    "\n",
    "        elif args.opt =='radam_look':\n",
    "            optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=args.start_lr), alpha=0.5, k=5)\n",
    "        if optimizer == None:\n",
    "            assert False, 'no have optimizer'\n",
    "        \n",
    "        # ------------------------\n",
    "        #  scheduler\n",
    "        # ------------------------\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "\n",
    "        log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "        #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "        log.write('\\n')\n",
    "\n",
    "        ## start training here! ##############################################\n",
    "        #array([0.57142857, 0.42857143])\n",
    "        log.write('** start training here! **\\n')\n",
    "        log.write('   is_mixed_precision = %s \\n'%str(args.amp))\n",
    "        log.write('   batch_size = %d \\n'%(args.batch_size))\n",
    "        log.write('             |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n",
    "        log.write('rate  epoch  | dice   loss   tp     tn     | loss           | time           \\n')\n",
    "        log.write('-------------------------------------------------------------------------------------\\n')\n",
    "                  #0.00100   0.50  0.80 | 0.891  0.020  0.000  0.000  | 0.000  0.000   |  0 hr 02 min\n",
    "\n",
    "        def message(mode='print'):\n",
    "            if mode==('print'):\n",
    "                asterisk = ' '\n",
    "                loss = batch_loss\n",
    "            if mode==('log'):\n",
    "                asterisk = '*'\n",
    "                loss = train_loss\n",
    "\n",
    "            text = \\\n",
    "                '%0.5f  %s%s    | '%(rate, epoch, asterisk,) +\\\n",
    "                '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n",
    "                '%4.3f  %4.3f   | '%(*loss,) +\\\n",
    "                '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "\n",
    "            return text\n",
    "\n",
    "        #----\n",
    "        valid_loss = np.zeros(4,np.float32)\n",
    "        train_loss = np.zeros(2,np.float32)\n",
    "        batch_loss = np.zeros_like(train_loss)\n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = 0\n",
    "        loss = torch.FloatTensor([0]).sum()\n",
    "\n",
    "\n",
    "        start_timer = timer()\n",
    "        rate = 0\n",
    "        best_dice = 0\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            #print('\\r',end='',flush=True)\n",
    "            #log.write(message(mode='log')+'\\n')\n",
    "            # training\n",
    "            for t, batch in enumerate(train_loader):\n",
    "\n",
    "                # learning rate schduler -------------\n",
    "                #adjust_learning_rate(optimizer, schduler(iteration))\n",
    "                rate = get_learning_rate(optimizer)\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                batch_size = len(batch['index'])\n",
    "                net.train()\n",
    "\n",
    "                if args.amp:\n",
    "                    #image = image.half()\n",
    "                    with autocast():\n",
    "                        mask  = batch['mask'].to(device)\n",
    "                        image = batch['image'].to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        #logit = data_parallel(net, image)\n",
    "                        if args.clf_head:\n",
    "                            logit, logit2 = net(image) # seg logit, clf logit\n",
    "                        else:\n",
    "                            logit = net(image)\n",
    "                        if args.loss == 'bce':\n",
    "                            if args.label_smoothing:\n",
    "                                loss = LabelSmoothing()(logit, mask)\n",
    "                            else:\n",
    "                                loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                            if args.clf_head:\n",
    "                                loss += args.clf_alpha *nn.BCEWithLogitsLoss()(logit2, (mask.sum(dim=(2,3))>0).float() )\n",
    "                        elif args.loss =='lovasz':\n",
    "                            #loss = LovaszHingeLoss()(logit, mask)\n",
    "                            loss = symmetric_lovasz(logit, mask)\n",
    "                            \n",
    "                        elif args.loss == 'bce_dice':\n",
    "                            loss = DiceBCELoss()(logit, mask)\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else :\n",
    "                    mask  = batch['mask'].to(device)\n",
    "                    image = batch['image'].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    #logit = data_parallel(net, image)\n",
    "                    logit = net(image)\n",
    "                    if args.loss == 'bce':\n",
    "                        loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                    elif args.loss =='lovasz':\n",
    "                        loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # print statistics  --------\n",
    "\n",
    "                batch_loss = np.array([ loss.item(), 0 ])\n",
    "                sum_train_loss += batch_loss\n",
    "                sum_train += 1\n",
    "\n",
    "                #print('\\r',end='',flush=True)\n",
    "                #print(message(mode='print'), end='',flush=True)\n",
    "            \n",
    "\n",
    "            # train loss\n",
    "            train_loss = sum_train_loss/(sum_train+1e-12)\n",
    "            sum_train_loss[...] = 0\n",
    "            sum_train = 0\n",
    "            print(\"do valid...\")\n",
    "            # scheudler\n",
    "            valid_loss1 = do_valid(net, valid_loader1) #\n",
    "            valid_loss2 = do_valid(net, valid_loader2)\n",
    "            valid_loss3 = do_valid(net, valid_loader3)\n",
    "            valid_loss = (np.array(valid_loss1) + np.array(valid_loss2) + np.array(valid_loss3))/3\n",
    "            \n",
    "            log.write(message(mode='log')+'\\n')\n",
    "            log.write(f'{val_img_id1} dice : {valid_loss1[0]:.5f}, {val_img_id2} dice : {valid_loss2[0]:.5f}, {val_img_id3} dice : {valid_loss3[0]:.5f}\\n')\n",
    "            \n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(valid_loss[0])\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # saved models\n",
    "            #if valid_loss[0] > best_dice:\n",
    "            if valid_loss[0] > best_dice:\n",
    "                best_dice = valid_loss[0]\n",
    "                log.write(f'\\n saved best models, dice:{best_dice:.5f}\\n')\n",
    "                torch.save({\n",
    "                    'state_dict': net.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                }, out_dir + f'/checkpoint/{n_fold}fold_{epoch}epoch_{best_dice:.4f}_{args.encoders[n_fold]}_{args.decoders[n_fold]}model.pth')\n",
    "            \n",
    "            log.write('='*80+'\\n')\n",
    "\n",
    "        log.write('\\n')\n",
    "        \n",
    "        all_dice.append(best_dice)\n",
    "    \n",
    "    print(f'all dice score : {sum(all_dice)/len(all_dice) : .4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chief-lexington",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__module__       : __main__\n",
      "amp              : True\n",
      "gpu              : 0,1,7\n",
      "encoder          : b4\n",
      "decoder          : unet\n",
      "diff_arch        : True\n",
      "encoders         : ['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']\n",
      "decoders         : ['unet', 'unet', 'unet', 'unet', 'unet']\n",
      "batch_size       : 16\n",
      "weight_decay     : 1e-06\n",
      "epochs           : 15\n",
      "n_fold           : 5\n",
      "fold             : 0\n",
      "all_fold_train   : True\n",
      "image_size       : 1024\n",
      "crop_size        : 1024\n",
      "tile_size        : 1024\n",
      "tile_step        : 512\n",
      "tile_scale       : 0.5\n",
      "dataset          : 0.5_1024_512_train_fold\n",
      "val_dataset      : 0.5_1024_1024_val_fold\n",
      "dir              : 15_['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']_['unet', 'unet', 'unet', 'unet', 'unet']_1024_1024_512_0.5\n",
      "T_max            : 10\n",
      "opt              : radam_look\n",
      "scheduler        : CosineAnnealingLR\n",
      "loss             : bce\n",
      "factor           : 0.4\n",
      "patience         : 3\n",
      "eps              : 1e-06\n",
      "decay_epoch      : [4, 8, 12]\n",
      "T_0              : 4\n",
      "start_lr         : 0.001\n",
      "min_lr           : 1e-06\n",
      "clf_head         : False\n",
      "label_smoothing  : False\n",
      "multi_gpu        : True\n",
      "clf_alpha        : 0.3\n",
      "smoothing        : 0.1\n",
      "dice_smoothing   : 1\n",
      "num_workers      : 8\n",
      "seed             : 42\n",
      "__dict__         : <attribute '__dict__' of 'args' objects>\n",
      "__weakref__      : <attribute '__weakref__' of 'args' objects>\n",
      "__doc__          : None\n",
      "\tout_dir  = /home/jeonghokim/competition/HubMap/data//result/15_['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']_['unet', 'unet', 'unet', 'unet', 'unet']_1024_1024_512_0.5_b4_1024\n",
      "\n",
      "** dataset setting **\n",
      "fold = 0\n",
      "train_dataset : \n",
      "\tlen  = 6559\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 172\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 78\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 253\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.924  0.037  0.940  0.997  | 0.172  0.000   |  0 hr 07 min\n",
      "b9a3865fc dice : 0.93208, 0486052bb dice : 0.94035, afa5e8098 dice : 0.89876\n",
      "\n",
      " saved best models, dice:0.92373\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.928  0.016  0.938  0.998  | 0.028  0.000   |  0 hr 15 min\n",
      "b9a3865fc dice : 0.93742, 0486052bb dice : 0.94162, afa5e8098 dice : 0.90386\n",
      "\n",
      " saved best models, dice:0.92763\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.929  0.012  0.953  0.997  | 0.016  0.000   |  0 hr 22 min\n",
      "b9a3865fc dice : 0.93664, 0486052bb dice : 0.94321, afa5e8098 dice : 0.90854\n",
      "\n",
      " saved best models, dice:0.92946\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.929  0.013  0.951  0.997  | 0.014  0.000   |  0 hr 29 min\n",
      "b9a3865fc dice : 0.93556, 0486052bb dice : 0.94211, afa5e8098 dice : 0.90788\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.929  0.012  0.956  0.997  | 0.012  0.000   |  0 hr 37 min\n",
      "b9a3865fc dice : 0.94015, 0486052bb dice : 0.94277, afa5e8098 dice : 0.90474\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.930  0.013  0.958  0.997  | 0.011  0.000   |  0 hr 44 min\n",
      "b9a3865fc dice : 0.93993, 0486052bb dice : 0.94428, afa5e8098 dice : 0.90539\n",
      "\n",
      " saved best models, dice:0.92987\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.933  0.012  0.956  0.997  | 0.010  0.000   |  0 hr 52 min\n",
      "b9a3865fc dice : 0.94183, 0486052bb dice : 0.94490, afa5e8098 dice : 0.91343\n",
      "\n",
      " saved best models, dice:0.93339\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.934  0.011  0.952  0.998  | 0.009  0.000   |  0 hr 59 min\n",
      "b9a3865fc dice : 0.94221, 0486052bb dice : 0.94650, afa5e8098 dice : 0.91290\n",
      "\n",
      " saved best models, dice:0.93387\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.934  0.011  0.959  0.997  | 0.009  0.000   |  1 hr 07 min\n",
      "b9a3865fc dice : 0.94149, 0486052bb dice : 0.94737, afa5e8098 dice : 0.91413\n",
      "\n",
      " saved best models, dice:0.93433\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.934  0.012  0.957  0.997  | 0.008  0.000   |  1 hr 14 min\n",
      "b9a3865fc dice : 0.94252, 0486052bb dice : 0.94685, afa5e8098 dice : 0.91216\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.933  0.012  0.957  0.997  | 0.008  0.000   |  1 hr 22 min\n",
      "b9a3865fc dice : 0.94257, 0486052bb dice : 0.94717, afa5e8098 dice : 0.90980\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.934  0.012  0.956  0.997  | 0.008  0.000   |  1 hr 29 min\n",
      "b9a3865fc dice : 0.94227, 0486052bb dice : 0.94722, afa5e8098 dice : 0.91210\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.932  0.013  0.961  0.997  | 0.008  0.000   |  1 hr 36 min\n",
      "b9a3865fc dice : 0.94072, 0486052bb dice : 0.94466, afa5e8098 dice : 0.91058\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.933  0.013  0.960  0.997  | 0.008  0.000   |  1 hr 43 min\n",
      "b9a3865fc dice : 0.94224, 0486052bb dice : 0.94687, afa5e8098 dice : 0.90946\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.930  0.012  0.949  0.997  | 0.009  0.000   |  1 hr 49 min\n",
      "b9a3865fc dice : 0.94395, 0486052bb dice : 0.94346, afa5e8098 dice : 0.90233\n",
      "================================================================================\n",
      "\n",
      "fold = 1\n",
      "train_dataset : \n",
      "\tlen  = 6626\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 29\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 186\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 270\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.918  0.037  0.924  0.997  | 0.137  0.000   |  0 hr 06 min\n",
      "aaa6a05cc dice : 0.89612, cb2d976f4 dice : 0.93130, 4ef6695ce dice : 0.92741\n",
      "\n",
      " saved best models, dice:0.91828\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.924  0.018  0.919  0.998  | 0.025  0.000   |  0 hr 12 min\n",
      "aaa6a05cc dice : 0.90169, cb2d976f4 dice : 0.94022, 4ef6695ce dice : 0.93024\n",
      "\n",
      " saved best models, dice:0.92405\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.932  0.014  0.925  0.998  | 0.015  0.000   |  0 hr 19 min\n",
      "aaa6a05cc dice : 0.91439, cb2d976f4 dice : 0.94413, 4ef6695ce dice : 0.93674\n",
      "\n",
      " saved best models, dice:0.93176\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.916  0.015  0.880  0.999  | 0.013  0.000   |  0 hr 27 min\n",
      "aaa6a05cc dice : 0.88923, cb2d976f4 dice : 0.93224, 4ef6695ce dice : 0.92727\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.932  0.013  0.944  0.997  | 0.013  0.000   |  0 hr 33 min\n",
      "aaa6a05cc dice : 0.91709, cb2d976f4 dice : 0.93900, 4ef6695ce dice : 0.93921\n",
      "\n",
      " saved best models, dice:0.93177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.936  0.012  0.932  0.998  | 0.011  0.000   |  0 hr 40 min\n",
      "aaa6a05cc dice : 0.92297, cb2d976f4 dice : 0.94314, 4ef6695ce dice : 0.94249\n",
      "\n",
      " saved best models, dice:0.93620\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.935  0.012  0.928  0.998  | 0.010  0.000   |  0 hr 46 min\n",
      "aaa6a05cc dice : 0.92311, cb2d976f4 dice : 0.94166, 4ef6695ce dice : 0.94146\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.931  0.013  0.921  0.998  | 0.009  0.000   |  0 hr 53 min\n",
      "aaa6a05cc dice : 0.91924, cb2d976f4 dice : 0.93894, 4ef6695ce dice : 0.93613\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.934  0.012  0.925  0.998  | 0.009  0.000   |  0 hr 59 min\n",
      "aaa6a05cc dice : 0.92139, cb2d976f4 dice : 0.94065, 4ef6695ce dice : 0.93959\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.937  0.012  0.939  0.998  | 0.008  0.000   |  1 hr 08 min\n",
      "aaa6a05cc dice : 0.92252, cb2d976f4 dice : 0.94431, 4ef6695ce dice : 0.94335\n",
      "\n",
      " saved best models, dice:0.93673\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.937  0.012  0.941  0.998  | 0.008  0.000   |  1 hr 14 min\n",
      "aaa6a05cc dice : 0.92368, cb2d976f4 dice : 0.94412, 4ef6695ce dice : 0.94335\n",
      "\n",
      " saved best models, dice:0.93705\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.936  0.012  0.943  0.997  | 0.008  0.000   |  1 hr 21 min\n",
      "aaa6a05cc dice : 0.91997, cb2d976f4 dice : 0.94443, 4ef6695ce dice : 0.94222\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.935  0.012  0.930  0.998  | 0.008  0.000   |  1 hr 28 min\n",
      "aaa6a05cc dice : 0.92231, cb2d976f4 dice : 0.94372, 4ef6695ce dice : 0.94018\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.933  0.013  0.925  0.998  | 0.008  0.000   |  1 hr 34 min\n",
      "aaa6a05cc dice : 0.92008, cb2d976f4 dice : 0.94020, 4ef6695ce dice : 0.93839\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.937  0.012  0.943  0.998  | 0.009  0.000   |  1 hr 51 min\n",
      "aaa6a05cc dice : 0.92491, cb2d976f4 dice : 0.94768, 4ef6695ce dice : 0.93922\n",
      "\n",
      " saved best models, dice:0.93727\n",
      "================================================================================\n",
      "\n",
      "fold = 2\n",
      "train_dataset : \n",
      "\tlen  = 7178\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 60\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 152\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 137\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.853  0.152  0.937  0.987  | 0.447  0.000   |  0 hr 06 min\n",
      "e79de561c dice : 0.85241, 095bf7a1f dice : 0.85942, 1e2425f28 dice : 0.84622\n",
      "\n",
      " saved best models, dice:0.85268\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.926  0.045  0.926  0.997  | 0.085  0.000   |  0 hr 13 min\n",
      "e79de561c dice : 0.92910, 095bf7a1f dice : 0.92493, 1e2425f28 dice : 0.92514\n",
      "\n",
      " saved best models, dice:0.92639\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.932  0.028  0.939  0.996  | 0.033  0.000   |  0 hr 20 min\n",
      "e79de561c dice : 0.93324, 095bf7a1f dice : 0.92975, 1e2425f28 dice : 0.93181\n",
      "\n",
      " saved best models, dice:0.93160\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.933  0.022  0.947  0.996  | 0.020  0.000   |  0 hr 27 min\n",
      "e79de561c dice : 0.93388, 095bf7a1f dice : 0.93102, 1e2425f28 dice : 0.93385\n",
      "\n",
      " saved best models, dice:0.93292\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.936  0.019  0.933  0.997  | 0.015  0.000   |  0 hr 34 min\n",
      "e79de561c dice : 0.93603, 095bf7a1f dice : 0.93417, 1e2425f28 dice : 0.93867\n",
      "\n",
      " saved best models, dice:0.93629\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.934  0.019  0.940  0.997  | 0.012  0.000   |  0 hr 42 min\n",
      "e79de561c dice : 0.93096, 095bf7a1f dice : 0.93598, 1e2425f28 dice : 0.93465\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.933  0.018  0.938  0.997  | 0.011  0.000   |  0 hr 49 min\n",
      "e79de561c dice : 0.93057, 095bf7a1f dice : 0.93362, 1e2425f28 dice : 0.93532\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.934  0.018  0.940  0.997  | 0.010  0.000   |  0 hr 56 min\n",
      "e79de561c dice : 0.93088, 095bf7a1f dice : 0.93633, 1e2425f28 dice : 0.93431\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.934  0.018  0.932  0.997  | 0.009  0.000   |  1 hr 03 min\n",
      "e79de561c dice : 0.93103, 095bf7a1f dice : 0.93467, 1e2425f28 dice : 0.93610\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.935  0.018  0.933  0.997  | 0.009  0.000   |  1 hr 10 min\n",
      "e79de561c dice : 0.93240, 095bf7a1f dice : 0.93665, 1e2425f28 dice : 0.93490\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.935  0.018  0.937  0.997  | 0.009  0.000   |  1 hr 19 min\n",
      "e79de561c dice : 0.93306, 095bf7a1f dice : 0.93662, 1e2425f28 dice : 0.93474\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.934  0.018  0.934  0.997  | 0.009  0.000   |  1 hr 26 min\n",
      "e79de561c dice : 0.92999, 095bf7a1f dice : 0.93652, 1e2425f28 dice : 0.93491\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.931  0.019  0.923  0.997  | 0.009  0.000   |  1 hr 33 min\n",
      "e79de561c dice : 0.92065, 095bf7a1f dice : 0.93702, 1e2425f28 dice : 0.93504\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.935  0.018  0.934  0.997  | 0.009  0.000   |  1 hr 40 min\n",
      "e79de561c dice : 0.93374, 095bf7a1f dice : 0.93582, 1e2425f28 dice : 0.93633\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.930  0.018  0.918  0.997  | 0.009  0.000   |  1 hr 47 min\n",
      "e79de561c dice : 0.93202, 095bf7a1f dice : 0.93254, 1e2425f28 dice : 0.92640\n",
      "================================================================================\n",
      "\n",
      "fold = 3\n",
      "train_dataset : \n",
      "\tlen  = 7425\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 71\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 176\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 44\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.910  0.080  0.851  0.999  | 0.310  0.000   |  0 hr 07 min\n",
      "2f6ecfcdf dice : 0.90693, 8242609fa dice : 0.93052, b2dc8411c dice : 0.89341\n",
      "\n",
      " saved best models, dice:0.91029\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.942  0.021  0.918  0.999  | 0.045  0.000   |  0 hr 14 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2f6ecfcdf dice : 0.94170, 8242609fa dice : 0.94866, b2dc8411c dice : 0.93461\n",
      "\n",
      " saved best models, dice:0.94166\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.914  0.017  0.854  1.000  | 0.021  0.000   |  0 hr 21 min\n",
      "2f6ecfcdf dice : 0.91227, 8242609fa dice : 0.93815, b2dc8411c dice : 0.89189\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.945  0.012  0.928  0.999  | 0.016  0.000   |  0 hr 28 min\n",
      "2f6ecfcdf dice : 0.94330, 8242609fa dice : 0.95065, b2dc8411c dice : 0.94075\n",
      "\n",
      " saved best models, dice:0.94490\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.944  0.010  0.920  0.999  | 0.013  0.000   |  0 hr 35 min\n",
      "2f6ecfcdf dice : 0.94079, 8242609fa dice : 0.95304, b2dc8411c dice : 0.93732\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.948  0.009  0.925  0.999  | 0.011  0.000   |  0 hr 41 min\n",
      "2f6ecfcdf dice : 0.94899, 8242609fa dice : 0.95413, b2dc8411c dice : 0.93998\n",
      "\n",
      " saved best models, dice:0.94770\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.947  0.009  0.926  0.999  | 0.011  0.000   |  0 hr 49 min\n",
      "2f6ecfcdf dice : 0.94615, 8242609fa dice : 0.95476, b2dc8411c dice : 0.94100\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.949  0.009  0.926  0.999  | 0.010  0.000   |  0 hr 56 min\n",
      "2f6ecfcdf dice : 0.94882, 8242609fa dice : 0.95411, b2dc8411c dice : 0.94276\n",
      "\n",
      " saved best models, dice:0.94857\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.950  0.009  0.935  0.999  | 0.009  0.000   |  1 hr 03 min\n",
      "2f6ecfcdf dice : 0.95185, 8242609fa dice : 0.95537, b2dc8411c dice : 0.94407\n",
      "\n",
      " saved best models, dice:0.95043\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.950  0.009  0.927  0.999  | 0.009  0.000   |  1 hr 10 min\n",
      "2f6ecfcdf dice : 0.95115, 8242609fa dice : 0.95540, b2dc8411c dice : 0.94274\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.950  0.009  0.928  0.999  | 0.009  0.000   |  1 hr 17 min\n",
      "2f6ecfcdf dice : 0.95100, 8242609fa dice : 0.95536, b2dc8411c dice : 0.94280\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.952  0.008  0.934  0.999  | 0.009  0.000   |  1 hr 25 min\n",
      "2f6ecfcdf dice : 0.95314, 8242609fa dice : 0.95685, b2dc8411c dice : 0.94545\n",
      "\n",
      " saved best models, dice:0.95181\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.950  0.009  0.928  0.999  | 0.009  0.000   |  1 hr 32 min\n",
      "2f6ecfcdf dice : 0.95203, 8242609fa dice : 0.95362, b2dc8411c dice : 0.94318\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.950  0.008  0.935  0.999  | 0.009  0.000   |  1 hr 39 min\n",
      "2f6ecfcdf dice : 0.95100, 8242609fa dice : 0.95481, b2dc8411c dice : 0.94513\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.932  0.011  0.890  0.999  | 0.009  0.000   |  1 hr 46 min\n",
      "2f6ecfcdf dice : 0.93238, 8242609fa dice : 0.94582, b2dc8411c dice : 0.91849\n",
      "================================================================================\n",
      "\n",
      "fold = 4\n",
      "train_dataset : \n",
      "\tlen  = 6700\n",
      "\n",
      "valid_dataset1 : \n",
      "\tlen  = 62\n",
      "\n",
      "valid_dataset2 : \n",
      "\tlen  = 175\n",
      "\n",
      "valid_dataset3 : \n",
      "\tlen  = 227\n",
      "\n",
      "\n",
      "** net setting **\n",
      "multi gpuoptimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "do valid...\n",
      "0.00100  1*    | 0.903  0.093  0.895  0.998  | 0.337  0.000   |  0 hr 06 min\n",
      "54f2eec69 dice : 0.91034, 26dc41664 dice : 0.92399, c68fe75ea dice : 0.87474\n",
      "\n",
      " saved best models, dice:0.90302\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00098  2*    | 0.916  0.029  0.914  0.998  | 0.055  0.000   |  0 hr 13 min\n",
      "54f2eec69 dice : 0.92581, 26dc41664 dice : 0.93565, c68fe75ea dice : 0.88790\n",
      "\n",
      " saved best models, dice:0.91645\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00090  3*    | 0.914  0.019  0.929  0.997  | 0.024  0.000   |  0 hr 19 min\n",
      "54f2eec69 dice : 0.92503, 26dc41664 dice : 0.93713, c68fe75ea dice : 0.88043\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00079  4*    | 0.921  0.015  0.933  0.997  | 0.017  0.000   |  0 hr 26 min\n",
      "54f2eec69 dice : 0.92481, 26dc41664 dice : 0.94678, c68fe75ea dice : 0.89258\n",
      "\n",
      " saved best models, dice:0.92139\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00065  5*    | 0.923  0.014  0.922  0.998  | 0.014  0.000   |  0 hr 32 min\n",
      "54f2eec69 dice : 0.92622, 26dc41664 dice : 0.94518, c68fe75ea dice : 0.89769\n",
      "\n",
      " saved best models, dice:0.92303\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00050  6*    | 0.916  0.014  0.899  0.998  | 0.011  0.000   |  0 hr 39 min\n",
      "54f2eec69 dice : 0.92031, 26dc41664 dice : 0.94221, c68fe75ea dice : 0.88614\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  7*    | 0.923  0.013  0.908  0.998  | 0.010  0.000   |  0 hr 45 min\n",
      "54f2eec69 dice : 0.92518, 26dc41664 dice : 0.94328, c68fe75ea dice : 0.90077\n",
      "\n",
      " saved best models, dice:0.92307\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  8*    | 0.925  0.013  0.934  0.998  | 0.010  0.000   |  0 hr 53 min\n",
      "54f2eec69 dice : 0.93108, 26dc41664 dice : 0.94888, c68fe75ea dice : 0.89578\n",
      "\n",
      " saved best models, dice:0.92524\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  9*    | 0.924  0.013  0.921  0.998  | 0.009  0.000   |  0 hr 59 min\n",
      "54f2eec69 dice : 0.92488, 26dc41664 dice : 0.94738, c68fe75ea dice : 0.90067\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  10*    | 0.925  0.013  0.928  0.998  | 0.009  0.000   |  1 hr 06 min\n",
      "54f2eec69 dice : 0.92905, 26dc41664 dice : 0.94921, c68fe75ea dice : 0.89767\n",
      "\n",
      " saved best models, dice:0.92531\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00000  11*    | 0.925  0.013  0.928  0.998  | 0.009  0.000   |  1 hr 12 min\n",
      "54f2eec69 dice : 0.92844, 26dc41664 dice : 0.94904, c68fe75ea dice : 0.89793\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00003  12*    | 0.925  0.013  0.925  0.998  | 0.009  0.000   |  1 hr 19 min\n",
      "54f2eec69 dice : 0.92573, 26dc41664 dice : 0.94816, c68fe75ea dice : 0.90004\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00010  13*    | 0.923  0.013  0.932  0.998  | 0.009  0.000   |  1 hr 29 min\n",
      "54f2eec69 dice : 0.92832, 26dc41664 dice : 0.94951, c68fe75ea dice : 0.89213\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00021  14*    | 0.924  0.013  0.937  0.997  | 0.009  0.000   |  1 hr 36 min\n",
      "54f2eec69 dice : 0.92821, 26dc41664 dice : 0.94824, c68fe75ea dice : 0.89475\n",
      "================================================================================\n",
      "do valid...\n",
      "0.00035  15*    | 0.922  0.013  0.924  0.998  | 0.009  0.000   |  1 hr 42 min\n",
      "54f2eec69 dice : 0.92704, 26dc41664 dice : 0.94652, c68fe75ea dice : 0.89167\n",
      "================================================================================\n",
      "\n",
      "all dice score :  0.9370\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set seed\n",
    "    print('no set seed') if args.seed ==-1 else set_seeds(seed=args.seed)\n",
    "    run_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-influence",
   "metadata": {},
   "source": [
    "# validation 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-factory",
   "metadata": {},
   "source": [
    "eval mode : 모델들 불러와서 validation에 해당하는 이미지 예측후 cv측정, threshold별 dice 계산\n",
    "\n",
    "gen_image : validation에 해당하는 이미지 예측후 visualize(저장된 이미지로 확인가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "antique-ready",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    server ='local' # ['kaggle', 'local'] local은 cv측정용도\n",
    "    amp = False\n",
    "    gpu = 1\n",
    "    \n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    n_fold = 5\n",
    "    diff_arch = True\n",
    "    encoders = [\"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\"]\n",
    "    decoders = [\"unet\", \"unet\", \"unet\", \"unet\", \"unet\"]\n",
    "    batch_size=16\n",
    "    #fold=0\n",
    "    mode = 'eval' # ['eval', 'gen_image']\n",
    "    loss = 'bce'\n",
    "    clf_head=False\n",
    "    dataset = '0.5_1024_512_train_fold'#'[0.25_256_128_train', '0.25_480_240_train' ]# dataset size\n",
    "    val_dataset = '0.5_1024_1024_val_fold'\n",
    "    \n",
    "    model_path = [\"./data/result/15_['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']_['unet', 'unet', 'unet', 'unet', 'unet']_1024_1024_512_0.5_b4_1024\" +\n",
    "                  \"/checkpoint/\" + x for x in \\\n",
    "                 ['0fold_9epoch_0.9343_efficientnet-b4_unetmodel.pth','1fold_15epoch_0.9373_efficientnet-b4_unetmodel.pth',\n",
    "                 '2fold_5epoch_0.9363_efficientnet-b4_unetmodel.pth','3fold_12epoch_0.9518_efficientnet-b4_unetmodel.pth',\n",
    "                 '4fold_10epoch_0.9253_efficientnet-b4_unetmodel.pth']]\n",
    "    \n",
    "    sub = '[visualize][04.05]_0.9337_models'# submission name\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    \n",
    "    tile_size = 1024\n",
    "    tile_average_step = 512\n",
    "    tile_scale = 0.5\n",
    "    tile_min_score = 0.25  \n",
    "\n",
    "#assert args.server!='local', 'not implement'\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "round-stranger",
   "metadata": {
    "code_folding": [
     2,
     41,
     189,
     199,
     204,
     214,
     218
    ]
   },
   "outputs": [],
   "source": [
    "all_dice_dict={}\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2, valid_probability3 = [],[],[]\n",
    "    valid_mask, valid_mask2, valid_mask3 = [],[],[]\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "            \n",
    "            if args.clf_head:\n",
    "                logit, _ = net(image) # seg, clf\n",
    "            else:\n",
    "                logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "                \n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "    #assert(valid_num == len(valid_loader.dataset)) # drop last True이면 assert되는거임\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    if args.loss =='bce':\n",
    "        loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "    elif args.loss =='lovasz':\n",
    "        loss = 0\n",
    "    \n",
    "    dice = [np_dice_score2(probability, mask, round(th, 2)) for th in np.arange(0.1, 0.7, 0.05)]\n",
    "    #tp, tn = np_accuracy(probability, mask)\n",
    "\n",
    "    return np.array(dice)#[dice_dict, loss,  tp, tn]\n",
    "\n",
    "\n",
    "def gen_val_image(args):\n",
    "    out_dir = args.model_path[0].split('checkpoint')[0]\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint','train','valid','backup'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.val.txt',mode='a')\n",
    "\n",
    "    # my log argument\n",
    "    print_args(args, log)\n",
    "\n",
    "    submit_dir = out_dir + '/valid/%s-mean'%(args.server)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    #\n",
    "    for fold in range(5):\n",
    "        scaler = GradScaler()\n",
    "        net = SegModel() \n",
    "        net = net.to(device)\n",
    "        state_dict = torch.load(args.model_path[fold], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        net.load_state_dict(state_dict,strict=True)  #True\n",
    "        net = net.eval()\n",
    "\n",
    "        #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "        log.write('\\n')\n",
    "\n",
    "        #----      \n",
    "\n",
    "        # make validation predict images\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "        #\n",
    "        a = pd.read_csv('../hubmap/tile/0.25_320_160_train_fold/image_id_split.csv')\n",
    "        b = a[a['fold']==fold]\n",
    "        valid_image_id = b['tile_id'].apply(lambda x : x.split('/')[-2]).unique()\n",
    "\n",
    "        #\n",
    "        start_timer = timer()\n",
    "        for id in valid_image_id:\n",
    "            image_file = data_dir + '/train/%s.tiff' % id\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "            structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "            mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "            mask  = read_mask(mask_file)\n",
    "\n",
    "            #--- predict here!  ---\n",
    "            tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "            tile_image = tile['tile_image']\n",
    "            tile_image = np.stack(tile_image)[..., ::-1]\n",
    "            tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "            tile_image = tile_image.astype(np.float32)/255\n",
    "            print(tile_image.shape)\n",
    "            tile_probability = []\n",
    "\n",
    "            batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "            for t,m in enumerate(batch):\n",
    "                print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "                m = torch.from_numpy(m).to(device)\n",
    "\n",
    "                p = []\n",
    "                with torch.no_grad():\n",
    "                    logit = net(m)\n",
    "                    p.append(torch.sigmoid(logit))\n",
    "                    if args.server == 'local':\n",
    "                        if 0: #tta here\n",
    "                            #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.cat(p)\n",
    "\n",
    "                tile_probability.append(p.data.cpu().numpy())\n",
    "            print('\\r' , end='',flush=True)\n",
    "            log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "            tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "            height, width = tile['image_small'].shape[:2]\n",
    "            probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                                  tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                                  aggregate='mean')\n",
    "            #\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            overlay = np.dstack([\n",
    "                np.zeros_like(truth),\n",
    "                probability, #green\n",
    "                truth, #red\n",
    "            ])\n",
    "\n",
    "            image_small = tile['image_small'].astype(np.float32)/255\n",
    "            #predict = (probability>thres).astype(np.float32)\n",
    "            overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "            overlay2 = image_small.copy()\n",
    "            overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "            overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "            overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "            if 1:\n",
    "                #cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "                cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "def eval_image(args):\n",
    "\n",
    "    #-----------dataset split --------------------#\n",
    "    tile_id = []\n",
    "    image_dir_ = f'{args.dataset}'#'0.25_320_160_train'\n",
    "    image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "    \n",
    "    image_dir_val_ = f'{args.val_dataset}'#'0.25_320_320_val'\n",
    "    image_dir_val=[image_dir_val_, ]\n",
    "    \n",
    "    for i in range(len(image_dir)):\n",
    "        df = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir[i]) )\n",
    "\n",
    "    for i in range(len(image_dir_val)):\n",
    "        df2 = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir_val[i]) )\n",
    "    df2['img_id'] = df2['tile_id'].apply(lambda x: x.split('/')[-2])\n",
    "        \n",
    "    all_dice = []\n",
    "    for n_fold in range(5):\n",
    "\n",
    "        train_df = df[df['fold']!= n_fold].reset_index(drop=True)\n",
    "        val_df = df2[df2['fold']== n_fold].reset_index(drop=True).copy()\n",
    "        \n",
    "        # validation loader 3개 만들기 위함\n",
    "        unique_value = val_df['tile_id'].apply(lambda x: x.split('/')[-2]).unique() #[valid_id1, valid_id2, valid_id3 ]\n",
    "        val_img_id1 = unique_value[0] ; val_img_id2 = unique_value[1] ; val_img_id3= unique_value[2]\n",
    "        val_df1= val_df[val_df['img_id']==val_img_id1].reset_index(drop=True)\n",
    "        val_df2= val_df[val_df['img_id']==val_img_id2].reset_index(drop=True)\n",
    "        val_df3= val_df[val_df['img_id']==val_img_id3].reset_index(drop=True)\n",
    "        #####################################################\n",
    "        # val loader1\n",
    "        valid_dataset1 = HuDataset(\n",
    "            df = val_df1\n",
    "            ,\n",
    "        )\n",
    "        valid_loader1 = DataLoader(\n",
    "            valid_dataset1,\n",
    "            sampler = SequentialSampler(valid_dataset1),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader2\n",
    "        valid_dataset2 = HuDataset(\n",
    "            df = val_df2\n",
    "            ,\n",
    "        )\n",
    "        \n",
    "        valid_loader2 = DataLoader(\n",
    "            valid_dataset2,\n",
    "            sampler = SequentialSampler(valid_dataset2),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader3\n",
    "        valid_dataset3 = HuDataset(\n",
    "            df = val_df3\n",
    "            ,\n",
    "        )\n",
    "        valid_loader3 = DataLoader(\n",
    "            valid_dataset3,\n",
    "            sampler = SequentialSampler(valid_dataset3),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # ------------------------\n",
    "        #  Model\n",
    "        # ------------------------\n",
    "\n",
    "        scaler = GradScaler()\n",
    "        models = SegModel() \n",
    "        net = models[n_fold].to(device)\n",
    "        state_dict = torch.load(args.model_path[n_fold], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        # 병렬처리를 했으면 앞에 module이 붙으므로 키를 바꿔줘야 한다. \n",
    "        for key in list(state_dict.keys()):\n",
    "            if \"module.\" in key:\n",
    "                state_dict[key.replace(\"module.\", \"\")] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        net.load_state_dict(state_dict,strict=True)  #True\n",
    "        net = net.eval()\n",
    "        \n",
    "        print(\"model load success!!!\")\n",
    "        # scheudler\n",
    "        valid_loss1 = do_valid(net, valid_loader1) #\n",
    "        valid_loss2 = do_valid(net, valid_loader2)\n",
    "        valid_loss3 = do_valid(net, valid_loader3)\n",
    "        valid_loss = (valid_loss1 + valid_loss2 + valid_loss3)/3\n",
    "        \n",
    "        all_dice.append(valid_loss)\n",
    "        \n",
    "    dice = sum(all_dice)/len(all_dice)\n",
    "    for n, th in enumerate(np.arange(0.1, 0.7, 0.05)):\n",
    "        th = round(th, 2)\n",
    "        print(f'th:{th}, dice score : {dice[n] : .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "documented-enforcement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "th:0.1, dice score :  0.9062\n",
      "th:0.15, dice score :  0.9173\n",
      "th:0.2, dice score :  0.9242\n",
      "th:0.25, dice score :  0.9289\n",
      "th:0.3, dice score :  0.9323\n",
      "th:0.35, dice score :  0.9346\n",
      "th:0.4, dice score :  0.9362\n",
      "th:0.45, dice score :  0.9370\n",
      "th:0.5, dice score :  0.9370\n",
      "th:0.55, dice score :  0.9363\n",
      "th:0.6, dice score :  0.9349\n",
      "th:0.65, dice score :  0.9327\n"
     ]
    }
   ],
   "source": [
    "\"\"\"red is real\"\"\"\n",
    "if 1: #normal\n",
    "    if __name__ == '__main__':\n",
    "        if args.mode == 'eval':\n",
    "            eval_image(args)\n",
    "        elif args.mode =='gen_image':\n",
    "            gen_val_image(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b45f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "minute-hydrogen",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "varied-feeling",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    server ='kaggle' # ['kaggle', 'local'] local은 cv측정용도\n",
    "    amp = False\n",
    "    gpu = 0\n",
    "    \n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    \n",
    "    diff_arch = True\n",
    "    encoders = [\"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\"]\n",
    "    decoders = [\"unet\", \"unet\", \"unet\", \"unet\", \"unet\"]\n",
    "    n_fold = 5\n",
    "    batch_size=64\n",
    "    clf_head=False\n",
    "    \n",
    "    threshold = 0.45\n",
    "    \n",
    "    model_path = '../hubmap/result/'\n",
    "\n",
    "    en_model_path = [\"./data/result/15_['efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b4']_['unet', 'unet', 'unet', 'unet', 'unet']_1024_1024_512_0.5_b4_1024\" +\n",
    "                  \"/checkpoint/\" + x for x in \\\n",
    "                 ['0fold_9epoch_0.9343_efficientnet-b4_unetmodel.pth','1fold_15epoch_0.9373_efficientnet-b4_unetmodel.pth',\n",
    "                 '2fold_5epoch_0.9363_efficientnet-b4_unetmodel.pth','3fold_12epoch_0.9518_efficientnet-b4_unetmodel.pth',\n",
    "                 '4fold_10epoch_0.9253_efficientnet-b4_unetmodel.pth']]\n",
    "    sub = '30epoch_imagefold_0.9338_320_160'# submission name\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    \n",
    "    tile_size = 1024\n",
    "    tile_average_step = 512\n",
    "    tile_scale = 0.5\n",
    "    tile_min_score = 0.25  \n",
    "\n",
    "assert args.server!='local', 'not implement'\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "horizontal-booking",
   "metadata": {
    "code_folding": [
     4,
     26,
     257,
     353,
     363
    ]
   },
   "outputs": [],
   "source": [
    "thres = args.threshold\n",
    "\n",
    "prob = []\n",
    "\n",
    "def mask_to_csv(image_id, submit_dir):\n",
    "\n",
    "    predicted = []\n",
    "    for id in image_id:\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        predict_file = submit_dir + '/%s.predict.png' % id\n",
    "        # predict = cv2.imread(predict_file, cv2.IMREAD_GRAYSCALE)\n",
    "        predict = np.array(Image.open(predict_file))\n",
    "        predict = cv2.resize(predict, dsize=(width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        predict = (predict > 128).astype(np.uint8) * 255\n",
    "\n",
    "        p = rle_encode(predict)\n",
    "        predicted.append(p)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = image_id\n",
    "    df['predicted'] = predicted\n",
    "    return df\n",
    "\n",
    "def run_submit(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.model_path.split('checkpoint')[0]\n",
    "    initial_checkpoint = out_dir + '/checkpoint' + args.model_path.split('checkpoint')[1]\n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-mean-thres(%s)'%(server, initial_checkpoint[-18:-4],thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    # ----- model -------\n",
    "    net = SegModel() \n",
    "    net.to(device)\n",
    "    state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "    net.load_state_dict(state_dict,strict=True)  #True\n",
    "    net = net.eval()\n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        if server == 'local':\n",
    "            image_file = data_dir + '/train/%s.tiff' % id\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "            structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "            mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "            mask  = read_mask(mask_file)\n",
    "\n",
    "        if server == 'kaggle':\n",
    "            image_file = data_dir + '/test/%s.tiff' % id\n",
    "            json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "            structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "            mask = None\n",
    "\n",
    "\n",
    "        #--- predict here!  ---\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        tile_image = tile['tile_image']\n",
    "        tile_image = np.stack(tile_image)[..., ::-1]\n",
    "        tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "        tile_image = tile_image.astype(np.float32)/255\n",
    "        print(tile_image.shape)\n",
    "        tile_probability = []\n",
    "        \n",
    "        batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "        for t,m in enumerate(batch):\n",
    "            print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "            m = torch.from_numpy(m).to(device)\n",
    "\n",
    "            p = []\n",
    "            with torch.no_grad():\n",
    "                logit = net(m)\n",
    "                p.append(torch.sigmoid(logit))\n",
    "\n",
    "                #---\n",
    "                if server == 'kaggle':\n",
    "                    if 1: #tta here\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.stack(p).mean(0)\n",
    "                if server == 'local':\n",
    "                    if 0: #tta here\n",
    "                        #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.cat(p)\n",
    "                    #p = torch.stack(p)\n",
    "\n",
    "            tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "        print('\\r' , end='',flush=True)\n",
    "        log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "        tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "        height, width = tile['image_small'].shape[:2]\n",
    "        probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                              tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                              aggregate='mean')\n",
    "        \n",
    "\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = (probability>thres).astype(np.float32)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir + args.sub+'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0\n",
    "    \n",
    "def run_submit_ensemble(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.en_model_path[0].split('checkpoint')[0]\n",
    "    \n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-thres(%s)'%(server, args.sub,thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        fold_prob = []\n",
    "        models = SegModel()\n",
    "        for i, m_p in enumerate(args.en_model_path):\n",
    "            initial_checkpoint = m_p\n",
    "            # ----- model -------\n",
    "            net = models[i]\n",
    "            net.to(device)\n",
    "            state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "            for key in list(state_dict.keys()):\n",
    "                if \"module.\" in key:\n",
    "                    state_dict[key.replace(\"module.\", \"\")] = state_dict[key]\n",
    "                    del state_dict[key]\n",
    "            net.load_state_dict(state_dict,strict=True)  #True\n",
    "            net = net.eval()\n",
    "            print(\"model load success!!!\")\n",
    "            if server == 'local':\n",
    "                image_file = data_dir + '/train/%s.tiff' % id\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "                structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "                mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "                mask  = read_mask(mask_file)\n",
    "\n",
    "            if server == 'kaggle':\n",
    "                image_file = data_dir + '/test/%s.tiff' % id\n",
    "                json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "                structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "                mask = None\n",
    "\n",
    "\n",
    "            #--- predict here!  ---\n",
    "            tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "            tile_image = tile['tile_image']\n",
    "            tile_image = np.stack(tile_image)[..., ::-1]\n",
    "            tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "            tile_image = tile_image.astype(np.float32)/255\n",
    "            print(tile_image.shape)\n",
    "            tile_probability = []\n",
    "\n",
    "            batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "            for t,m in enumerate(batch):\n",
    "                print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "                m = torch.from_numpy(m).to(device)\n",
    "\n",
    "                p = []\n",
    "                with torch.no_grad():\n",
    "                    logit = net(m)\n",
    "                    p.append(torch.sigmoid(logit))\n",
    "\n",
    "                    #---\n",
    "                    if server == 'kaggle':\n",
    "                        if 1: #tta here\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.stack(p).mean(0)\n",
    "                    if server == 'local':\n",
    "                        if 0: #tta here\n",
    "                            #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.cat(p)\n",
    "                        #p = torch.stack(p)\n",
    "\n",
    "                tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "            print('\\r' , end='',flush=True)\n",
    "            log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "            tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "            height, width = tile['image_small'].shape[:2]\n",
    "            probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                                  tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                                  aggregate='mean')\n",
    "\n",
    "            fold_prob.append(probability)\n",
    "        \n",
    "        probability = sum(fold_prob)/len(args.en_model_path)\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = (probability>thres).astype(np.float32)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir +'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "brazilian-pennsylvania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_size = 1024 \n",
      "tile_average_step = 512 \n",
      "tile_scale = 0.500000 \n",
      "tile_min_score = 0.250000 \n",
      "\n",
      "model load success!!!\n",
      "(569, 3, 1024, 1024)\n",
      "2ec3f1bb9  141 / 142    1 min 48 secc\n",
      "model load success!!!\n",
      "(569, 3, 1024, 1024)\n",
      "2ec3f1bb9  141 / 142    3 min 26 secc\n",
      "model load success!!!\n",
      "(569, 3, 1024, 1024)\n",
      "2ec3f1bb9  141 / 142    4 min 51 secc\n",
      "model load success!!!\n",
      "(569, 3, 1024, 1024)\n",
      "2ec3f1bb9  141 / 142    6 min 13 secc\n",
      "model load success!!!\n",
      "(569, 3, 1024, 1024)\n",
      "2ec3f1bb9  141 / 142    7 min 34 secc\n",
      "model load success!!!\n",
      "(204, 3, 1024, 1024)\n",
      "3589adb90  50 / 51   10 min 22 secc\n",
      "model load success!!!\n",
      "(204, 3, 1024, 1024)\n",
      "3589adb90  50 / 51   10 min 53 secc\n",
      "model load success!!!\n",
      "(204, 3, 1024, 1024)\n",
      "3589adb90  50 / 51   11 min 24 secc\n",
      "model load success!!!\n",
      "(204, 3, 1024, 1024)\n",
      "3589adb90  50 / 51   11 min 53 secc\n",
      "model load success!!!\n",
      "(204, 3, 1024, 1024)\n",
      "3589adb90  50 / 51   12 min 23 secc\n",
      "model load success!!!\n",
      "(380, 3, 1024, 1024)\n",
      "57512b7f1  94 / 95   14 min 06 secc\n",
      "model load success!!!\n",
      "(380, 3, 1024, 1024)\n",
      "57512b7f1  94 / 95   15 min 15 secc\n",
      "model load success!!!\n",
      "(380, 3, 1024, 1024)\n",
      "57512b7f1  94 / 95   16 min 26 secc\n",
      "model load success!!!\n",
      "(380, 3, 1024, 1024)\n",
      "57512b7f1  94 / 95   17 min 37 secc\n",
      "model load success!!!\n",
      "(380, 3, 1024, 1024)\n",
      "57512b7f1  94 / 95   18 min 47 secc\n",
      "model load success!!!\n",
      "(616, 3, 1024, 1024)\n",
      "aa05346ff  153 / 154   31 min 39 secc\n",
      "model load success!!!\n",
      "(616, 3, 1024, 1024)\n",
      "aa05346ff  153 / 154   35 min 31 secc\n",
      "model load success!!!\n",
      "(616, 3, 1024, 1024)\n",
      "aa05346ff  153 / 154   37 min 33 secc\n",
      "model load success!!!\n",
      "(616, 3, 1024, 1024)\n",
      "aa05346ff  153 / 154   39 min 44 secc\n",
      "model load success!!!\n",
      "(616, 3, 1024, 1024)\n",
      "aa05346ff  153 / 154   41 min 49 secc\n",
      "model load success!!!\n",
      "(359, 3, 1024, 1024)\n",
      "d488c759a  88 / 89   62 min 21 secc\n",
      "model load success!!!\n",
      "(359, 3, 1024, 1024)\n",
      "d488c759a  88 / 89   63 min 25 secc\n",
      "model load success!!!\n",
      "(359, 3, 1024, 1024)\n",
      "d488c759a  88 / 89   64 min 28 secc\n",
      "model load success!!!\n",
      "(359, 3, 1024, 1024)\n",
      "d488c759a  88 / 89   65 min 32 secc\n",
      "model load success!!!\n",
      "(359, 3, 1024, 1024)\n",
      "d488c759a  88 / 89   66 min 36 secc\n",
      "          id                                          predicted\n",
      "0  2ec3f1bb9  60762295 26 60786285 26 60810259 52 60834249 5...\n",
      "1  3589adb90  68600098 49 68629530 51 68658950 71 68688382 7...\n",
      "2  57512b7f1  328952557 30 328985797 30 329019029 46 3290522...\n",
      "3  aa05346ff  52856689 36 52887409 36 52918117 58 52948837 5...\n",
      "4  d488c759a  534763897 12 534810557 12 534857205 38 5349038...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 0: #normal\n",
    "    if __name__ == '__main__':\n",
    "        run_submit(args)\n",
    "elif 1:# ensemble\n",
    "    if __name__ == '__main__':\n",
    "        run_submit_ensemble(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-cornell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-fields",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubmap",
   "language": "python",
   "name": "hubmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
