{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "endless-cleanup",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# ------------Library--------------#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingWarmRestarts, CosineAnnealingLR\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import tifffile as tiff\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "from timeit import default_timer as timer\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "#\n",
    "from sklearn.model_selection import KFold\n",
    "#from lib.net.lookahead import *\n",
    "#from lib.net.radam import *\n",
    "# constant #\n",
    "PI  = np.pi\n",
    "INF = np.inf\n",
    "EPS = 1e-12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "induced-archives",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    amp = True  # mixed precision 나중에는 false\n",
    "    gpu = 3\n",
    "    dir = \"100epoch_nooverlap_640_25_50\"  # 로그 저장 폴더\n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    batch_size= 16  #잘 안건드림 128이상은 별로 \n",
    "    weight_decay=1e-6  \n",
    "    n_fold=5\n",
    "    fold=0 # [0, 1, 2, 3, 4] \n",
    "    all_fold_train = True # all fold training 총5번 돔 \n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    image_size=512\n",
    "    dataset = '0.25_640_320_train' # dataset size\n",
    "    overlap = False\n",
    "\n",
    "    # ---- optimizer, scheduler .. ---- #\n",
    "    epochs=100   # 바꿔보기\n",
    "    opt =  'radam_look' # [adamw, radam_look]\n",
    "    scheduler='CosineAnnealingLR' #'MultiStepLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    loss = 'bce' # lovasz\n",
    "    factor=0.2 # ReduceLROnPlateau, MultiStepLR\n",
    "    patience=2 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=10 # CosineAnnealingLR\n",
    "    decay_epoch = [4, 8, 12]\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "    #encoder_lr=4e-4\n",
    "    #decoder_lr=4e-4\n",
    "    start_lr = 1e-3\n",
    "    min_lr=1e-6\n",
    "    \n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=42\n",
    "    \n",
    "data_dir = \"/home/jeonghokim/competition/HubMap/data/\"\n",
    "##----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-mauritius",
   "metadata": {},
   "source": [
    "# useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handled-silence",
   "metadata": {
    "code_folding": [
     33,
     41,
     64,
     90,
     96,
     111
    ]
   },
   "outputs": [],
   "source": [
    "#-------evaluation metric---------#\n",
    "###################################\n",
    "def np_binary_cross_entropy_loss(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    #---\n",
    "    logp = -np.log(np.clip(p,1e-6,1))\n",
    "    logn = -np.log(np.clip(1-p,1e-6,1))\n",
    "    loss = t*logp +(1-t)*logn\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def np_dice_score(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "def np_accuracy(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    tp = (p*t).sum()/(t).sum()\n",
    "    tn = ((1-p)*(1-t)).sum()/(1-t).sum()\n",
    "    return tp, tn\n",
    "\n",
    "def criterion_binary_cross_entropy(logit, mask):\n",
    "    logit = logit.reshape(-1)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logit, mask)\n",
    "    return loss\n",
    "\n",
    "# threshold dice score\n",
    "def np_dice_score2(probability, mask, threshold):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>threshold\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "###################################\n",
    "#-------ELSE function---------#\n",
    "###################################\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True # for faster training, but not deterministic\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "def print_args(args, logger=None):\n",
    "    for k, v in vars(args).items():\n",
    "        if logger is not None:\n",
    "            logger.write('{:<16} : {}'.format(k, v))\n",
    "        else:\n",
    "            print('{:<16} : {}'.format(k, v))\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "closed-mileage",
   "metadata": {
    "code_folding": [
     20,
     27,
     52,
     67,
     153,
     196,
     208
    ]
   },
   "outputs": [],
   "source": [
    "#-------masking & tile & decode---------#\n",
    "def read_tiff(image_file):\n",
    "    \"\"\"\n",
    "    *data size*\n",
    "    e.g.) (3, w, h) or (1,1,3,w,h) or (w, h, 3)  --> transform --> (w, h, 3)\n",
    "    \"\"\"\n",
    "    image = tiff.imread(image_file)\n",
    "    if image.shape[0] == 1:\n",
    "        image = image[0][0]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    elif image.shape[0] == 3:\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    return image\n",
    "\n",
    "def read_mask(mask_file):\n",
    "    mask = np.array(Image.open(mask_file))\n",
    "    return mask\n",
    "\n",
    "def read_json_as_df(json_file):\n",
    "    with open(json_file) as f:\n",
    "        j = json.load(f)\n",
    "    df = pd.json_normalize(j)\n",
    "    return df\n",
    "\n",
    "\n",
    "def draw_strcuture(df, height, width, fill=255, structure=[]):\n",
    "    mask = np.zeros((height, width), np.uint8)\n",
    "    for row in df.values:\n",
    "        type  = row[2]  #geometry.type\n",
    "        coord = row[3]  # geometry.coordinates\n",
    "        name  = row[4]   # properties.classification.name\n",
    "\n",
    "        if structure !=[]:\n",
    "            if not any(s in name for s in structure): continue\n",
    "\n",
    "\n",
    "        if type=='Polygon':\n",
    "            pt = np.array(coord).astype(np.int32)\n",
    "            #cv2.polylines(mask, [coord.reshape((-1, 1, 2))], True, 255, 1)\n",
    "            cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "        if type=='MultiPolygon':\n",
    "            for pt in coord:\n",
    "                pt = np.array(pt).astype(np.int32)\n",
    "                cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# resize, cvtcolor, generate mask\n",
    "# 원하는 object 영역만 따오는 mask\n",
    "def draw_strcuture_from_hue(image, fill=255, scale=1/32): # 0.25/32 default\n",
    "    height, width, _ = image.shape\n",
    "    vv = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = cv2.cvtColor(vv, cv2.COLOR_RGB2HSV)\n",
    "    # image_show('v[0]', v[:,:,0])\n",
    "    # image_show('v[1]', v[:,:,1])\n",
    "    # image_show('v[2]', v[:,:,2])\n",
    "    # cv2.waitKey(0)\n",
    "    mask = (vv[:, :, 1] > 32).astype(np.uint8) # rgb2hsv를 하고나서 1채널에 대해 시행하면 원하는 object만 잘따온다.\n",
    "    mask = mask*fill\n",
    "    mask = cv2.resize(mask, dsize=(width, height), interpolation=cv2.INTER_LINEAR) # 다시 원래사이즈로 복구\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --- rle ---------------------------------\n",
    "def rle_decode(rle, height, width , fill=255):\n",
    "    s = rle.split()\n",
    "    start, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    start -= 1\n",
    "    mask = np.zeros(height*width, dtype=np.uint8)\n",
    "    for i, l in zip(start, length):\n",
    "        mask[i:i+l] = fill\n",
    "    mask = mask.reshape(width,height).T\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def rle_encode(mask):\n",
    "    m = mask.T.flatten()\n",
    "    m = np.concatenate([[0], m, [0]])\n",
    "    run = np.where(m[1:] != m[:-1])[0] + 1\n",
    "    run[1::2] -= run[::2]\n",
    "    rle =  ' '.join(str(r) for r in run)\n",
    "    return rle\n",
    "\n",
    "\n",
    "# --- tile ---------------------------------\n",
    "\"\"\"\n",
    "-결국, tile_image, tile_mask만 가져다가 쓴다.\n",
    "1. scale로 resize를 하고 image size와 step만큼 건너뛰며 이미지를 만든다.\n",
    "2. 이때 일정 영역이 빈마스크면 데이터에서 제외한다.\n",
    "3. 쌓은 image와 mask를 return\n",
    "\"\"\"\n",
    "def to_tile(image, mask, structure, scale, size, step, min_score): \n",
    "    half = size//2\n",
    "    image_small = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR) # defualt는 1/4만큼 w,h를 줄인다.\n",
    "    height, width, _ = image_small.shape\n",
    "\n",
    "    #make score\n",
    "    structure_small = cv2.resize(structure, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = structure_small.astype(np.float32)/255\n",
    "\n",
    "    #make coord\n",
    "    xx = np.linspace(half, width  - half, int(np.ceil((width  - size) / step)))\n",
    "    yy = np.linspace(half, height - half, int(np.ceil((height - size) / step)))\n",
    "    xx = [int(x) for x in xx]\n",
    "    yy = [int(y) for y in yy]\n",
    "\n",
    "    coord  = []\n",
    "    reject = []\n",
    "    for cy in yy:\n",
    "        for cx in xx:\n",
    "            cv = vv[cy - half:cy + half, cx - half:cx + half].mean() # h, w // tiling한 마스크(structure)가 평균 0.25를 안넘으면 버린다.\n",
    "            if cv>min_score: # min_score ,default:0.25, 0.25의 의미?, 타일링 이미지의 1/4는 object여야 한다는 의미?\n",
    "                coord.append([cx,cy,cv])\n",
    "            else:\n",
    "                reject.append([cx,cy,cv])\n",
    "    #-----\n",
    "    if 1: # resize한 image를 tiling 하여 리스트만든다\n",
    "        tile_image = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = image_small[cy - half:cy + half, cx - half:cx + half] # resize한 image에서 indexing만 하는과정\n",
    "            assert (t.shape == (size, size, 3))\n",
    "            tile_image.append(t)\n",
    "\n",
    "    if mask is not None: # mask를 resize하고 tiling하여 리스트 만든다\n",
    "        mask_small = cv2.resize(mask, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        tile_mask = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = mask_small[cy - half:cy + half, cx - half:cx + half]\n",
    "            assert (t.shape == (size, size))\n",
    "            tile_mask.append(t)\n",
    "    else:\n",
    "        mask_small = None\n",
    "        tile_mask  = None\n",
    "\n",
    "    return {\n",
    "        'image_small': image_small,\n",
    "        'mask_small' : mask_small,\n",
    "        'structure_small' : structure_small,\n",
    "        'tile_image' : tile_image,\n",
    "        'tile_mask'  : tile_mask,\n",
    "        'coord'  : coord,\n",
    "        'reject' : reject,\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "submission할때 쓰임\n",
    "\"\"\"\n",
    "def to_mask(tile, coord, height, width, scale, size, step, min_score, aggregate='mean'):\n",
    "\n",
    "    half = size//2\n",
    "    mask  = np.zeros((height, width), np.float32)\n",
    "\n",
    "    if 'mean' in aggregate:\n",
    "        w = np.ones((size,size), np.float32)\n",
    "\n",
    "        #if 'sq' in aggregate:\n",
    "        if 1:\n",
    "            #https://stackoverflow.com/questions/17190649/how-to-obtain-a-gaussian-filter-in-python\n",
    "            y,x = np.mgrid[-half:half,-half:half]\n",
    "            y = half-abs(y)\n",
    "            x = half-abs(x)\n",
    "            w = np.minimum(x,y)\n",
    "            w = w/w.max()#*2.5\n",
    "            w = np.minimum(w,1)\n",
    "\n",
    "        #--------------\n",
    "        count = np.zeros((height, width), np.float32)\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask [cy - half:cy + half, cx - half:cx + half] += tile[t]*w\n",
    "            count[cy - half:cy + half, cx - half:cx + half] += w\n",
    "               # see unet paper for \"Overlap-tile strategy for seamless segmentation of arbitrary large images\"\n",
    "        m = (count != 0)\n",
    "        mask[m] /= count[m]\n",
    "\n",
    "    if aggregate=='max':\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask[cy - half:cy + half, cx - half:cx + half] = np.maximum(\n",
    "                mask[cy - half:cy + half, cx - half:cx + half], tile[t] )\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --------------이 아래로 안씀 ------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# --draw ------------------------------------------\n",
    "\"\"\"\n",
    "경계선을 그리게 만든다, 컨투어\n",
    "하지만 안씀\n",
    "\"\"\"\n",
    "def mask_to_inner_contour(mask):\n",
    "    mask = mask>0.5\n",
    "    pad = np.lib.pad(mask, ((1, 1), (1, 1)), 'reflect')\n",
    "    contour = mask & (\n",
    "            (pad[1:-1,1:-1] != pad[:-2,1:-1]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[2:,1:-1])  \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,:-2]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,2:])\n",
    "    )\n",
    "    return contour\n",
    "\n",
    "\n",
    "def draw_contour_overlay(image, mask, color=(0,0,255), thickness=1):\n",
    "    contour =  mask_to_inner_contour(mask)\n",
    "    if thickness==1:\n",
    "        image[contour] = color\n",
    "    else:\n",
    "        r = max(1,thickness//2)\n",
    "        for y,x in np.stack(np.where(contour)).T:\n",
    "            cv2.circle(image, (x,y), r, color, lineType=cv2.LINE_4 )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-compiler",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "curious-spotlight",
   "metadata": {
    "code_folding": [
     0,
     85,
     175
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------ make dataset --------- #\n",
    "#################################\n",
    "\n",
    "# <todo> make difference scale tile\n",
    "\n",
    "tile_scale = 0.25\n",
    "tile_min_score = 0.25  # \n",
    "tile_size = 480#320  # 480 #\n",
    "tile_average_step = 240#160 #240  # 160 #192\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "# train,tiling (image,mask) png 저장용도\n",
    "def run_make_train_tile():\n",
    "    #tile_scale = 0.25\n",
    "    #tile_min_score = 0.25\n",
    "    #tile_size = 480#480#320  #480 #\n",
    "    #tile_average_step = 240#240 #160 #240  # 192\n",
    "\n",
    "\n",
    "    train_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_train' #nipa2\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "    \n",
    "    df_all = []\n",
    "    \n",
    "    os.makedirs(train_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "        # 1. image 불러오고\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        #mask = rle_decode(encoding, height, width, 255)\n",
    "        # 2. mask, target 불러오고\n",
    "        mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "        mask = read_mask(mask_file)\n",
    "        \n",
    "        # 3. 일정영역,object만 표시한 mask불러오기.\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "        print(id, mask_file)\n",
    "        \n",
    "        # make tile\n",
    "        # 4. 학습할 tile image, mask를 생성한다.\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(train_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            s = '%s_y%08d_x%08d' % (id, cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(train_tile_dir + '/%s.png' % (s), tile_image)\n",
    "            cv2.imwrite(train_tile_dir + '/%s.mask.png' % (s), tile_mask)\n",
    "\n",
    "            #image_show('tile_image', tile_image)\n",
    "            #image_show('tile_mask', tile_mask)\n",
    "            #cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        df_image['tile_id']=tile_id\n",
    "        df_all.append(df_image)\n",
    "    df_all = pd.concat(df_all, 0).reset_index(drop=True)\n",
    "    df_all[['tile_id','cx','cy','cv']].to_csv(train_tile_dir+'/image_id.csv', index=False)\n",
    "        #------\n",
    "\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "# test tiling image png 저장용도\n",
    "def run_make_test_tile():\n",
    "    #tile_scale = 0.25\n",
    "    #tile_min_score = 0.25\n",
    "    #tile_size = 480#320  # 480 #\n",
    "    #tile_average_step = 240#160 #240  # 160 #192\n",
    "\n",
    "    #test_tile_dir = '/home/ubuntu/gwang/hubmap/etc/tile/0.25_640_320_test'\n",
    "    test_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_test'\n",
    "    #---\n",
    "\n",
    "\n",
    "    os.makedirs(test_tile_dir, exist_ok=True)\n",
    "    assert False, 'todo modify test file'\n",
    "    for id in ['c68fe75ea','afa5e8098',]:\n",
    "        print(id)\n",
    "\n",
    "        # 1. test image load\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "        image = read_tiff(image_file)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        mask = None\n",
    "        # 2. test structure load\n",
    "        structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "        # structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "\n",
    "        # 3. test를 위한 tile image 생성\n",
    "        #make tile\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(test_tile_dir+'/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            s = 'y%08d_x%08d' % (cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            cv2.imwrite(test_tile_dir + '/%s/%s.png' % (id, s), tile_image)\n",
    "            #image_show('tile_image', tile_image)\n",
    "            #cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        df_image['tile_id']=tile_id\n",
    "        df_image[['tile_id','cx','cy','cv']].to_csv(test_tile_dir+'/%s.csv'%id, index=False)\n",
    "        #------\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "# tile이 아닌 train image의 mask생성\n",
    "def run_make_train_mask():\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    print(df_train)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    for i in range(0,len(df_train)):\n",
    "        id, encoding = df_train.iloc[i]\n",
    "\n",
    "        image_file = data_dir + '/train/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        if image.shape[0]==1:\n",
    "            image = image[0][0]\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            image = np.ascontiguousarray(image)\n",
    "            height, width = image.shape[:2]\n",
    "        elif image.shape[0] == 3:\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            image = np.ascontiguousarray(image)\n",
    "            height, width = image.shape[:2]\n",
    "        else:\n",
    "            height, width = image.shape[:2]\n",
    "        mask = rle_decode(encoding, height, width, 255)\n",
    "\n",
    "        cv2.imwrite(data_dir + '/train/%s.mask.png' % id, mask)\n",
    "\n",
    "\n",
    "#make tile train image\n",
    "def run_make_pseudo_tile():\n",
    "\n",
    "    \n",
    "    tile_scale = 0.25\n",
    "    tile_min_score = 0.25\n",
    "    tile_size = 480#320  #480 #\n",
    "    tile_average_step = 240 #160 #240  # 192\n",
    "    #---\n",
    "    pseudo_tile_dir = data_dir + f'/tile/{tile_scale}_{tile_size}_{tile_average_step}_pseudo_0.95'\n",
    "    #df_train = pd.read_csv(data_dir + '/train.csv')\n",
    "    #df_pseudo = pd.read_csv('/root/share1/kaggle/2020/hubmap/result/resnet34/fold2/submit-fold-2-resnet34-00010000_model_lb0.837.csv')\n",
    "    df_pseudo = pd.read_csv('../../submission/0.891_submission-fold6-00004000_model_thres-0.9.csv')\n",
    "    \n",
    "    print(df_pseudo)\n",
    "    print(df_pseudo.shape)\n",
    "\n",
    "    os.makedirs(pseudo_tile_dir, exist_ok=True)\n",
    "    for i in range(0,len(df_pseudo)):\n",
    "        id, encoding = df_pseudo.iloc[i]\n",
    "\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        mask = rle_decode(encoding, height, width, 255)\n",
    "\n",
    "        #make tile\n",
    "        structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)\n",
    "\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "        #to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        #mask, scale, size, step, min_score\n",
    "\n",
    "        coord = np.array(tile['coord'])\n",
    "        df_image = pd.DataFrame()\n",
    "        df_image['cx']=coord[:,0].astype(np.int32)\n",
    "        df_image['cy']=coord[:,1].astype(np.int32)\n",
    "        df_image['cv']=coord[:,2]\n",
    "\n",
    "        # --- save ---\n",
    "        os.makedirs(pseudo_tile_dir + '/%s'%id, exist_ok=True)\n",
    "\n",
    "        tile_id =[]\n",
    "        num = len(tile['tile_image'])\n",
    "        for t in range(num):\n",
    "            cx,cy,cv   = tile['coord'][t]\n",
    "            s = 'y%08d_x%08d' % (cy, cx)\n",
    "            tile_id.append(s)\n",
    "\n",
    "            tile_image = tile['tile_image'][t]\n",
    "            tile_mask  = tile['tile_mask'][t]\n",
    "            cv2.imwrite(pseudo_tile_dir + '/%s/%s.png' % (id, s), tile_image)\n",
    "            cv2.imwrite(pseudo_tile_dir + '/%s/%s.mask.png' % (id, s), tile_mask)\n",
    "\n",
    "            #image_show('tile_image', tile_image)\n",
    "            #image_show('tile_mask', tile_mask)\n",
    "            #cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        df_image['tile_id']=tile_id\n",
    "        df_image[['tile_id','cx','cy','cv']].to_csv(pseudo_tile_dir+'/%s.csv'%id, index=False)\n",
    "        #------\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if 0:\n",
    "    if __name__ == '__main__':\n",
    "        #print('started run make train mask')\n",
    "        # 1.\n",
    "        print('started 1')\n",
    "        run_make_train_mask()\n",
    "        # 2.\n",
    "        print('started 2')\n",
    "        run_make_train_tile()\n",
    "        # 3.\n",
    "        #print('started 3')\n",
    "        #run_make_test_tile()\n",
    "        # 4. if use pseudo datasets\n",
    "        #run_make_pseudo_tile()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-japanese",
   "metadata": {},
   "source": [
    "# Dataset & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "duplicate-macedonia",
   "metadata": {
    "code_folding": [
     0,
     6,
     44,
     92
    ]
   },
   "outputs": [],
   "source": [
    "#--------------- Dataset ----------------#\n",
    "##########################################\n",
    "\n",
    "#--------------- \n",
    "# Old version\n",
    "#--------------- \n",
    "def make_image_id_v1(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset_v1(Dataset):\n",
    "    def __init__(self, image_id, image_dir, augment=None):\n",
    "        self.augment = augment\n",
    "        self.image_id = image_id\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        tile_id = []\n",
    "        for i in range(len(image_dir)):\n",
    "            for id in image_id[i]: \n",
    "                df = pd.read_csv(data_dir + '/tile/%s/%s.csv'% (self.image_dir[i],id) )\n",
    "                tile_id += ('%s/%s/'%(self.image_dir[i],id) + df.tile_id).tolist()\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        string += '\\timage_dir = %s\\n'%self.image_dir\n",
    "        string += '\\timage_id  = %s\\n'%str(self.image_id)\n",
    "        string += '\\t          = %d\\n'%sum(len(i) for i in self.image_id)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(data_dir + '/tile/%s.png'%(id), cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(data_dir + '/tile/%s.mask.png'%(id), cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "\n",
    "#--------------- \n",
    "# New version\n",
    "#--------------- \n",
    "def make_image_id(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset(Dataset):\n",
    "    def __init__(self, tile_id, augment=None):\n",
    "        self.augment = augment\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.png', cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "        \n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "    \n",
    "    \n",
    "# no over lapped dataset\n",
    "class HuDataset_nol(Dataset):\n",
    "    def __init__(self, data_dir : str, tile_info : str, fold : int, train=True, augment=None):\n",
    "        self.augment = augment\n",
    "        train_names = sorted([\"1e2425f28\", \"2f6ecfcdf\", \"4ef6695ce\", \"26dc41664\", \"54f2eec69\",\n",
    "                      \"095bf7a1f\", \"0486052bb\", \"8242609fa\", \"aaa6a05cc\", \"afa5e8098\",\n",
    "                      \"b2dc8411c\", \"b9a3865fc\", \"c68fe75ea\", \"cb2d976f4\", \"e79de561c\"])\n",
    "        valid_names = []\n",
    "        for i in range(3):\n",
    "            valid_names.append(train_names.pop((fold-1)*3))\n",
    "        self.img_paths = []\n",
    "        self.mask_paths = []\n",
    "        if train:\n",
    "            for name in train_names:\n",
    "                self.img_paths += glob(os.path.join(data_dir, \"tile\", tile_info, \"{}_*.png\".format(name)))\n",
    "                self.mask_paths += glob(os.path.join(data_dir, \"tile\", tile_info, \"{}_*.mask.png\".format(name)))\n",
    "            self.mask_paths.sort()\n",
    "            self.img_paths = sorted(list(set(self.img_paths) - set(self.mask_paths)))\n",
    "        else:\n",
    "            for name in valid_names:\n",
    "                self.img_paths += glob(os.path.join(data_dir, \"tile\", tile_info, \"{}_*.png\".format(name)))\n",
    "                self.mask_paths += glob(os.path.join(data_dir, \"tile\", tile_info, \"{}_*.mask.png\".format(name)))\n",
    "            self.mask_paths.sort()\n",
    "            self.img_paths = sorted(list(set(self.img_paths) - set(self.mask_paths)))\n",
    "\n",
    "        assert len(self.img_paths) == len(self.mask_paths), \"different number of images!!!!\"\n",
    "        print(\"{} => # imgs : {}, # masks : {}\".format(\"train\" if train else \"valid\", len(self.img_paths), len(self.mask_paths)))\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_paths[idx], cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        r = {\n",
    "        'index' : idx,\n",
    "        #'tile_id' : id,\n",
    "        'mask' : mask,\n",
    "        'image' : img,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "        \n",
    "    \n",
    "\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    index = []\n",
    "    mask = []\n",
    "    image = []\n",
    "    for r in batch:\n",
    "        index.append(r['index'])\n",
    "        mask.append(r['mask'])\n",
    "        image.append(r['image'])\n",
    "\n",
    "    image = np.stack(image)\n",
    "    image = image[...,::-1]\n",
    "    image = image.transpose(0,3,1,2)\n",
    "    image = np.ascontiguousarray(image)\n",
    "\n",
    "    mask  = np.stack(mask)\n",
    "    mask  = np.ascontiguousarray(mask)\n",
    "\n",
    "    #---\n",
    "    image = torch.from_numpy(image).contiguous().float()\n",
    "    mask  = torch.from_numpy(mask).contiguous().unsqueeze(1)\n",
    "    mask  = (mask>0.5).float()\n",
    "\n",
    "    return {\n",
    "        'index' : index,\n",
    "        'mask' : mask,\n",
    "        'image' : image,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broadband-placement",
   "metadata": {
    "code_folding": [
     0,
     3,
     19,
     27,
     42,
     82,
     91,
     97,
     103,
     122
    ]
   },
   "outputs": [],
   "source": [
    "#---------- augmentation ---------------------#\n",
    "###############################################\n",
    "#flip\n",
    "def do_random_flip_transpose(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = image.transpose(1,0,2)\n",
    "        mask = mask.transpose(1,0)\n",
    "\n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return image, mask\n",
    "\n",
    "#geometric\n",
    "def do_random_crop(image, mask, size):\n",
    "    height, width = image.shape[:2]\n",
    "    x = np.random.choice(width -size)\n",
    "    y = np.random.choice(height-size)\n",
    "    image = image[y:y+size,x:x+size]\n",
    "    mask  = mask[y:y+size,x:x+size]\n",
    "    return image, mask\n",
    "\n",
    "def do_random_scale_crop(image, mask, size, mag):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    s = 1 + np.random.uniform(-1, 1)*mag\n",
    "    s =  int(s*size)\n",
    "\n",
    "    x = np.random.choice(width -s)\n",
    "    y = np.random.choice(height-s)\n",
    "    image = image[y:y+s,x:x+s]\n",
    "    mask  = mask[y:y+s,x:x+s]\n",
    "    if s!=size:\n",
    "        image = cv2.resize(image, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "        mask  = cv2.resize(mask, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rotate_crop(image, mask, size, mag=30 ):\n",
    "    angle = 1+np.random.uniform(-1, 1)*mag\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    dst = np.array([\n",
    "        [0,0],[size,size], [size,0], [0,size],\n",
    "    ])\n",
    "\n",
    "    c = np.cos(angle/180*2*PI)\n",
    "    s = np.sin(angle/180*2*PI)\n",
    "    src = (dst-size//2)@np.array([[c, -s],[s, c]]).T\n",
    "    src[:,0] -= src[:,0].min()\n",
    "    src[:,1] -= src[:,1].min()\n",
    "\n",
    "    src[:,0] = src[:,0] + np.random.uniform(0,width -src[:,0].max())\n",
    "    src[:,1] = src[:,1] + np.random.uniform(0,height-src[:,1].max())\n",
    "\n",
    "    if 0: #debug\n",
    "        def to_int(f):\n",
    "            return (int(f[0]),int(f[1]))\n",
    "\n",
    "        cv2.line(image, to_int(src[0]), to_int(src[1]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[1]), to_int(src[2]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[2]), to_int(src[3]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[3]), to_int(src[0]), (0,0,1), 16)\n",
    "        image_show_norm('image', image, min=0, max=1)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "    transform = cv2.getAffineTransform(src[:3].astype(np.float32), dst[:3].astype(np.float32))\n",
    "    image = cv2.warpAffine( image, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "    mask  = cv2.warpAffine( mask, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask\n",
    "\n",
    "#warp/elastic deform ...\n",
    "#<todo>\n",
    "\n",
    "#noise\n",
    "def do_random_noise(image, mask, mag=0.1):\n",
    "    height, width = image.shape[:2]\n",
    "    noise = np.random.uniform(-1,1, (height, width,1))*mag\n",
    "    image = image + noise\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "#intensity\n",
    "def do_random_contast(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image * alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_gain(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image ** alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h = hsv[:, :, 0].astype(np.float32)  # hue\n",
    "    s = hsv[:, :, 1].astype(np.float32)  # saturation\n",
    "    v = hsv[:, :, 2].astype(np.float32)  # value\n",
    "    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n",
    "    s =  s*(1 + random.uniform(-1,1)*mag[1])\n",
    "    v =  v*(1 + random.uniform(-1,1)*mag[2])\n",
    "\n",
    "    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n",
    "    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n",
    "    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    image = image.astype(np.float32)/255\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def filter_small(mask, min_size):\n",
    "\n",
    "    m = (mask*255).astype(np.uint8)\n",
    "\n",
    "    num_comp, comp, stat, centroid = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
    "    if num_comp==1: return mask\n",
    "\n",
    "    filtered = np.zeros(comp.shape,dtype=np.uint8)\n",
    "    area = stat[:, -1]\n",
    "    for i in range(1, num_comp):\n",
    "        if area[i] >= min_size:\n",
    "            filtered[comp == i] = 255\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gothic-lebanon",
   "metadata": {
    "code_folding": [
     2,
     41
    ]
   },
   "outputs": [],
   "source": [
    "#---------- optimizer ---------------------#\n",
    "############################################\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, alpha=0.5, k=6):\n",
    "\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "\n",
    "        self.slow_weights = [\n",
    "                [p.clone().detach() for p in group['params']]\n",
    "            for group in self.param_groups]\n",
    "\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        self.state = optimizer.state\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        loss = self.optimizer.step()\n",
    "\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(p.data - q.data, alpha=self.alpha )\n",
    "                p.data.copy_(q.data)\n",
    "        return loss\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-scanner",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "periodic-dairy",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def SegModel():\n",
    "    if args.encoder in ['b0','b1','b2','b3','b4','b5','b6','b7']:\n",
    "        encoder_name_ = f'efficientnet-{args.encoder}' #'timm-efficientnet-b4'\n",
    "        print('encoder : ', encoder_name_)\n",
    "    else:\n",
    "        encoder_name_ = args.encoder\n",
    "    if args.decoder =='fpn':\n",
    "        print('fpn loaded')\n",
    "        model = smp.FPN(\n",
    "            encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "            in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "            classes=1,                      # model output channels (number of classes in your dataset)\n",
    "            )\n",
    "    elif args.decoder =='unet':\n",
    "        print('unet loaded')\n",
    "        model = smp.Unet(\n",
    "            encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "            in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "            classes=1,                      # model output channels (number of classes in your dataset)\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-moore",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "complex-label",
   "metadata": {
    "code_folding": [
     68
    ]
   },
   "outputs": [],
   "source": [
    "# augmentation\n",
    "def train_augment(record):\n",
    "    image = record['image']\n",
    "    mask  = record['mask']\n",
    "    \n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask : do_random_rotate_crop(image, mask, size=args.image_size, mag=45),\n",
    "        lambda image, mask : do_random_scale_crop(image, mask, size=args.image_size, mag=0.075),\n",
    "        lambda image, mask : do_random_crop(image, mask, size=args.image_size),\n",
    "    ],1): image, mask = fn(image, mask)\n",
    "\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask : (image, mask),\n",
    "        lambda image, mask : do_random_contast(image, mask, mag=0.8),\n",
    "        lambda image, mask : do_random_gain(image, mask, mag=0.9),\n",
    "        #lambda image, mask : do_random_hsv(image, mask, mag=[0.1, 0.2, 0]),\n",
    "        lambda image, mask : do_random_noise(image, mask, mag=0.1),\n",
    "    ],2): image, mask =  fn(image, mask)\n",
    "    #if (np.random.choice(10,1)<7)[0]:\n",
    "    image, mask = do_random_hsv(image, mask, mag=[0.1, 0.2, 0])\n",
    "    image, mask = do_random_flip_transpose(image, mask)\n",
    "\n",
    "    record['mask'] = mask\n",
    "    record['image'] = image\n",
    "    return record\n",
    "\n",
    "# validation\n",
    "# validation\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2 = [],[]\n",
    "    valid_mask = []\n",
    "\n",
    "    net = net.eval()\n",
    "    lovasz_loss=0\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            batch_size = len(batch['index'])\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "\n",
    "            logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "\n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "            valid_num += batch_size\n",
    "\n",
    "\n",
    "    assert(valid_num == len(valid_loader.dataset))\n",
    "    #print('')\n",
    "    #------\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "\n",
    "    dice = np_dice_score(probability, mask)\n",
    "    tp, tn = np_accuracy(probability, mask)\n",
    "\n",
    "    return [dice, loss,  tp, tn]\n",
    "\n",
    "def run_train(args):\n",
    "    if args.overlap:\n",
    "        out_dir = data_dir + f'/result/{args.dir}_fold{args.fold}_{args.encoder}_{args.image_size}'\n",
    "\n",
    "        ## setup  ----------------------------------------\n",
    "        for f in ['checkpoint','train','valid','backup'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "        #backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n",
    "        log = Logger()\n",
    "        log.open(out_dir+'/log.train.txt',mode='a')\n",
    "\n",
    "        # my log argument\n",
    "        print_args(args, log)\n",
    "\n",
    "        log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "        log.write('\\n')\n",
    "\n",
    "\n",
    "        log.write('** dataset setting **\\n')\n",
    "        #-----------dataset split --------------------#\n",
    "        tile_id = []\n",
    "        image_dir_ = f'{args.dataset}'#'0.25_480_240_train'\n",
    "        image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "\n",
    "        for i in range(len(image_dir)):\n",
    "            df = pd.read_csv(data_dir + '/tile/%s/image_id.csv'% (image_dir[i]) )\n",
    "            tile_id += ('%s/'%(image_dir[i]) + df.tile_id).tolist()\n",
    "\n",
    "        kf = KFold(n_splits=args.n_fold, random_state=args.seed, shuffle=True)\n",
    "        all_dice = []\n",
    "        for n_fold, (trn_idx, val_idx) in enumerate(kf.split(tile_id)):\n",
    "            if not args.all_fold_train:\n",
    "                if n_fold != args.fold:\n",
    "                    print(f'{n_fold} fold pass')\n",
    "                    continue\n",
    "\n",
    "            #####################################################\n",
    "            train_dataset = HuDataset(\n",
    "                tile_id = df.loc[trn_idx]['tile_id'].tolist(),\n",
    "                augment = train_augment\n",
    "            )\n",
    "            train_loader  = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size  = args.batch_size,\n",
    "                drop_last   = False,\n",
    "                num_workers = 8,\n",
    "                pin_memory  = True,\n",
    "                collate_fn  = null_collate\n",
    "            )\n",
    "\n",
    "            valid_dataset = HuDataset(\n",
    "                tile_id = df.loc[val_idx]['tile_id'].tolist()\n",
    "                ,\n",
    "            )\n",
    "            valid_loader = DataLoader(\n",
    "                valid_dataset,\n",
    "                sampler = SequentialSampler(valid_dataset),\n",
    "                batch_size  = args.batch_size,\n",
    "                drop_last   = False,\n",
    "                num_workers = 4,\n",
    "                pin_memory  = True,\n",
    "                collate_fn  = null_collate\n",
    "            )\n",
    "\n",
    "            log.write('fold = %s\\n'%str(n_fold))\n",
    "            log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "            log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n",
    "            log.write('\\n')\n",
    "\n",
    "            ## net ----------------------------------------\n",
    "            log.write('** net setting **\\n')\n",
    "\n",
    "            scaler = GradScaler()\n",
    "            net = SegModel() \n",
    "            net = net.to(device)\n",
    "\n",
    "            if args.opt =='adamw':\n",
    "                optimizer = torch.optim.AdamW(net.parameters(), lr = args.start_lr)\n",
    "                #optimizer = torch.optim.AdamW([\n",
    "                #    {'params': model.decoder.parameters(), 'lr': start_lr}, \n",
    "                #    {'params': model.encoder.parameters(), 'lr': start_lr},  \n",
    "                #])\n",
    "            elif args.opt =='radam_look':\n",
    "                optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=args.start_lr), alpha=0.5, k=5)\n",
    "            if optimizer == None:\n",
    "                assert False, 'no have optimizer'\n",
    "\n",
    "            #if args.scheduler == 'multistep':\n",
    "            #    m_e = args.multistep.split(',')\n",
    "            #    scheduler = MultiStepLR(optimizer, milestones=[int(m_e[0]), int(m_e[1])], gamma=args.multistep_gamma)\n",
    "            if args.scheduler =='CosineAnnealingWarmRestarts':\n",
    "                scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                 T_0 = args.epochs//args.T_0, \n",
    "                                                 T_mult=1, \n",
    "                                                 eta_min=0, \n",
    "                                                 last_epoch=-1)\n",
    "            elif args.scheduler == 'CosineAnnealingLR':\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.min_lr, last_epoch=-1)\n",
    "            else:\n",
    "                scheduler=None\n",
    "\n",
    "\n",
    "            log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "            #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "            log.write('\\n')\n",
    "\n",
    "            ## start training here! ##############################################\n",
    "            #array([0.57142857, 0.42857143])\n",
    "            log.write('** start training here! **\\n')\n",
    "            log.write('   is_mixed_precision = %s \\n'%str(args.amp))\n",
    "            log.write('   batch_size = %d \\n'%(args.batch_size))\n",
    "            log.write('             |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n",
    "            log.write('rate  epoch  | dice   loss   tp     tn     | loss           | time           \\n')\n",
    "            log.write('-------------------------------------------------------------------------------------\\n')\n",
    "                      #0.00100   0.50  0.80 | 0.891  0.020  0.000  0.000  | 0.000  0.000   |  0 hr 02 min\n",
    "\n",
    "            def message(mode='print'):\n",
    "                if mode==('print'):\n",
    "                    asterisk = ' '\n",
    "                    loss = batch_loss\n",
    "                if mode==('log'):\n",
    "                    asterisk = '*'\n",
    "                    loss = train_loss\n",
    "\n",
    "                text = \\\n",
    "                    '%0.5f  %s%s    | '%(rate, epoch, asterisk,) +\\\n",
    "                    '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n",
    "                    '%4.3f  %4.3f   | '%(*loss,) +\\\n",
    "                    '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "\n",
    "                return text\n",
    "\n",
    "            #----\n",
    "            valid_loss = np.zeros(4,np.float32)\n",
    "            train_loss = np.zeros(2,np.float32)\n",
    "            batch_loss = np.zeros_like(train_loss)\n",
    "            sum_train_loss = np.zeros_like(train_loss)\n",
    "            sum_train = 0\n",
    "            loss = torch.FloatTensor([0]).sum()\n",
    "\n",
    "\n",
    "            start_timer = timer()\n",
    "            rate = 0\n",
    "            best_dice = 0\n",
    "            #while  iteration < num_iteration:\n",
    "            for epoch in range(args.epochs):\n",
    "                print('\\r',end='',flush=True)\n",
    "                log.write(message(mode='log')+'\\n')\n",
    "                # training\n",
    "                for t, batch in enumerate(train_loader):\n",
    "\n",
    "                    # learning rate schduler -------------\n",
    "                    #adjust_learning_rate(optimizer, schduler(iteration))\n",
    "                    rate = get_learning_rate(optimizer)\n",
    "\n",
    "                    # one iteration update  -------------\n",
    "                    batch_size = len(batch['index'])\n",
    "                    net.train()\n",
    "\n",
    "                    if args.amp:\n",
    "                        #image = image.half()\n",
    "                        with autocast():\n",
    "                            mask  = batch['mask'].to(device)\n",
    "                            image = batch['image'].to(device)\n",
    "\n",
    "                            optimizer.zero_grad()\n",
    "                            #logit = data_parallel(net, image)\n",
    "                            logit = net(image)\n",
    "                            if args.loss == 'bce':\n",
    "                                loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                            elif args.loss =='lovasz':\n",
    "                                loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                    else :\n",
    "                        mask  = batch['mask'].to(device)\n",
    "                        image = batch['image'].to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        #logit = data_parallel(net, image)\n",
    "                        logit = net(image)\n",
    "                        if args.loss == 'bce':\n",
    "                            loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                        elif args.loss =='lovasz':\n",
    "                            loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                    # print statistics  --------\n",
    "\n",
    "                    batch_loss = np.array([ loss.item(), 0 ])\n",
    "                    sum_train_loss += batch_loss\n",
    "                    sum_train += 1\n",
    "\n",
    "                    print('\\r',end='',flush=True)\n",
    "                    print(message(mode='print'), end='',flush=True)\n",
    "\n",
    "                # train loss\n",
    "                train_loss = sum_train_loss/(sum_train+1e-12)\n",
    "                sum_train_loss[...] = 0\n",
    "                sum_train = 0\n",
    "\n",
    "                # scheudler\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                # validation\n",
    "                valid_loss = do_valid(net, valid_loader) #\n",
    "\n",
    "                # saved models\n",
    "                if valid_loss[0] > best_dice:\n",
    "                    best_dice = valid_loss[0]\n",
    "                    print(f'\\n saved best models, dice:{best_dice:.5f}')\n",
    "                    torch.save({\n",
    "                        'state_dict': net.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                    }, out_dir + f'/checkpoint/{n_fold}fold_{epoch}epoch_{best_dice:.4f}_model.pth')\n",
    "\n",
    "            log.write('\\n')\n",
    "            all_dice.append(best_dice)\n",
    "\n",
    "            print(f'all dice score : {sum(all_dice)/len(all_dice) : .4f}')\n",
    "    else:  # no overlap\n",
    "        out_dir = data_dir + f'/result/{args.dir}_fold{args.fold}_{args.encoder}_{args.image_size}'\n",
    "\n",
    "        ## setup  ----------------------------------------\n",
    "        for f in ['checkpoint','train','valid','backup'] : \n",
    "            os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "        #backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n",
    "        log = Logger()\n",
    "        log.open(out_dir+'/log.train.txt',mode='a')\n",
    "\n",
    "        # my log argument\n",
    "        print_args(args, log)\n",
    "\n",
    "        log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "        log.write('\\n')\n",
    "\n",
    "\n",
    "        log.write('** dataset setting **\\n')\n",
    "        #-----------dataset split --------------------#\n",
    "        all_dice = []\n",
    "        for n_fold in range(1,6):\n",
    "            train_dataset = HuDataset_nol(data_dir, args.dataset, n_fold, train=True, augment=train_augment)\n",
    "            train_loader  = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size  = args.batch_size,\n",
    "                drop_last   = False,\n",
    "                num_workers = 8,\n",
    "                pin_memory  = True,\n",
    "                collate_fn  = null_collate\n",
    "            )\n",
    "            valid_dataset = HuDataset_nol(data_dir, args.dataset, n_fold, train=False, augment=None)\n",
    "            valid_loader = DataLoader(\n",
    "                valid_dataset,\n",
    "                sampler = SequentialSampler(valid_dataset),\n",
    "                batch_size  = args.batch_size,\n",
    "                drop_last   = False,\n",
    "                num_workers = 4,\n",
    "                pin_memory  = True,\n",
    "                collate_fn  = null_collate\n",
    "            )\n",
    "\n",
    "            log.write('fold = %s\\n'%str(n_fold))\n",
    "            log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "            log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n",
    "            log.write('\\n')\n",
    "\n",
    "            ## net ----------------------------------------\n",
    "            log.write('** net setting **\\n')\n",
    "\n",
    "            scaler = GradScaler()\n",
    "            net = SegModel() \n",
    "            net = net.to(device)\n",
    "\n",
    "            if args.opt =='adamw':\n",
    "                optimizer = torch.optim.AdamW(net.parameters(), lr = args.start_lr)\n",
    "                #optimizer = torch.optim.AdamW([\n",
    "                #    {'params': model.decoder.parameters(), 'lr': start_lr}, \n",
    "                #    {'params': model.encoder.parameters(), 'lr': start_lr},  \n",
    "                #])\n",
    "            elif args.opt =='radam_look':\n",
    "                optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=args.start_lr), alpha=0.5, k=5)\n",
    "            if optimizer == None:\n",
    "                assert False, 'no have optimizer'\n",
    "\n",
    "            #if args.scheduler == 'multistep':\n",
    "            #    m_e = args.multistep.split(',')\n",
    "            #    scheduler = MultiStepLR(optimizer, milestones=[int(m_e[0]), int(m_e[1])], gamma=args.multistep_gamma)\n",
    "            if args.scheduler =='CosineAnnealingWarmRestarts':\n",
    "                scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                 T_0 = args.epochs//args.T_0, \n",
    "                                                 T_mult=1, \n",
    "                                                 eta_min=0, \n",
    "                                                 last_epoch=-1)\n",
    "            elif args.scheduler == 'CosineAnnealingLR':\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.min_lr, last_epoch=-1)\n",
    "            else:\n",
    "                scheduler=None\n",
    "\n",
    "\n",
    "            log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "            #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "            log.write('\\n')\n",
    "\n",
    "            ## start training here! ##############################################\n",
    "            #array([0.57142857, 0.42857143])\n",
    "            log.write('** start training here! **\\n')\n",
    "            log.write('   is_mixed_precision = %s \\n'%str(args.amp))\n",
    "            log.write('   batch_size = %d \\n'%(args.batch_size))\n",
    "            log.write('             |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n",
    "            log.write('rate  epoch  | dice   loss   tp     tn     | loss           | time           \\n')\n",
    "            log.write('-------------------------------------------------------------------------------------\\n')\n",
    "                      #0.00100   0.50  0.80 | 0.891  0.020  0.000  0.000  | 0.000  0.000   |  0 hr 02 min\n",
    "\n",
    "            def message(mode='print'):\n",
    "                if mode==('print'):\n",
    "                    asterisk = ' '\n",
    "                    loss = batch_loss\n",
    "                if mode==('log'):\n",
    "                    asterisk = '*'\n",
    "                    loss = train_loss\n",
    "\n",
    "                text = \\\n",
    "                    '%0.5f  %s%s    | '%(rate, epoch, asterisk,) +\\\n",
    "                    '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n",
    "                    '%4.3f  %4.3f   | '%(*loss,) +\\\n",
    "                    '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "\n",
    "                return text\n",
    "\n",
    "            #----\n",
    "            valid_loss = np.zeros(4,np.float32)\n",
    "            train_loss = np.zeros(2,np.float32)\n",
    "            batch_loss = np.zeros_like(train_loss)\n",
    "            sum_train_loss = np.zeros_like(train_loss)\n",
    "            sum_train = 0\n",
    "            loss = torch.FloatTensor([0]).sum()\n",
    "\n",
    "\n",
    "            start_timer = timer()\n",
    "            rate = 0\n",
    "            best_dice = 0\n",
    "            #while  iteration < num_iteration:\n",
    "            for epoch in range(args.epochs):\n",
    "                print('\\r',end='',flush=True)\n",
    "                log.write(message(mode='log')+'\\n')\n",
    "                # training\n",
    "                for t, batch in enumerate(train_loader):\n",
    "\n",
    "                    # learning rate schduler -------------\n",
    "                    #adjust_learning_rate(optimizer, schduler(iteration))\n",
    "                    rate = get_learning_rate(optimizer)\n",
    "\n",
    "                    # one iteration update  -------------\n",
    "                    batch_size = len(batch['index'])\n",
    "                    net.train()\n",
    "\n",
    "                    if args.amp:\n",
    "                        #image = image.half()\n",
    "                        with autocast():\n",
    "                            mask  = batch['mask'].to(device)\n",
    "                            image = batch['image'].to(device)\n",
    "\n",
    "                            optimizer.zero_grad()\n",
    "                            #logit = data_parallel(net, image)\n",
    "                            logit = net(image)\n",
    "                            if args.loss == 'bce':\n",
    "                                loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                            elif args.loss =='lovasz':\n",
    "                                loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                    else :\n",
    "                        mask  = batch['mask'].to(device)\n",
    "                        image = batch['image'].to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        #logit = data_parallel(net, image)\n",
    "                        logit = net(image)\n",
    "                        if args.loss == 'bce':\n",
    "                            loss = criterion_binary_cross_entropy(logit, mask)\n",
    "                        elif args.loss =='lovasz':\n",
    "                            loss = symmetric_lovasz(logit, mask)\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                    # print statistics  --------\n",
    "\n",
    "                    batch_loss = np.array([ loss.item(), 0 ])\n",
    "                    sum_train_loss += batch_loss\n",
    "                    sum_train += 1\n",
    "\n",
    "                    print('\\r',end='',flush=True)\n",
    "                    print(message(mode='print'), end='',flush=True)\n",
    "\n",
    "                # train loss\n",
    "                train_loss = sum_train_loss/(sum_train+1e-12)\n",
    "                sum_train_loss[...] = 0\n",
    "                sum_train = 0\n",
    "\n",
    "                # scheudler\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                # validation\n",
    "                valid_loss = do_valid(net, valid_loader) #\n",
    "\n",
    "                # saved models\n",
    "                if valid_loss[0] > best_dice:\n",
    "                    best_dice = valid_loss[0]\n",
    "                    print(f'\\n saved best models, dice:{best_dice:.5f}')\n",
    "                    torch.save({\n",
    "                        'state_dict': net.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                    }, out_dir + f'/checkpoint/{n_fold}fold_{epoch}epoch_{best_dice:.4f}_model.pth')\n",
    "\n",
    "            log.write('\\n')\n",
    "            all_dice.append(best_dice)\n",
    "\n",
    "            print(f'all dice score : {sum(all_dice)/len(all_dice) : .4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-speaker",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__module__       : __main__amp              : Truegpu              : 3dir              : 100epoch_nooverlap_640_25_50encoder          : b4decoder          : unetbatch_size       : 16weight_decay     : 1e-06n_fold           : 5fold             : 0all_fold_train   : Trueimage_size       : 512dataset          : 0.25_640_320_trainoverlap          : Falseepochs           : 100opt              : radam_lookscheduler        : CosineAnnealingLRloss             : bcefactor           : 0.2patience         : 2eps              : 1e-06T_max            : 10decay_epoch      : [4, 8, 12]T_0              : 4start_lr         : 0.001min_lr           : 1e-06num_workers      : 8seed             : 42__dict__         : <attribute '__dict__' of 'args' objects>__weakref__      : <attribute '__weakref__' of 'args' objects>__doc__          : None\tout_dir  = /home/jeonghokim/competition/HubMap/data//result/100epoch_nooverlap_640_25_50_fold0_b4_512\n",
      "\n",
      "** dataset setting **\n",
      "train => # imgs : 4556, # masks : 4556\n",
      "valid => # imgs : 948, # masks : 948\n",
      "fold = 1\n",
      "train_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fcd932964c0>\n",
      "valid_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fcd93296ac0>\n",
      "\n",
      "** net setting **\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "optimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "0.00000  0*    | 0.000  0.000  0.000  0.000  | 0.000  0.000   |  0 hr 00 min\n",
      "0.00100  0     | 0.000  0.000  0.000  0.000  | 0.108  0.000   |  0 hr 02 min\n",
      " saved best models, dice:0.91916\n",
      "0.00100  1*    | 0.919  0.095  0.924  0.997  | 0.280  0.000   |  0 hr 03 min\n",
      "0.00098  1     | 0.919  0.095  0.924  0.997  | 0.035  0.000   |  0 hr 05 min\n",
      " saved best models, dice:0.92692\n",
      "0.00098  2*    | 0.927  0.034  0.938  0.997  | 0.058  0.000   |  0 hr 06 min\n",
      "0.00090  2     | 0.927  0.034  0.938  0.997  | 0.016  0.000   |  0 hr 08 min\n",
      " saved best models, dice:0.92977\n",
      "0.00090  3*    | 0.930  0.022  0.930  0.997  | 0.026  0.000   |  0 hr 09 min\n",
      "0.00079  3     | 0.930  0.022  0.930  0.997  | 0.014  0.000   |  0 hr 11 min\n",
      " saved best models, dice:0.93295\n",
      "0.00079  4*    | 0.933  0.018  0.951  0.996  | 0.018  0.000   |  0 hr 12 min\n",
      "0.00065  4     | 0.933  0.018  0.951  0.996  | 0.011  0.000   |  0 hr 15 min\n",
      " saved best models, dice:0.93583\n",
      "0.00065  5*    | 0.936  0.016  0.941  0.997  | 0.015  0.000   |  0 hr 15 min\n",
      "0.00050  6*    | 0.934  0.016  0.964  0.996  | 0.013  0.000   |  0 hr 18 min\n",
      "0.00035  6     | 0.934  0.016  0.964  0.996  | 0.008  0.000   |  0 hr 21 min\n",
      " saved best models, dice:0.93641\n",
      "0.00035  7*    | 0.936  0.014  0.948  0.997  | 0.012  0.000   |  0 hr 22 min\n",
      "0.00021  8*    | 0.936  0.015  0.956  0.997  | 0.012  0.000   |  0 hr 25 min\n",
      "0.00010  8     | 0.936  0.015  0.956  0.997  | 0.017  0.000   |  0 hr 27 min\n",
      " saved best models, dice:0.93834\n",
      "0.00010  9*    | 0.938  0.014  0.951  0.997  | 0.011  0.000   |  0 hr 28 min\n",
      "0.00003  10*    | 0.938  0.014  0.952  0.997  | 0.010  0.000   |  0 hr 31 min\n",
      "0.00000  11*    | 0.938  0.014  0.952  0.997  | 0.010  0.000   |  0 hr 34 min\n",
      "0.00003  12*    | 0.938  0.014  0.951  0.997  | 0.010  0.000   |  0 hr 37 min\n",
      "0.00010  12     | 0.938  0.014  0.951  0.997  | 0.013  0.000   |  0 hr 40 min\n",
      " saved best models, dice:0.93847\n",
      "0.00010  13*    | 0.938  0.014  0.948  0.997  | 0.010  0.000   |  0 hr 40 min\n",
      "0.00021  14*    | 0.938  0.014  0.947  0.997  | 0.010  0.000   |  0 hr 44 min\n",
      "0.00035  15*    | 0.937  0.014  0.951  0.997  | 0.010  0.000   |  0 hr 47 min\n",
      "0.00050  16*    | 0.938  0.013  0.951  0.997  | 0.010  0.000   |  0 hr 50 min\n",
      "0.00065  17*    | 0.938  0.014  0.951  0.997  | 0.010  0.000   |  0 hr 53 min\n",
      "0.00079  18*    | 0.937  0.014  0.942  0.997  | 0.011  0.000   |  0 hr 56 min\n",
      "0.00090  19*    | 0.937  0.013  0.952  0.997  | 0.011  0.000   |  0 hr 59 min\n",
      "0.00098  20*    | 0.894  0.026  0.949  0.993  | 0.011  0.000   |  1 hr 03 min\n",
      "0.00100  21*    | 0.935  0.013  0.935  0.997  | 0.010  0.000   |  1 hr 06 min\n",
      "0.00098  21     | 0.935  0.013  0.935  0.997  | 0.007  0.000   |  1 hr 08 min\n",
      " saved best models, dice:0.93863\n",
      "0.00098  22*    | 0.939  0.013  0.944  0.997  | 0.010  0.000   |  1 hr 09 min\n",
      "0.00090  23*    | 0.939  0.013  0.945  0.997  | 0.009  0.000   |  1 hr 12 min\n",
      "0.00079  24*    | 0.936  0.014  0.944  0.997  | 0.009  0.000   |  1 hr 15 min\n",
      "0.00065  25*    | 0.939  0.013  0.946  0.997  | 0.008  0.000   |  1 hr 18 min\n",
      "0.00050  26*    | 0.938  0.014  0.949  0.997  | 0.008  0.000   |  1 hr 22 min\n",
      "0.00035  26     | 0.938  0.014  0.949  0.997  | 0.007  0.000   |  1 hr 24 min\n",
      " saved best models, dice:0.93876\n",
      "0.00035  27*    | 0.939  0.014  0.948  0.997  | 0.008  0.000   |  1 hr 25 min\n",
      "0.00021  28*    | 0.938  0.014  0.955  0.997  | 0.008  0.000   |  1 hr 28 min\n",
      "0.00010  28     | 0.938  0.014  0.955  0.997  | 0.008  0.000   |  1 hr 31 min\n",
      " saved best models, dice:0.93961\n",
      "0.00010  29*    | 0.940  0.014  0.951  0.997  | 0.007  0.000   |  1 hr 31 min\n",
      "0.00003  29     | 0.940  0.014  0.951  0.997  | 0.005  0.000   |  1 hr 34 min\n",
      " saved best models, dice:0.93987\n",
      "0.00003  30*    | 0.940  0.014  0.951  0.997  | 0.007  0.000   |  1 hr 34 min\n",
      "0.00000  31*    | 0.940  0.014  0.951  0.997  | 0.007  0.000   |  1 hr 37 min\n",
      "0.00003  31     | 0.940  0.014  0.951  0.997  | 0.009  0.000   |  1 hr 40 min\n",
      " saved best models, dice:0.93989\n",
      "0.00003  32*    | 0.940  0.014  0.951  0.997  | 0.007  0.000   |  1 hr 41 min\n",
      "0.00010  33*    | 0.939  0.014  0.952  0.997  | 0.007  0.000   |  1 hr 44 min\n",
      "0.00021  34*    | 0.939  0.014  0.948  0.997  | 0.007  0.000   |  1 hr 47 min\n",
      "0.00035  35*    | 0.938  0.014  0.944  0.997  | 0.007  0.000   |  1 hr 50 min\n",
      "0.00050  36*    | 0.938  0.015  0.948  0.997  | 0.008  0.000   |  1 hr 53 min\n",
      "0.00065  37*    | 0.933  0.015  0.945  0.997  | 0.008  0.000   |  1 hr 56 min\n",
      "0.00079  38*    | 0.939  0.014  0.940  0.997  | 0.008  0.000   |  2 hr 00 min\n",
      "0.00090  39*    | 0.931  0.016  0.941  0.997  | 0.009  0.000   |  2 hr 03 min\n",
      "0.00098  40*    | 0.741  0.108  0.987  0.973  | 0.009  0.000   |  2 hr 06 min\n",
      "0.00100  41*    | 0.935  0.013  0.916  0.998  | 0.009  0.000   |  2 hr 09 min\n",
      "0.00098  42*    | 0.938  0.014  0.945  0.997  | 0.008  0.000   |  2 hr 12 min\n",
      "0.00090  43*    | 0.937  0.015  0.953  0.997  | 0.008  0.000   |  2 hr 16 min\n",
      "0.00079  44*    | 0.937  0.014  0.958  0.997  | 0.008  0.000   |  2 hr 19 min\n",
      "0.00065  45*    | 0.939  0.015  0.947  0.997  | 0.008  0.000   |  2 hr 22 min\n",
      "0.00050  46*    | 0.939  0.015  0.952  0.997  | 0.007  0.000   |  2 hr 25 min\n",
      "0.00035  46     | 0.939  0.015  0.952  0.997  | 0.009  0.000   |  2 hr 28 min\n",
      " saved best models, dice:0.94012\n",
      "0.00035  47*    | 0.940  0.014  0.942  0.998  | 0.007  0.000   |  2 hr 28 min\n",
      "0.00021  48*    | 0.940  0.015  0.951  0.997  | 0.007  0.000   |  2 hr 31 min\n",
      "0.00010  49*    | 0.940  0.015  0.952  0.997  | 0.007  0.000   |  2 hr 35 min\n",
      "0.00003  50*    | 0.940  0.015  0.950  0.997  | 0.007  0.000   |  2 hr 38 min\n",
      "0.00000  51*    | 0.940  0.015  0.952  0.997  | 0.007  0.000   |  2 hr 41 min\n",
      "0.00003  51     | 0.940  0.015  0.952  0.997  | 0.004  0.000   |  2 hr 43 min\n",
      " saved best models, dice:0.94049\n",
      "0.00003  52*    | 0.940  0.015  0.950  0.997  | 0.007  0.000   |  2 hr 44 min\n",
      "0.00010  53*    | 0.940  0.015  0.950  0.997  | 0.007  0.000   |  2 hr 47 min\n",
      "0.00021  54*    | 0.940  0.015  0.944  0.997  | 0.007  0.000   |  2 hr 50 min\n",
      "0.00035  55*    | 0.939  0.015  0.949  0.997  | 0.007  0.000   |  2 hr 53 min\n",
      "0.00050  56*    | 0.940  0.015  0.950  0.997  | 0.007  0.000   |  2 hr 57 min\n",
      "0.00065  57*    | 0.938  0.016  0.949  0.997  | 0.007  0.000   |  3 hr 00 min\n",
      "0.00079  58*    | 0.935  0.015  0.950  0.997  | 0.007  0.000   |  3 hr 03 min\n",
      "0.00090  59*    | 0.939  0.014  0.951  0.997  | 0.008  0.000   |  3 hr 06 min\n",
      "0.00098  60*    | 0.922  0.018  0.910  0.997  | 0.008  0.000   |  3 hr 10 min\n",
      "0.00100  61*    | 0.938  0.014  0.950  0.997  | 0.008  0.000   |  3 hr 13 min\n",
      "0.00098  62*    | 0.939  0.014  0.948  0.997  | 0.008  0.000   |  3 hr 16 min\n",
      "0.00090  63*    | 0.938  0.014  0.949  0.997  | 0.008  0.000   |  3 hr 19 min\n",
      "0.00079  64*    | 0.939  0.014  0.949  0.997  | 0.007  0.000   |  3 hr 23 min\n",
      "0.00065  65*    | 0.940  0.015  0.940  0.998  | 0.007  0.000   |  3 hr 26 min\n",
      "0.00050  66*    | 0.939  0.015  0.949  0.997  | 0.007  0.000   |  3 hr 29 min\n",
      "0.00035  67*    | 0.939  0.015  0.949  0.997  | 0.007  0.000   |  3 hr 32 min\n",
      "0.00021  68*    | 0.940  0.015  0.948  0.997  | 0.007  0.000   |  3 hr 35 min\n",
      "0.00010  69*    | 0.940  0.015  0.949  0.997  | 0.006  0.000   |  3 hr 38 min\n",
      "0.00003  70*    | 0.940  0.015  0.951  0.997  | 0.006  0.000   |  3 hr 42 min\n",
      "0.00000  71*    | 0.940  0.015  0.950  0.997  | 0.007  0.000   |  3 hr 45 min\n",
      "0.00003  72*    | 0.940  0.015  0.952  0.997  | 0.006  0.000   |  3 hr 48 min\n",
      "0.00010  73*    | 0.940  0.015  0.949  0.997  | 0.006  0.000   |  3 hr 51 min\n",
      "0.00021  74*    | 0.940  0.015  0.947  0.997  | 0.006  0.000   |  3 hr 54 min\n",
      "0.00035  75*    | 0.939  0.016  0.946  0.997  | 0.006  0.000   |  3 hr 57 min\n",
      "0.00050  76*    | 0.940  0.015  0.946  0.997  | 0.007  0.000   |  4 hr 00 min\n",
      "0.00065  77*    | 0.936  0.016  0.942  0.997  | 0.007  0.000   |  4 hr 03 min\n",
      "0.00079  78*    | 0.937  0.015  0.946  0.997  | 0.007  0.000   |  4 hr 06 min\n",
      "0.00090  79*    | 0.938  0.016  0.947  0.997  | 0.007  0.000   |  4 hr 09 min\n",
      "0.00098  80*    | 0.938  0.015  0.946  0.997  | 0.007  0.000   |  4 hr 12 min\n",
      "0.00100  81*    | 0.934  0.016  0.924  0.998  | 0.008  0.000   |  4 hr 15 min\n",
      "0.00098  82*    | 0.937  0.016  0.937  0.997  | 0.008  0.000   |  4 hr 18 min\n",
      "0.00090  83*    | 0.938  0.015  0.949  0.997  | 0.007  0.000   |  4 hr 21 min\n",
      "0.00079  84*    | 0.937  0.016  0.940  0.997  | 0.007  0.000   |  4 hr 24 min\n",
      "0.00065  85*    | 0.937  0.016  0.953  0.997  | 0.007  0.000   |  4 hr 28 min\n",
      "0.00050  86*    | 0.938  0.016  0.944  0.997  | 0.007  0.000   |  4 hr 31 min\n",
      "0.00035  87*    | 0.939  0.016  0.950  0.997  | 0.007  0.000   |  4 hr 34 min\n",
      "0.00021  88*    | 0.939  0.016  0.950  0.997  | 0.006  0.000   |  4 hr 37 min\n",
      "0.00010  89*    | 0.939  0.016  0.950  0.997  | 0.006  0.000   |  4 hr 40 min\n",
      "0.00003  90*    | 0.939  0.017  0.951  0.997  | 0.006  0.000   |  4 hr 43 min\n",
      "0.00000  91*    | 0.939  0.016  0.950  0.997  | 0.006  0.000   |  4 hr 46 min\n",
      "0.00003  92*    | 0.939  0.016  0.950  0.997  | 0.006  0.000   |  4 hr 49 min\n",
      "0.00010  93*    | 0.939  0.016  0.950  0.997  | 0.006  0.000   |  4 hr 52 min\n",
      "0.00021  94*    | 0.940  0.017  0.945  0.997  | 0.006  0.000   |  4 hr 55 min\n",
      "0.00035  95*    | 0.939  0.016  0.947  0.997  | 0.006  0.000   |  4 hr 58 min\n",
      "0.00050  96*    | 0.940  0.016  0.947  0.997  | 0.006  0.000   |  5 hr 01 min\n",
      "0.00065  97*    | 0.939  0.016  0.957  0.997  | 0.006  0.000   |  5 hr 04 min\n",
      "0.00079  98*    | 0.938  0.016  0.951  0.997  | 0.007  0.000   |  5 hr 07 min\n",
      "0.00090  99*    | 0.939  0.014  0.945  0.997  | 0.007  0.000   |  5 hr 10 min\n",
      "0.00098  99     | 0.939  0.014  0.945  0.997  | 0.005  0.000   |  5 hr 13 min\n",
      "all dice score :  0.9405\n",
      "train => # imgs : 4091, # masks : 4091\n",
      "valid => # imgs : 1413, # masks : 1413\n",
      "fold = 2\n",
      "train_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fcd8865bf10>\n",
      "valid_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fcd8865b910>\n",
      "\n",
      "** net setting **\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "optimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "0.00000  0*    | 0.000  0.000  0.000  0.000  | 0.000  0.000   |  0 hr 00 min\n",
      "0.00100  0     | 0.000  0.000  0.000  0.000  | 0.175  0.000   |  0 hr 02 min\n",
      " saved best models, dice:0.91390\n",
      "0.00100  1*    | 0.914  0.174  0.926  0.997  | 0.404  0.000   |  0 hr 03 min\n",
      "0.00098  1     | 0.914  0.174  0.926  0.997  | 0.054  0.000   |  0 hr 05 min\n",
      " saved best models, dice:0.92619\n",
      "0.00098  2*    | 0.926  0.052  0.924  0.998  | 0.097  0.000   |  0 hr 06 min\n",
      "0.00090  2     | 0.926  0.052  0.924  0.998  | 0.028  0.000   |  0 hr 08 min\n",
      " saved best models, dice:0.93439\n",
      "0.00090  3*    | 0.934  0.027  0.926  0.998  | 0.040  0.000   |  0 hr 09 min\n",
      "0.00079  4*    | 0.933  0.020  0.950  0.997  | 0.025  0.000   |  0 hr 12 min\n",
      "0.00065  4     | 0.933  0.020  0.950  0.997  | 0.020  0.000   |  0 hr 14 min\n",
      " saved best models, dice:0.93831\n",
      "0.00065  5*    | 0.938  0.016  0.934  0.998  | 0.019  0.000   |  0 hr 15 min\n",
      "0.00050  5     | 0.938  0.016  0.934  0.998  | 0.011  0.000   |  0 hr 17 min\n",
      " saved best models, dice:0.94070\n",
      "0.00050  6*    | 0.941  0.014  0.934  0.998  | 0.017  0.000   |  0 hr 18 min\n",
      "0.00035  7*    | 0.939  0.014  0.945  0.998  | 0.014  0.000   |  0 hr 21 min\n",
      "0.00021  8*    | 0.940  0.013  0.937  0.998  | 0.013  0.000   |  0 hr 24 min\n",
      "0.00010  9*    | 0.939  0.013  0.932  0.998  | 0.013  0.000   |  0 hr 27 min\n",
      "0.00003  10*    | 0.940  0.013  0.930  0.998  | 0.012  0.000   |  0 hr 30 min\n",
      "0.00000  11*    | 0.940  0.013  0.931  0.998  | 0.012  0.000   |  0 hr 33 min\n",
      "0.00003  12*    | 0.941  0.013  0.934  0.998  | 0.012  0.000   |  0 hr 36 min\n",
      "0.00010  12     | 0.941  0.013  0.934  0.998  | 0.008  0.000   |  0 hr 39 min\n",
      " saved best models, dice:0.94187\n",
      "0.00010  13*    | 0.942  0.012  0.940  0.998  | 0.012  0.000   |  0 hr 39 min\n",
      "0.00021  14*    | 0.941  0.013  0.949  0.998  | 0.012  0.000   |  0 hr 42 min\n",
      "0.00035  15*    | 0.940  0.011  0.936  0.998  | 0.011  0.000   |  0 hr 45 min\n",
      "0.00050  16*    | 0.938  0.012  0.940  0.998  | 0.011  0.000   |  0 hr 49 min\n",
      "0.00065  17*    | 0.942  0.011  0.947  0.998  | 0.011  0.000   |  0 hr 52 min\n",
      "0.00079  18*    | 0.938  0.011  0.950  0.997  | 0.012  0.000   |  0 hr 55 min\n",
      "0.00090  19*    | 0.941  0.011  0.940  0.998  | 0.011  0.000   |  0 hr 58 min\n",
      "0.00098  20*    | 0.941  0.011  0.934  0.998  | 0.011  0.000   |  1 hr 01 min\n",
      "0.00100  21*    | 0.941  0.010  0.938  0.998  | 0.010  0.000   |  1 hr 04 min\n",
      "0.00098  22*    | 0.939  0.011  0.934  0.998  | 0.010  0.000   |  1 hr 07 min\n",
      "0.00090  23*    | 0.937  0.011  0.914  0.999  | 0.010  0.000   |  1 hr 10 min\n",
      "0.00079  24*    | 0.940  0.010  0.932  0.998  | 0.009  0.000   |  1 hr 13 min\n",
      "0.00065  25*    | 0.940  0.011  0.926  0.998  | 0.009  0.000   |  1 hr 16 min\n",
      "0.00050  25     | 0.940  0.011  0.926  0.998  | 0.011  0.000   |  1 hr 18 min\n",
      " saved best models, dice:0.94223\n",
      "0.00050  26*    | 0.942  0.010  0.932  0.998  | 0.009  0.000   |  1 hr 19 min\n",
      "0.00035  27*    | 0.942  0.010  0.934  0.998  | 0.008  0.000   |  1 hr 22 min\n",
      "0.00021  27     | 0.942  0.010  0.934  0.998  | 0.007  0.000   |  1 hr 24 min\n",
      " saved best models, dice:0.94333\n",
      "0.00021  28*    | 0.943  0.010  0.939  0.998  | 0.008  0.000   |  1 hr 25 min\n",
      "0.00010  29*    | 0.943  0.010  0.933  0.998  | 0.008  0.000   |  1 hr 28 min\n",
      "0.00003  30*    | 0.943  0.010  0.934  0.998  | 0.008  0.000   |  1 hr 31 min\n",
      "0.00000  31*    | 0.943  0.010  0.935  0.998  | 0.007  0.000   |  1 hr 34 min\n",
      "0.00003  32*    | 0.943  0.010  0.935  0.998  | 0.008  0.000   |  1 hr 37 min\n",
      "0.00010  33*    | 0.943  0.010  0.935  0.998  | 0.007  0.000   |  1 hr 40 min\n",
      "0.00021  34*    | 0.940  0.011  0.924  0.999  | 0.008  0.000   |  1 hr 44 min\n",
      "0.00035  35*    | 0.942  0.010  0.938  0.998  | 0.007  0.000   |  1 hr 47 min\n",
      "0.00050  36*    | 0.943  0.011  0.940  0.998  | 0.008  0.000   |  1 hr 50 min\n",
      "0.00065  37*    | 0.943  0.011  0.929  0.999  | 0.008  0.000   |  1 hr 53 min\n",
      "0.00079  38*    | 0.938  0.012  0.926  0.998  | 0.009  0.000   |  1 hr 56 min\n",
      "0.00090  39*    | 0.931  0.013  0.902  0.999  | 0.010  0.000   |  1 hr 59 min\n",
      "0.00098  40*    | 0.939  0.011  0.944  0.998  | 0.009  0.000   |  2 hr 02 min\n",
      "0.00100  41*    | 0.939  0.012  0.934  0.998  | 0.009  0.000   |  2 hr 05 min\n",
      "0.00098  42*    | 0.942  0.011  0.927  0.999  | 0.009  0.000   |  2 hr 08 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00090  43*    | 0.941  0.010  0.925  0.999  | 0.008  0.000   |  2 hr 11 min\n",
      "0.00079  44*    | 0.943  0.010  0.935  0.998  | 0.008  0.000   |  2 hr 14 min\n",
      "0.00065  44     | 0.943  0.010  0.935  0.998  | 0.007  0.000   |  2 hr 16 min\n",
      " saved best models, dice:0.94337\n",
      "0.00065  45*    | 0.943  0.010  0.934  0.998  | 0.008  0.000   |  2 hr 17 min\n",
      "0.00050  46*    | 0.940  0.011  0.920  0.999  | 0.008  0.000   |  2 hr 20 min\n",
      "0.00035  47*    | 0.939  0.011  0.920  0.999  | 0.008  0.000   |  2 hr 23 min\n",
      "0.00021  48*    | 0.940  0.011  0.922  0.999  | 0.007  0.000   |  2 hr 26 min\n",
      "0.00010  49*    | 0.942  0.011  0.929  0.998  | 0.007  0.000   |  2 hr 29 min\n",
      "0.00003  50*    | 0.941  0.011  0.927  0.999  | 0.007  0.000   |  2 hr 32 min\n",
      "0.00000  51*    | 0.942  0.011  0.929  0.999  | 0.007  0.000   |  2 hr 35 min\n",
      "0.00003  52*    | 0.942  0.011  0.929  0.999  | 0.007  0.000   |  2 hr 38 min\n",
      "0.00010  53*    | 0.942  0.011  0.928  0.999  | 0.007  0.000   |  2 hr 41 min\n",
      "0.00021  54*    | 0.941  0.011  0.929  0.998  | 0.007  0.000   |  2 hr 44 min\n",
      "0.00035  55*    | 0.942  0.011  0.929  0.998  | 0.007  0.000   |  2 hr 48 min\n",
      "0.00050  56*    | 0.939  0.011  0.929  0.998  | 0.007  0.000   |  2 hr 51 min\n",
      "0.00065  57*    | 0.940  0.011  0.934  0.998  | 0.008  0.000   |  2 hr 54 min\n",
      "0.00079  58*    | 0.939  0.011  0.921  0.999  | 0.008  0.000   |  2 hr 57 min\n",
      "0.00090  59*    | 0.938  0.012  0.924  0.998  | 0.008  0.000   |  3 hr 00 min\n",
      "0.00098  60*    | 0.939  0.011  0.923  0.999  | 0.009  0.000   |  3 hr 03 min\n",
      "0.00100  61*    | 0.942  0.011  0.943  0.998  | 0.008  0.000   |  3 hr 06 min\n",
      "0.00098  61     | 0.942  0.011  0.943  0.998  | 0.004  0.000   |  3 hr 08 min\n",
      " saved best models, dice:0.94374\n",
      "0.00098  62*    | 0.944  0.010  0.939  0.998  | 0.008  0.000   |  3 hr 09 min\n",
      "0.00090  63*    | 0.940  0.011  0.923  0.999  | 0.008  0.000   |  3 hr 12 min\n",
      "0.00079  64*    | 0.939  0.011  0.917  0.999  | 0.008  0.000   |  3 hr 15 min\n",
      "0.00065  65*    | 0.939  0.012  0.925  0.999  | 0.007  0.000   |  3 hr 18 min\n",
      "0.00050  66*    | 0.941  0.011  0.932  0.998  | 0.007  0.000   |  3 hr 21 min\n",
      "0.00035  67*    | 0.940  0.012  0.931  0.998  | 0.007  0.000   |  3 hr 24 min\n",
      "0.00021  68*    | 0.941  0.011  0.930  0.998  | 0.007  0.000   |  3 hr 27 min\n",
      "0.00010  69*    | 0.941  0.011  0.932  0.998  | 0.007  0.000   |  3 hr 30 min\n",
      "0.00003  70*    | 0.941  0.011  0.930  0.998  | 0.007  0.000   |  3 hr 33 min\n",
      "0.00000  71*    | 0.941  0.011  0.930  0.998  | 0.006  0.000   |  3 hr 36 min\n",
      "0.00003  72*    | 0.941  0.011  0.930  0.998  | 0.007  0.000   |  3 hr 39 min\n",
      "0.00010  73*    | 0.941  0.011  0.928  0.998  | 0.007  0.000   |  3 hr 43 min\n",
      "0.00021  74*    | 0.942  0.011  0.933  0.998  | 0.006  0.000   |  3 hr 46 min\n",
      "0.00035  75*    | 0.943  0.011  0.933  0.998  | 0.007  0.000   |  3 hr 49 min\n",
      "0.00050  76*    | 0.939  0.012  0.932  0.998  | 0.007  0.000   |  3 hr 52 min\n",
      "0.00065  77*    | 0.943  0.011  0.934  0.998  | 0.007  0.000   |  3 hr 55 min\n",
      "0.00079  78*    | 0.941  0.011  0.932  0.998  | 0.007  0.000   |  3 hr 58 min\n",
      "0.00090  79*    | 0.941  0.011  0.940  0.998  | 0.007  0.000   |  4 hr 01 min\n",
      "0.00098  80*    | 0.936  0.013  0.923  0.998  | 0.008  0.000   |  4 hr 04 min\n",
      "0.00100  81*    | 0.937  0.012  0.923  0.998  | 0.008  0.000   |  4 hr 07 min\n",
      "0.00098  82*    | 0.939  0.012  0.926  0.998  | 0.008  0.000   |  4 hr 10 min\n",
      "0.00090  83*    | 0.937  0.012  0.915  0.999  | 0.007  0.000   |  4 hr 13 min\n",
      "0.00079  84*    | 0.939  0.012  0.923  0.999  | 0.007  0.000   |  4 hr 16 min\n",
      "0.00065  85*    | 0.937  0.012  0.915  0.999  | 0.007  0.000   |  4 hr 19 min\n",
      "0.00050  86*    | 0.941  0.011  0.928  0.999  | 0.007  0.000   |  4 hr 22 min\n",
      "0.00035  87*    | 0.941  0.012  0.931  0.998  | 0.007  0.000   |  4 hr 25 min\n",
      "0.00021  88*    | 0.940  0.012  0.927  0.998  | 0.006  0.000   |  4 hr 28 min\n",
      "0.00010  89*    | 0.941  0.012  0.929  0.998  | 0.006  0.000   |  4 hr 31 min\n",
      "0.00003  90*    | 0.941  0.012  0.929  0.998  | 0.006  0.000   |  4 hr 34 min\n",
      "0.00000  91*    | 0.940  0.012  0.927  0.998  | 0.006  0.000   |  4 hr 37 min\n",
      "0.00003  92*    | 0.941  0.012  0.929  0.998  | 0.006  0.000   |  4 hr 40 min\n",
      "0.00010  93*    | 0.941  0.012  0.929  0.998  | 0.006  0.000   |  4 hr 43 min\n",
      "0.00021  94*    | 0.942  0.012  0.930  0.998  | 0.006  0.000   |  4 hr 46 min\n",
      "0.00035  95*    | 0.941  0.012  0.927  0.999  | 0.007  0.000   |  4 hr 49 min\n",
      "0.00050  96*    | 0.941  0.012  0.927  0.999  | 0.006  0.000   |  4 hr 52 min\n",
      "0.00065  97*    | 0.942  0.011  0.929  0.998  | 0.006  0.000   |  4 hr 55 min\n",
      "0.00079  98*    | 0.937  0.012  0.908  0.999  | 0.007  0.000   |  4 hr 59 min\n",
      "0.00090  99*    | 0.939  0.010  0.925  0.998  | 0.008  0.000   |  5 hr 02 min\n",
      "0.00098  99     | 0.939  0.010  0.925  0.998  | 0.007  0.000   |  5 hr 04 min\n",
      "all dice score :  0.9421\n",
      "train => # imgs : 4790, # masks : 4790\n",
      "valid => # imgs : 714, # masks : 714\n",
      "fold = 3\n",
      "train_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fce38772640>\n",
      "valid_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fcd8863a700>\n",
      "\n",
      "** net setting **\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "optimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "0.00000  0*    | 0.000  0.000  0.000  0.000  | 0.000  0.000   |  0 hr 00 min\n",
      "0.00100  0     | 0.000  0.000  0.000  0.000  | 0.124  0.000   |  0 hr 02 min\n",
      " saved best models, dice:0.91944\n",
      "0.00100  1*    | 0.919  0.091  0.942  0.995  | 0.297  0.000   |  0 hr 03 min\n",
      "0.00098  1     | 0.919  0.091  0.942  0.995  | 0.038  0.000   |  0 hr 05 min\n",
      " saved best models, dice:0.93463\n",
      "0.00098  2*    | 0.935  0.036  0.926  0.998  | 0.064  0.000   |  0 hr 06 min\n",
      "0.00090  2     | 0.935  0.036  0.926  0.998  | 0.016  0.000   |  0 hr 08 min\n",
      " saved best models, dice:0.93857\n",
      "0.00090  3*    | 0.939  0.023  0.953  0.997  | 0.029  0.000   |  0 hr 09 min\n",
      "0.00079  3     | 0.939  0.023  0.953  0.997  | 0.029  0.000   |  0 hr 11 min\n",
      " saved best models, dice:0.93903\n",
      "0.00079  4*    | 0.939  0.018  0.928  0.998  | 0.020  0.000   |  0 hr 12 min\n",
      "0.00065  4     | 0.939  0.018  0.928  0.998  | 0.019  0.000   |  0 hr 14 min\n",
      " saved best models, dice:0.94324\n",
      "0.00065  5*    | 0.943  0.015  0.945  0.997  | 0.016  0.000   |  0 hr 15 min\n",
      "0.00050  5     | 0.943  0.015  0.945  0.997  | 0.007  0.000   |  0 hr 17 min\n",
      " saved best models, dice:0.94536\n",
      "0.00050  6*    | 0.945  0.014  0.953  0.997  | 0.013  0.000   |  0 hr 18 min\n",
      "0.00035  7*    | 0.943  0.014  0.942  0.998  | 0.013  0.000   |  0 hr 21 min\n",
      "0.00021  8*    | 0.945  0.013  0.944  0.998  | 0.011  0.000   |  0 hr 24 min\n",
      "0.00010  8     | 0.945  0.013  0.944  0.998  | 0.007  0.000   |  0 hr 27 min\n",
      " saved best models, dice:0.94577\n",
      "0.00010  9*    | 0.946  0.013  0.944  0.998  | 0.011  0.000   |  0 hr 27 min\n",
      "0.00003  10*    | 0.946  0.013  0.945  0.998  | 0.011  0.000   |  0 hr 30 min\n",
      "0.00000  11*    | 0.946  0.013  0.942  0.998  | 0.010  0.000   |  0 hr 33 min\n",
      "0.00003  12*    | 0.945  0.013  0.942  0.998  | 0.011  0.000   |  0 hr 36 min\n",
      "0.00010  13*    | 0.945  0.013  0.942  0.998  | 0.010  0.000   |  0 hr 39 min\n",
      "0.00021  14*    | 0.943  0.013  0.950  0.997  | 0.011  0.000   |  0 hr 42 min\n",
      "0.00035  15*    | 0.944  0.013  0.946  0.998  | 0.010  0.000   |  0 hr 45 min\n",
      "0.00050  16*    | 0.941  0.015  0.945  0.997  | 0.011  0.000   |  0 hr 48 min\n",
      "0.00065  17*    | 0.944  0.013  0.946  0.998  | 0.011  0.000   |  0 hr 51 min\n",
      "0.00079  18*    | 0.943  0.012  0.928  0.998  | 0.012  0.000   |  0 hr 54 min\n",
      "0.00090  19*    | 0.944  0.012  0.936  0.998  | 0.010  0.000   |  0 hr 57 min\n",
      "0.00098  20*    | 0.941  0.013  0.925  0.998  | 0.010  0.000   |  1 hr 01 min\n",
      "0.00100  21*    | 0.945  0.012  0.941  0.998  | 0.010  0.000   |  1 hr 04 min\n",
      "0.00098  22*    | 0.945  0.012  0.948  0.997  | 0.010  0.000   |  1 hr 07 min\n",
      "0.00090  23*    | 0.946  0.012  0.941  0.998  | 0.009  0.000   |  1 hr 10 min\n",
      "0.00079  24*    | 0.946  0.012  0.950  0.997  | 0.009  0.000   |  1 hr 13 min\n",
      "0.00065  25*    | 0.945  0.012  0.936  0.998  | 0.009  0.000   |  1 hr 16 min\n",
      "0.00050  25     | 0.945  0.012  0.936  0.998  | 0.011  0.000   |  1 hr 18 min\n",
      " saved best models, dice:0.94666\n",
      "0.00050  26*    | 0.947  0.012  0.949  0.998  | 0.008  0.000   |  1 hr 19 min\n",
      "0.00035  27*    | 0.946  0.012  0.939  0.998  | 0.008  0.000   |  1 hr 22 min\n",
      "0.00021  28*    | 0.945  0.013  0.939  0.998  | 0.008  0.000   |  1 hr 25 min\n",
      "0.00010  29*    | 0.946  0.013  0.941  0.998  | 0.007  0.000   |  1 hr 28 min\n",
      "0.00003  30*    | 0.947  0.013  0.942  0.998  | 0.007  0.000   |  1 hr 31 min\n",
      "0.00000  31*    | 0.947  0.013  0.941  0.998  | 0.007  0.000   |  1 hr 34 min\n",
      "0.00003  31     | 0.947  0.013  0.941  0.998  | 0.011  0.000   |  1 hr 37 min\n",
      " saved best models, dice:0.94689\n",
      "0.00003  32*    | 0.947  0.013  0.943  0.998  | 0.007  0.000   |  1 hr 37 min\n",
      "0.00010  33*    | 0.947  0.013  0.945  0.998  | 0.008  0.000   |  1 hr 40 min\n",
      "0.00021  34*    | 0.945  0.014  0.938  0.998  | 0.007  0.000   |  1 hr 43 min\n",
      "0.00035  35*    | 0.946  0.013  0.946  0.998  | 0.007  0.000   |  1 hr 46 min\n",
      "0.00050  36*    | 0.945  0.013  0.935  0.998  | 0.008  0.000   |  1 hr 49 min\n",
      "0.00065  37*    | 0.854  0.058  0.953  0.988  | 0.008  0.000   |  1 hr 52 min\n",
      "0.00079  38*    | 0.942  0.013  0.933  0.998  | 0.009  0.000   |  1 hr 55 min\n",
      "0.00090  39*    | 0.942  0.013  0.930  0.998  | 0.009  0.000   |  1 hr 58 min\n",
      "0.00098  40*    | 0.936  0.014  0.910  0.998  | 0.009  0.000   |  2 hr 01 min\n",
      "0.00100  41*    | 0.946  0.013  0.948  0.998  | 0.008  0.000   |  2 hr 04 min\n",
      "0.00098  42*    | 0.945  0.013  0.939  0.998  | 0.009  0.000   |  2 hr 08 min\n",
      "0.00090  43*    | 0.940  0.013  0.928  0.998  | 0.009  0.000   |  2 hr 11 min\n",
      "0.00079  44*    | 0.946  0.012  0.934  0.998  | 0.009  0.000   |  2 hr 14 min\n",
      "0.00065  45*    | 0.930  0.016  0.909  0.998  | 0.008  0.000   |  2 hr 17 min\n",
      "0.00050  46*    | 0.946  0.013  0.939  0.998  | 0.007  0.000   |  2 hr 20 min\n",
      "0.00035  47*    | 0.946  0.013  0.944  0.998  | 0.007  0.000   |  2 hr 23 min\n",
      "0.00021  48*    | 0.944  0.013  0.934  0.998  | 0.007  0.000   |  2 hr 27 min\n",
      "0.00010  49*    | 0.946  0.013  0.944  0.998  | 0.007  0.000   |  2 hr 30 min\n",
      "0.00003  50*    | 0.946  0.013  0.940  0.998  | 0.007  0.000   |  2 hr 33 min\n",
      "0.00000  51*    | 0.945  0.013  0.940  0.998  | 0.007  0.000   |  2 hr 36 min\n",
      "0.00003  52*    | 0.946  0.013  0.941  0.998  | 0.007  0.000   |  2 hr 39 min\n",
      "0.00010  53*    | 0.945  0.013  0.940  0.998  | 0.007  0.000   |  2 hr 42 min\n",
      "0.00021  54*    | 0.946  0.013  0.940  0.998  | 0.007  0.000   |  2 hr 46 min\n",
      "0.00035  55*    | 0.946  0.013  0.944  0.998  | 0.007  0.000   |  2 hr 49 min\n",
      "0.00050  56*    | 0.945  0.013  0.949  0.997  | 0.008  0.000   |  2 hr 52 min\n",
      "0.00065  56     | 0.945  0.013  0.949  0.997  | 0.007  0.000   |  2 hr 55 min\n",
      " saved best models, dice:0.94693\n",
      "0.00065  57*    | 0.947  0.013  0.946  0.998  | 0.007  0.000   |  2 hr 55 min\n",
      "0.00079  58*    | 0.946  0.013  0.946  0.998  | 0.007  0.000   |  2 hr 58 min\n",
      "0.00090  59*    | 0.945  0.013  0.943  0.998  | 0.007  0.000   |  3 hr 01 min\n",
      "0.00098  60*    | 0.942  0.014  0.944  0.997  | 0.008  0.000   |  3 hr 05 min\n",
      "0.00100  61*    | 0.943  0.015  0.930  0.998  | 0.008  0.000   |  3 hr 08 min\n",
      "0.00098  62*    | 0.945  0.013  0.944  0.998  | 0.008  0.000   |  3 hr 11 min\n",
      "0.00090  63*    | 0.945  0.014  0.942  0.998  | 0.008  0.000   |  3 hr 14 min\n",
      "0.00079  64*    | 0.943  0.014  0.941  0.998  | 0.008  0.000   |  3 hr 17 min\n",
      "0.00065  65*    | 0.945  0.013  0.950  0.997  | 0.007  0.000   |  3 hr 21 min\n",
      "0.00050  66*    | 0.946  0.013  0.945  0.998  | 0.007  0.000   |  3 hr 24 min\n",
      "0.00035  67*    | 0.946  0.014  0.946  0.998  | 0.007  0.000   |  3 hr 27 min\n",
      "0.00021  68*    | 0.946  0.013  0.945  0.998  | 0.007  0.000   |  3 hr 30 min\n",
      "0.00010  69*    | 0.946  0.014  0.948  0.998  | 0.006  0.000   |  3 hr 33 min\n",
      "0.00003  70*    | 0.946  0.014  0.947  0.998  | 0.006  0.000   |  3 hr 37 min\n",
      "0.00000  71*    | 0.946  0.014  0.945  0.998  | 0.006  0.000   |  3 hr 40 min\n",
      "0.00003  72*    | 0.946  0.014  0.946  0.998  | 0.006  0.000   |  3 hr 43 min\n",
      "0.00010  73*    | 0.946  0.014  0.947  0.998  | 0.006  0.000   |  3 hr 46 min\n",
      "0.00021  74*    | 0.946  0.014  0.948  0.998  | 0.006  0.000   |  3 hr 49 min\n",
      "0.00035  75*    | 0.944  0.015  0.944  0.998  | 0.007  0.000   |  3 hr 53 min\n",
      "0.00050  76*    | 0.944  0.015  0.948  0.997  | 0.007  0.000   |  3 hr 56 min\n",
      "0.00065  77*    | 0.945  0.014  0.938  0.998  | 0.007  0.000   |  3 hr 59 min\n",
      "0.00079  78*    | 0.945  0.014  0.943  0.998  | 0.008  0.000   |  4 hr 02 min\n",
      "0.00090  79*    | 0.943  0.013  0.935  0.998  | 0.007  0.000   |  4 hr 05 min\n",
      "0.00098  80*    | 0.944  0.014  0.936  0.998  | 0.008  0.000   |  4 hr 09 min\n",
      "0.00100  81*    | 0.926  0.018  0.910  0.998  | 0.007  0.000   |  4 hr 12 min\n",
      "0.00098  82*    | 0.945  0.013  0.941  0.998  | 0.008  0.000   |  4 hr 15 min\n",
      "0.00090  83*    | 0.942  0.013  0.938  0.998  | 0.007  0.000   |  4 hr 18 min\n",
      "0.00079  84*    | 0.946  0.012  0.943  0.998  | 0.007  0.000   |  4 hr 21 min\n",
      "0.00065  85*    | 0.946  0.014  0.951  0.997  | 0.007  0.000   |  4 hr 25 min\n",
      "0.00050  86*    | 0.945  0.014  0.940  0.998  | 0.007  0.000   |  4 hr 28 min\n",
      "0.00035  87*    | 0.947  0.014  0.943  0.998  | 0.007  0.000   |  4 hr 31 min\n",
      "0.00021  88*    | 0.946  0.014  0.946  0.998  | 0.006  0.000   |  4 hr 34 min\n",
      "0.00010  89*    | 0.946  0.014  0.945  0.998  | 0.006  0.000   |  4 hr 37 min\n",
      "0.00003  90*    | 0.946  0.014  0.945  0.998  | 0.006  0.000   |  4 hr 41 min\n",
      "0.00000  91*    | 0.946  0.014  0.943  0.998  | 0.006  0.000   |  4 hr 44 min\n",
      "0.00003  92*    | 0.946  0.014  0.943  0.998  | 0.006  0.000   |  4 hr 47 min\n",
      "0.00010  93*    | 0.947  0.014  0.945  0.998  | 0.006  0.000   |  4 hr 50 min\n",
      "0.00021  94*    | 0.947  0.014  0.941  0.998  | 0.006  0.000   |  4 hr 53 min\n",
      "0.00035  95*    | 0.945  0.015  0.941  0.998  | 0.006  0.000   |  4 hr 57 min\n",
      "0.00050  96*    | 0.944  0.014  0.939  0.998  | 0.006  0.000   |  5 hr 00 min\n",
      "0.00065  97*    | 0.945  0.015  0.950  0.997  | 0.007  0.000   |  5 hr 03 min\n",
      "0.00079  98*    | 0.943  0.013  0.944  0.997  | 0.007  0.000   |  5 hr 06 min\n",
      "0.00090  99*    | 0.946  0.013  0.952  0.997  | 0.007  0.000   |  5 hr 09 min\n",
      "0.00098  99     | 0.946  0.013  0.952  0.997  | 0.003  0.000   |  5 hr 12 min\n",
      "all dice score :  0.9437\n",
      "train => # imgs : 4262, # masks : 4262\n",
      "valid => # imgs : 1242, # masks : 1242\n",
      "fold = 4\n",
      "train_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fcd8a539d30>\n",
      "valid_dataset : \n",
      "<__main__.HuDataset_nol object at 0x7fce38772640>\n",
      "\n",
      "** net setting **\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "optimizer\n",
      "  Lookahead (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "** start training here! **\n",
      "   is_mixed_precision = True \n",
      "   batch_size = 16 \n",
      "             |-------------- VALID---------|---- TRAIN/BATCH ----------------\n",
      "rate  epoch  | dice   loss   tp     tn     | loss           | time           \n",
      "-------------------------------------------------------------------------------------\n",
      "0.00000  0*    | 0.000  0.000  0.000  0.000  | 0.000  0.000   |  0 hr 00 min\n",
      "0.00100  0     | 0.000  0.000  0.000  0.000  | 0.114  0.000   |  0 hr 02 min\n",
      " saved best models, dice:0.87955\n",
      "0.00100  1*    | 0.880  0.102  0.888  0.996  | 0.341  0.000   |  0 hr 03 min\n",
      "0.00098  1     | 0.880  0.102  0.888  0.996  | 0.044  0.000   |  0 hr 05 min\n",
      " saved best models, dice:0.91237\n",
      "0.00098  2*    | 0.912  0.041  0.915  0.997  | 0.070  0.000   |  0 hr 06 min\n",
      "0.00090  2     | 0.912  0.041  0.915  0.997  | 0.025  0.000   |  0 hr 08 min\n",
      " saved best models, dice:0.91385\n",
      "0.00090  3*    | 0.914  0.027  0.957  0.996  | 0.034  0.000   |  0 hr 09 min\n",
      "0.00079  3     | 0.914  0.027  0.957  0.996  | 0.020  0.000   |  0 hr 12 min\n",
      " saved best models, dice:0.91811\n",
      "0.00079  4*    | 0.918  0.020  0.923  0.998  | 0.022  0.000   |  0 hr 12 min\n",
      "0.00065  4     | 0.918  0.020  0.923  0.998  | 0.021  0.000   |  0 hr 15 min\n",
      " saved best models, dice:0.91921\n",
      "0.00065  5*    | 0.919  0.017  0.928  0.997  | 0.017  0.000   |  0 hr 16 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00050  5     | 0.919  0.017  0.928  0.997  | 0.010  0.000   |  0 hr 18 min\n",
      " saved best models, dice:0.92071\n",
      "0.00050  6*    | 0.921  0.016  0.934  0.997  | 0.015  0.000   |  0 hr 19 min\n",
      "0.00035  7*    | 0.920  0.015  0.938  0.997  | 0.013  0.000   |  0 hr 22 min\n",
      "0.00021  7     | 0.920  0.015  0.938  0.997  | 0.008  0.000   |  0 hr 24 min\n",
      " saved best models, dice:0.92185\n",
      "0.00021  8*    | 0.922  0.015  0.934  0.997  | 0.013  0.000   |  0 hr 25 min\n",
      "0.00010  8     | 0.922  0.015  0.934  0.997  | 0.014  0.000   |  0 hr 28 min\n",
      " saved best models, dice:0.92222\n",
      "0.00010  9*    | 0.922  0.015  0.934  0.997  | 0.012  0.000   |  0 hr 28 min\n",
      "0.00003  10*    | 0.922  0.015  0.932  0.997  | 0.012  0.000   |  0 hr 32 min\n",
      "0.00000  11*    | 0.922  0.015  0.933  0.997  | 0.012  0.000   |  0 hr 35 min\n",
      "0.00003  12*    | 0.921  0.015  0.933  0.997  | 0.012  0.000   |  0 hr 38 min\n",
      "0.00010  12     | 0.921  0.015  0.933  0.997  | 0.008  0.000   |  0 hr 40 min\n",
      " saved best models, dice:0.92292\n",
      "0.00010  13*    | 0.923  0.015  0.938  0.997  | 0.012  0.000   |  0 hr 41 min\n",
      "0.00021  14*    | 0.921  0.014  0.933  0.997  | 0.011  0.000   |  0 hr 44 min\n",
      "0.00035  15*    | 0.918  0.013  0.914  0.998  | 0.011  0.000   |  0 hr 47 min\n",
      "0.00050  16*    | 0.920  0.014  0.952  0.997  | 0.011  0.000   |  0 hr 51 min\n",
      "0.00065  17*    | 0.923  0.013  0.939  0.997  | 0.012  0.000   |  0 hr 54 min\n",
      "0.00079  17     | 0.923  0.013  0.939  0.997  | 0.007  0.000   |  0 hr 56 min\n",
      " saved best models, dice:0.92493\n",
      "0.00079  18*    | 0.925  0.012  0.936  0.998  | 0.011  0.000   |  0 hr 57 min\n",
      "0.00090  19*    | 0.909  0.016  0.912  0.997  | 0.011  0.000   |  1 hr 00 min\n",
      "0.00098  19     | 0.909  0.016  0.912  0.997  | 0.004  0.000   |  1 hr 02 min\n",
      " saved best models, dice:0.92598\n",
      "0.00098  20*    | 0.926  0.012  0.929  0.998  | 0.011  0.000   |  1 hr 03 min\n",
      "0.00100  21*    | 0.921  0.013  0.918  0.998  | 0.011  0.000   |  1 hr 06 min\n",
      "0.00098  22*    | 0.921  0.013  0.933  0.997  | 0.011  0.000   |  1 hr 09 min\n",
      "0.00090  23*    | 0.922  0.014  0.934  0.997  | 0.010  0.000   |  1 hr 12 min\n",
      "0.00079  24*    | 0.919  0.013  0.919  0.998  | 0.010  0.000   |  1 hr 15 min\n",
      "0.00065  25*    | 0.923  0.013  0.929  0.998  | 0.009  0.000   |  1 hr 18 min\n",
      "0.00050  26*    | 0.926  0.013  0.944  0.997  | 0.009  0.000   |  1 hr 21 min\n",
      "0.00035  27*    | 0.924  0.013  0.943  0.997  | 0.009  0.000   |  1 hr 25 min\n",
      "0.00021  28*    | 0.924  0.014  0.941  0.997  | 0.008  0.000   |  1 hr 28 min\n",
      "0.00010  29*    | 0.922  0.015  0.943  0.997  | 0.008  0.000   |  1 hr 31 min\n",
      "0.00003  30*    | 0.922  0.015  0.943  0.997  | 0.008  0.000   |  1 hr 34 min\n",
      "0.00000  31*    | 0.923  0.014  0.941  0.997  | 0.008  0.000   |  1 hr 37 min\n",
      "0.00003  32*    | 0.922  0.015  0.944  0.997  | 0.008  0.000   |  1 hr 40 min\n",
      "0.00010  33*    | 0.923  0.015  0.945  0.997  | 0.008  0.000   |  1 hr 43 min\n",
      "0.00021  34*    | 0.925  0.015  0.950  0.997  | 0.008  0.000   |  1 hr 46 min\n",
      "0.00035  35*    | 0.922  0.014  0.934  0.997  | 0.008  0.000   |  1 hr 49 min\n",
      "0.00050  36*    | 0.920  0.016  0.928  0.997  | 0.008  0.000   |  1 hr 52 min\n",
      "0.00065  36     | 0.920  0.016  0.928  0.997  | 0.036  0.000   |  1 hr 55 min\n",
      " saved best models, dice:0.92965\n",
      "0.00065  37*    | 0.930  0.011  0.928  0.998  | 0.009  0.000   |  1 hr 56 min\n",
      "0.00079  38*    | 0.926  0.013  0.938  0.998  | 0.010  0.000   |  1 hr 59 min\n",
      "0.00090  39*    | 0.924  0.015  0.940  0.997  | 0.009  0.000   |  2 hr 02 min\n",
      "0.00098  40*    | 0.925  0.013  0.940  0.997  | 0.009  0.000   |  2 hr 05 min\n",
      "0.00100  41*    | 0.924  0.013  0.933  0.998  | 0.009  0.000   |  2 hr 08 min\n",
      "0.00098  42*    | 0.924  0.014  0.942  0.997  | 0.009  0.000   |  2 hr 11 min\n",
      "0.00090  43*    | 0.925  0.014  0.943  0.997  | 0.009  0.000   |  2 hr 14 min\n",
      "0.00079  44*    | 0.925  0.015  0.953  0.997  | 0.009  0.000   |  2 hr 17 min\n",
      "0.00065  45*    | 0.920  0.016  0.935  0.997  | 0.009  0.000   |  2 hr 20 min\n",
      "0.00050  46*    | 0.928  0.014  0.947  0.997  | 0.008  0.000   |  2 hr 23 min\n",
      "0.00035  47*    | 0.923  0.014  0.941  0.997  | 0.008  0.000   |  2 hr 27 min\n",
      "0.00021  48*    | 0.925  0.015  0.945  0.997  | 0.007  0.000   |  2 hr 30 min\n",
      "0.00010  49*    | 0.925  0.015  0.947  0.997  | 0.007  0.000   |  2 hr 33 min\n",
      "0.00003  50*    | 0.925  0.015  0.949  0.997  | 0.007  0.000   |  2 hr 36 min\n",
      "0.00000  51*    | 0.925  0.015  0.948  0.997  | 0.007  0.000   |  2 hr 39 min\n",
      "0.00003  52*    | 0.925  0.015  0.946  0.997  | 0.007  0.000   |  2 hr 42 min\n",
      "0.00010  53*    | 0.925  0.015  0.948  0.997  | 0.007  0.000   |  2 hr 46 min\n",
      "0.00021  54*    | 0.927  0.015  0.948  0.997  | 0.007  0.000   |  2 hr 49 min\n",
      "0.00035  55*    | 0.926  0.014  0.947  0.997  | 0.007  0.000   |  2 hr 52 min\n",
      "0.00050  56*    | 0.922  0.014  0.934  0.997  | 0.007  0.000   |  2 hr 55 min\n",
      "0.00065  57*    | 0.922  0.016  0.934  0.997  | 0.008  0.000   |  2 hr 58 min\n",
      "0.00079  58*    | 0.924  0.015  0.946  0.997  | 0.008  0.000   |  3 hr 01 min\n",
      "0.00090  59*    | 0.926  0.016  0.955  0.997  | 0.009  0.000   |  3 hr 04 min\n",
      "0.00098  59     | 0.926  0.016  0.955  0.997  | 0.007  0.000   |  3 hr 05 min\r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set seed\n",
    "    print('no set seed') if args.seed ==-1 else set_seeds(seed=args.seed)\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    run_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-tribute",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pregnant-boston",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    server ='kaggle' # ['kaggle', 'local'] local은 cv측정용도\n",
    "    amp = False\n",
    "    gpu = 3\n",
    "    \n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    batch_size=256\n",
    "\n",
    "    threshold = 0.4\n",
    "    min_size = 0 # 총 덩어리가 픽셀 1000개가 안되는 것은 삭제한다. 2021-04-21 post_processing\n",
    "    \n",
    "    model_path = './data/result/new_5fold_alltraining2_fold0_b4_256/checkpoint/0fold_28epoch_0.9458_model.pth'# 모델한개\n",
    "    \n",
    "    en_model_path = ['./data/result/(0.924)100epoch_nooverlap_640_25_50_fold0_b4_512/checkpoint/' + x for x in \\\n",
    "                     \n",
    "                     ['1fold_51epoch_0.9405_model.pth','2fold_61epoch_0.9437_model.pth','3fold_56epoch_0.9469_model.pth',\n",
    "                     '4fold_36epoch_0.9297_model.pth','5fold_34epoch_0.9342_model.pth' ]]# if ensemble 직접 입력해야한다. \n",
    "    sub = '100epoch_5fold'# submission name\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    \n",
    "    tile_size = 640  # 이것도 480으로 해보기 \n",
    "    tile_average_step = 320\n",
    "    tile_scale = 0.25\n",
    "    tile_min_score = 0.25  \n",
    "\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "located-montana",
   "metadata": {
    "code_folding": [
     4,
     26,
     76,
     268,
     375
    ]
   },
   "outputs": [],
   "source": [
    "thres = args.threshold\n",
    "\n",
    "prob = []\n",
    "\n",
    "def mask_to_csv(image_id, submit_dir):\n",
    "\n",
    "    predicted = []\n",
    "    for id in image_id:\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        predict_file = submit_dir + '/%s.predict.png' % id\n",
    "        # predict = cv2.imread(predict_file, cv2.IMREAD_GRAYSCALE)\n",
    "        predict = np.array(Image.open(predict_file))\n",
    "        predict = cv2.resize(predict, dsize=(width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        predict = (predict > 128).astype(np.uint8) * 255\n",
    "\n",
    "        p = rle_encode(predict)\n",
    "        predicted.append(p)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = image_id\n",
    "    df['predicted'] = predicted\n",
    "    return df\n",
    "\n",
    "def run_submit(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.model_path.split('checkpoint')[0]\n",
    "    initial_checkpoint = out_dir + '/checkpoint' + args.model_path.split('checkpoint')[1]\n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-mean-thres(%s)'%(server, initial_checkpoint[-18:-4],thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    # ----- model -------\n",
    "    net = SegModel() \n",
    "    net.to(device)\n",
    "    state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "    net.load_state_dict(state_dict,strict=True)  #True\n",
    "    net = net.eval()\n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        if server == 'local':\n",
    "            image_file = data_dir + '/train/%s.tiff' % id\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "            structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "            mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "            mask  = read_mask(mask_file)\n",
    "\n",
    "        if server == 'kaggle':\n",
    "            image_file = data_dir + '/test/%s.tiff' % id\n",
    "            json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "            structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "            mask = None\n",
    "\n",
    "\n",
    "        #--- predict here!  ---\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        tile_image = tile['tile_image']\n",
    "        tile_image = np.stack(tile_image)[..., ::-1]\n",
    "        tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "        tile_image = tile_image.astype(np.float32)/255\n",
    "        print(tile_image.shape)\n",
    "        tile_probability = []\n",
    "        \n",
    "        batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "        for t,m in enumerate(batch):\n",
    "            print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "            m = torch.from_numpy(m).cuda()\n",
    "\n",
    "            p = []\n",
    "            with torch.no_grad():\n",
    "                logit = net(m)\n",
    "                p.append(torch.sigmoid(logit))\n",
    "\n",
    "                #---\n",
    "                if server == 'kaggle':\n",
    "                    if 1: #tta here\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.stack(p).mean(0)\n",
    "                if server == 'local':\n",
    "                    if 0: #tta here\n",
    "                        #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.cat(p)\n",
    "                    #p = torch.stack(p)\n",
    "\n",
    "            tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "        print('\\r' , end='',flush=True)\n",
    "        log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "        tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "        height, width = tile['image_small'].shape[:2]\n",
    "        probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                              tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                              aggregate='mean')\n",
    "        \n",
    "\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = (probability>thres).astype(np.float32)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir + args.sub+'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0\n",
    "    \n",
    "def remove_small(probability, threshold, min_size):\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]  # 먼저 들어온 예측 값에 대하여 mask 생성\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))  # 이 함수는 0인 부분은 배경, 1인 부분은 하나의 덩어리로 생각해서\n",
    "    # 총 분리된 덩어리 갯수(N)와 각 덩어리에 숫자를 1,2, ..., N을 매겨서 반환한다. \n",
    "    predictions = np.zeros_like(mask)  # 최종 예측 마스크를 선언\n",
    "    for c in tqdm(range(1, num_component)):\n",
    "        p = (component == c)  # 각 덩어리를 체크한뒤에 이 덩어리의 픽셀 갯수가 min_size보다 크면 1로 체크한다. \n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "    return predictions\n",
    "    \n",
    "def run_submit_ensemble(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.en_model_path[0].split('checkpoint')[0]\n",
    "    \n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-thres(%s)_remove_small'%(server, args.sub,thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        fold_prob = []\n",
    "        for m_p in args.en_model_path:\n",
    "            initial_checkpoint = m_p\n",
    "            # ----- model -------\n",
    "            net = SegModel() \n",
    "            net.to(device)\n",
    "            state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "            net.load_state_dict(state_dict,strict=True)  #True\n",
    "            net = net.eval()\n",
    "            if server == 'local':\n",
    "                image_file = data_dir + '/train/%s.tiff' % id\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "                structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "                mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "                mask  = read_mask(mask_file)\n",
    "\n",
    "            if server == 'kaggle':\n",
    "                image_file = data_dir + '/test/%s.tiff' % id\n",
    "                json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "                structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "                mask = None\n",
    "\n",
    "\n",
    "            #--- predict here!  ---\n",
    "            tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "            tile_image = tile['tile_image']\n",
    "            tile_image = np.stack(tile_image)[..., ::-1]\n",
    "            tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "            tile_image = tile_image.astype(np.float32)/255\n",
    "            print(tile_image.shape)\n",
    "            tile_probability = []\n",
    "\n",
    "            batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "            for t,m in enumerate(batch):\n",
    "                print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "                m = torch.from_numpy(m).to(device)\n",
    "\n",
    "                p = []\n",
    "                with torch.no_grad():\n",
    "                    logit = net(m)\n",
    "                    p.append(torch.sigmoid(logit))\n",
    "\n",
    "                    #---\n",
    "                    if server == 'kaggle':\n",
    "                        if 1: #tta here\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.stack(p).mean(0)\n",
    "                    if server == 'local':\n",
    "                        if 0: #tta here\n",
    "                            #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.cat(p)\n",
    "                        #p = torch.stack(p)\n",
    "\n",
    "                tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "            print('\\r' , end='',flush=True)\n",
    "            log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "            tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "            height, width = tile['image_small'].shape[:2]\n",
    "            probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                                  tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                                  aggregate='mean')\n",
    "\n",
    "\n",
    "            fold_prob.append(probability)\n",
    "        \n",
    "        probability = sum(fold_prob)/len(args.en_model_path)\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = remove_small(probability, args.threshold, args.min_size)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir +'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powerful-citizen",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_size = 640 \n",
      "tile_average_step = 320 \n",
      "tile_scale = 0.250000 \n",
      "tile_min_score = 0.250000 \n",
      "\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(364, 3, 640, 640)\n",
      "2ec3f1bb9  90 / 91    0 min 38 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(364, 3, 640, 640)\n",
      "2ec3f1bb9  90 / 91    1 min 19 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(364, 3, 640, 640)\n",
      "2ec3f1bb9  90 / 91    1 min 54 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(364, 3, 640, 640)\n",
      "2ec3f1bb9  90 / 91    2 min 31 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(364, 3, 640, 640)\n",
      "2ec3f1bb9  90 / 91    3 min 12 secc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:41<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(139, 3, 640, 640)\n",
      "3589adb90  33 / 34    4 min 26 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(139, 3, 640, 640)\n",
      "3589adb90  33 / 34    4 min 42 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(139, 3, 640, 640)\n",
      "3589adb90  33 / 34    4 min 59 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(139, 3, 640, 640)\n",
      "3589adb90  33 / 34    5 min 16 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(139, 3, 640, 640)\n",
      "3589adb90  33 / 34    5 min 32 secc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:12<00:00, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(240, 3, 640, 640)\n",
      "57512b7f1  59 / 60    6 min 31 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(240, 3, 640, 640)\n",
      "57512b7f1  59 / 60    7 min 08 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(240, 3, 640, 640)\n",
      "57512b7f1  59 / 60    7 min 46 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(240, 3, 640, 640)\n",
      "57512b7f1  59 / 60    8 min 23 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(240, 3, 640, 640)\n",
      "57512b7f1  59 / 60    9 min 00 secc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:19<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(390, 3, 640, 640)\n",
      "aa05346ff  96 / 97   10 min 31 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(390, 3, 640, 640)\n",
      "aa05346ff  96 / 97   11 min 26 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(390, 3, 640, 640)\n",
      "aa05346ff  96 / 97   12 min 20 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(390, 3, 640, 640)\n",
      "aa05346ff  96 / 97   13 min 15 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(390, 3, 640, 640)\n",
      "aa05346ff  96 / 97   14 min 03 secc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:41<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(230, 3, 640, 640)\n",
      "d488c759a  56 / 57   15 min 41 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(230, 3, 640, 640)\n",
      "d488c759a  56 / 57   16 min 17 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(230, 3, 640, 640)\n",
      "d488c759a  56 / 57   16 min 53 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(230, 3, 640, 640)\n",
      "d488c759a  56 / 57   17 min 29 secc\n",
      "encoder :  efficientnet-b4\n",
      "unet loaded\n",
      "(230, 3, 640, 640)\n",
      "d488c759a  56 / 57   18 min 05 secc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:15<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                          predicted\n",
      "0  2ec3f1bb9  60762281 49 60786270 51 60810259 53 60834248 5...\n",
      "1  3589adb90  68541227 61 68570659 63 68600092 64 68629524 6...\n",
      "2  57512b7f1  328952562 22 328985801 24 329019041 24 3290522...\n",
      "3  aa05346ff  52856686 46 52887405 48 52918125 48 52948844 5...\n",
      "4  d488c759a  494044870 14 494091529 16 494138189 16 4941848...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 0: #normal\n",
    "    if __name__ == '__main__':\n",
    "        run_submit(args)\n",
    "elif 1:# ensemble\n",
    "    if __name__ == '__main__':\n",
    "        run_submit_ensemble(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-court",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-general",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-worthy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-swedish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-coordinate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-skill",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-tension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-picnic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubmap",
   "language": "python",
   "name": "hubmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
