{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indonesian-astrology",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ------------Library--------------#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.autograd import Variable\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import tifffile as tiff\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from sklearn.model_selection import KFold\n",
    "# loss\n",
    "#from lovasz import lovasz_hinge\n",
    "#from losses_pytorch.lovasz_loss import LovaszSoftmax\n",
    "PI  = np.pi\n",
    "INF = np.inf\n",
    "EPS = 1e-12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "upset-paint",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    amp = True\n",
    "    gpu = \"4\"\n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    diff_arch = True\n",
    "    encoders = [\"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\"]\n",
    "    decoders = [\"unet\", \"unet\", \"unet\", \"unet\", \"unet\"]\n",
    "    \n",
    "    batch_size=8\n",
    "    weight_decay=1e-6\n",
    "    epochs=50\n",
    "    n_fold=5\n",
    "    fold=0 # [0, 1, 2, 3, 4]\n",
    "    all_fold_train = True # all fold training\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    image_size=1024 # crop size\n",
    "    crop_size=image_size\n",
    "    \n",
    "    tile_size = 1280\n",
    "    tile_step = 640\n",
    "    tile_scale = 1\n",
    "    dataset = f'{tile_scale}_{tile_size}_{tile_step}_train_fold'#'0.25_320_160_train_fold'\n",
    "    val_dataset = f'{tile_scale}_{tile_size}_{tile_size}_val_fold'\n",
    "    if diff_arch:\n",
    "        dir = f'{epochs}_{encoders}_{decoders}_{image_size}_{tile_size}_{tile_step}_{tile_scale}'\n",
    "    else:\n",
    "        dir = f'{epochs}_{encoder}_{decoder}_{image_size}_{tile_size}_{tile_step}_{tile_scale}' \n",
    "    # ---- optimizer, scheduler .. ---- #\n",
    "    T_max=10 # CosineAnnealingLR\n",
    "    opt =  'radam_look' # [adamw, radam_look]\n",
    "    scheduler='CosineAnnealingLR' #'MultiStepLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    loss = 'bce' # [lovasz, bce, bce_dice, dice]\n",
    "    factor=0.4 # ReduceLROnPlateau, MultiStepLR\n",
    "    patience=3 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    \n",
    "    decay_epoch = [4, 8, 12]\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "    #encoder_lr=4e-4\n",
    "    #decoder_lr=4e-4\n",
    "    start_lr = 1e-3\n",
    "    min_lr=1e-6\n",
    "    #----------------------------------#\n",
    "    \n",
    "    \n",
    "    # ----- 여러 시도 ------#\n",
    "    clf_head=False # encoder에 classfication head 붙일지 여부\n",
    "    label_smoothing = False # label smoothing 여부\n",
    "    multi_gpu=True if len(gpu)>1 else False # multi gpu 사용\n",
    "    clf_alpha = 0.3 # classification head 의 loss 비율\n",
    "    smoothing = 0.1 # label smoothing factor\n",
    "    dice_smoothing = 1 # dice loss 사용시 하이퍼 파라미터\n",
    "    \n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=42\n",
    "    \n",
    "data_dir = '/home/jeonghokim/competition/HubMap/data/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-product",
   "metadata": {},
   "source": [
    "# useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continental-community",
   "metadata": {
    "code_folding": [
     0,
     2,
     13,
     24,
     42,
     56,
     73,
     87,
     108,
     125,
     133,
     146,
     192,
     228,
     230,
     234,
     249
    ]
   },
   "outputs": [],
   "source": [
    "#-------evaluation metric, loss---------#\n",
    "###################################\n",
    "def np_binary_cross_entropy_loss(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    #---\n",
    "    logp = -np.log(np.clip(p,1e-6,1))\n",
    "    logn = -np.log(np.clip(1-p,1e-6,1))\n",
    "    loss = t*logp +(1-t)*logn\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def np_dice_score(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "def dice_score(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "    https://catalyst-team.github.io/catalyst/_modules/catalyst/dl/utils/criterion/dice.html\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "        targets = (targets > threshold).float()\n",
    "\n",
    "    intersection = torch.sum(targets * outputs)\n",
    "    union = torch.sum(targets) + torch.sum(outputs)\n",
    "    dice = 2 * intersection / (union + eps)\n",
    "\n",
    "    return dice\n",
    "def torch_accuracy(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,):\n",
    "\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "        \n",
    "    tp = torch.sum(targets*outputs)/torch.sum(targets)\n",
    "    tn = torch.sum((1-outputs)*(1-targets))/torch.sum(1-targets)\n",
    "\n",
    "    return tp, tn\n",
    "\n",
    "def np_accuracy(probability, mask):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    tp = (p*t).sum()/((t).sum()+1e-7)\n",
    "    tn = ((1-p)*(1-t)).sum()/(1-t).sum()\n",
    "    return tp, tn\n",
    "\n",
    "def criterion_binary_cross_entropy(logit, mask):\n",
    "    logit = logit.reshape(-1)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logit, mask)\n",
    "    return loss\n",
    "\n",
    "# threshold dice score\n",
    "def np_dice_score2(probability, mask, threshold):\n",
    "    p = probability.reshape(-1)\n",
    "    t = mask.reshape(-1)\n",
    "\n",
    "    p = p>threshold\n",
    "    t = t>0.5\n",
    "    uion = p.sum() + t.sum()\n",
    "    overlap = (p*t).sum()\n",
    "    dice = 2*overlap/(uion+0.001)\n",
    "    return dice\n",
    "\n",
    "# --------------------\n",
    "# Loss\n",
    "# --------------------\n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=args.smoothing):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "        \n",
    "        Dice_BCE = BCE*0.6 + dice_loss*0.4\n",
    "        \n",
    "        return Dice_BCE.mean()\n",
    "class DiceLoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=args.dice_smoothing):\n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        inputs = F.sigmoid(inputs)   \n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "                \n",
    "        return dice_loss.mean()\n",
    "    \n",
    "#PyTorch lovasz\n",
    "def symmetric_lovasz(outputs, targets):\n",
    "    return 0.5*(lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1.0 - targets))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#from torch.autograd import Function\n",
    "# copy from: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/LovaszSoftmax/lovasz_loss.py\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "class LovaszSoftmax(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(LovaszSoftmax, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def prob_flatten(self, input, target):\n",
    "        assert input.dim() in [4, 5]\n",
    "        num_class = input.size(1)\n",
    "        if input.dim() == 4:\n",
    "            input = input.permute(0, 2, 3, 1).contiguous()\n",
    "            input_flatten = input.view(-1, num_class)\n",
    "        elif input.dim() == 5:\n",
    "            input = input.permute(0, 2, 3, 4, 1).contiguous()\n",
    "            input_flatten = input.view(-1, num_class)\n",
    "        target_flatten = target.view(-1)\n",
    "        return input_flatten, target_flatten\n",
    "\n",
    "    def lovasz_softmax_flat(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "        losses = []\n",
    "        for c in range(num_classes):\n",
    "            target_c = (targets == c).float()\n",
    "            if num_classes == 1:\n",
    "                input_c = inputs[:, 0]\n",
    "            else:\n",
    "                input_c = inputs[:, c]\n",
    "            loss_c = (torch.autograd.Variable(target_c) - input_c).abs()\n",
    "            loss_c_sorted, loss_index = torch.sort(loss_c, 0, descending=True)\n",
    "            target_c_sorted = target_c[loss_index]\n",
    "            losses.append(torch.dot(loss_c_sorted, torch.autograd.Variable(lovasz_grad(target_c_sorted))))\n",
    "        losses = torch.stack(losses)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            loss = losses\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = losses.sum()\n",
    "        else:\n",
    "            loss = losses.mean()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # print(inputs.shape, targets.shape) # (batch size, class_num, x,y,z), (batch size, 1, x,y,z)\n",
    "        inputs, targets = self.prob_flatten(inputs, targets)\n",
    "        # print(inputs.shape, targets.shape)\n",
    "        losses = self.lovasz_softmax_flat(inputs, targets)\n",
    "        return losses\n",
    "class Lovasz_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lovasz_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        return LovaszSoftmax()(inputs, targets)\n",
    "###################################\n",
    "#-------ELSE function---------#\n",
    "###################################\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "def print_args(args, logger=None):\n",
    "    for k, v in vars(args).items():\n",
    "        if logger is not None:\n",
    "            logger.write('{:<16} : {}\\n'.format(k, v))\n",
    "        else:\n",
    "            print('{:<16} : {}'.format(k, v))\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "###########################\n",
    "#---- label smoothing -----\n",
    "###########################\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = args.smoothing):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = x.float().flatten()\n",
    "        target = target.float() * (1-self.smoothing) + 0.5 * self.smoothing\n",
    "        target = target.flatten()\n",
    "\n",
    "\n",
    "        loss  = F.binary_cross_entropy_with_logits(x, target, reduction='mean')\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contained-information",
   "metadata": {
    "code_folding": [
     0,
     1,
     20,
     27,
     52,
     67,
     79,
     95,
     153,
     196,
     208
    ]
   },
   "outputs": [],
   "source": [
    "#-------masking & tile & decode---------#\n",
    "def read_tiff(image_file):\n",
    "    \"\"\"\n",
    "    *data size*\n",
    "    e.g.) (3, w, h) or (1,1,3,w,h) or (w, h, 3)  --> transform --> (w, h, 3)\n",
    "    \"\"\"\n",
    "    image = tiff.imread(image_file)\n",
    "    if image.shape[0] == 1:\n",
    "        image = image[0][0]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    elif image.shape[0] == 3:\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "    return image\n",
    "\n",
    "def read_mask(mask_file):\n",
    "    mask = np.array(Image.open(mask_file))\n",
    "    return mask\n",
    "\n",
    "def read_json_as_df(json_file):\n",
    "    with open(json_file) as f:\n",
    "        j = json.load(f)\n",
    "    df = pd.json_normalize(j)\n",
    "    return df\n",
    "\n",
    "\n",
    "def draw_strcuture(df, height, width, fill=255, structure=[]):\n",
    "    mask = np.zeros((height, width), np.uint8)\n",
    "    for row in df.values:\n",
    "        type  = row[2]  #geometry.type\n",
    "        coord = row[3]  # geometry.coordinates\n",
    "        name  = row[4]   # properties.classification.name\n",
    "\n",
    "        if structure !=[]:\n",
    "            if not any(s in name for s in structure): continue\n",
    "\n",
    "\n",
    "        if type=='Polygon':\n",
    "            pt = np.array(coord).astype(np.int32)\n",
    "            #cv2.polylines(mask, [coord.reshape((-1, 1, 2))], True, 255, 1)\n",
    "            cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "        if type=='MultiPolygon':\n",
    "            for pt in coord:\n",
    "                pt = np.array(pt).astype(np.int32)\n",
    "                cv2.fillPoly(mask, [pt.reshape((-1, 1, 2))], fill)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# resize, cvtcolor, generate mask\n",
    "# 원하는 object 영역만 따오는 mask\n",
    "def draw_strcuture_from_hue(image, fill=255, scale=1/32): # 0.25/32 default\n",
    "    height, width, _ = image.shape\n",
    "    vv = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = cv2.cvtColor(vv, cv2.COLOR_RGB2HSV)\n",
    "    # image_show('v[0]', v[:,:,0])\n",
    "    # image_show('v[1]', v[:,:,1])\n",
    "    # image_show('v[2]', v[:,:,2])\n",
    "    # cv2.waitKey(0)\n",
    "    mask = (vv[:, :, 1] > 32).astype(np.uint8) # rgb2hsv를 하고나서 1채널에 대해 시행하면 원하는 object만 잘따온다.\n",
    "    mask = mask*fill\n",
    "    mask = cv2.resize(mask, dsize=(width, height), interpolation=cv2.INTER_LINEAR) # 다시 원래사이즈로 복구\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --- rle ---------------------------------\n",
    "def rle_decode(rle, height, width , fill=255):\n",
    "    s = rle.split()\n",
    "    start, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    start -= 1\n",
    "    mask = np.zeros(height*width, dtype=np.uint8)\n",
    "    for i, l in zip(start, length):\n",
    "        mask[i:i+l] = fill\n",
    "    mask = mask.reshape(width,height).T\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def rle_encode(mask):\n",
    "    m = mask.T.flatten()\n",
    "    m = np.concatenate([[0], m, [0]])\n",
    "    run = np.where(m[1:] != m[:-1])[0] + 1\n",
    "    run[1::2] -= run[::2]\n",
    "    rle =  ' '.join(str(r) for r in run)\n",
    "    return rle\n",
    "\n",
    "\n",
    "# --- tile ---------------------------------\n",
    "\"\"\"\n",
    "-결국, tile_image, tile_mask만 가져다가 쓴다.\n",
    "1. scale로 resize를 하고 image size와 step만큼 건너뛰며 이미지를 만든다.\n",
    "2. 이때 일정 영역이 빈마스크면 데이터에서 제외한다.\n",
    "3. 쌓은 image와 mask를 return\n",
    "\"\"\"\n",
    "def to_tile(image, mask, structure, scale, size, step, min_score): \n",
    "    half = size//2\n",
    "    image_small = cv2.resize(image, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR) # defualt는 1/4만큼 w,h를 줄인다.\n",
    "    height, width, _ = image_small.shape\n",
    "\n",
    "    #make score\n",
    "    structure_small = cv2.resize(structure, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    vv = structure_small.astype(np.float32)/255\n",
    "\n",
    "    #make coord\n",
    "    xx = np.linspace(half, width  - half, int(np.ceil((width  - size) / step)))\n",
    "    yy = np.linspace(half, height - half, int(np.ceil((height - size) / step)))\n",
    "    xx = [int(x) for x in xx]\n",
    "    yy = [int(y) for y in yy]\n",
    "\n",
    "    coord  = []\n",
    "    reject = []\n",
    "    for cy in yy:\n",
    "        for cx in xx:\n",
    "            cv = vv[cy - half:cy + half, cx - half:cx + half].mean() # h, w // tiling한 마스크(structure)가 평균 0.25를 안넘으면 버린다.\n",
    "            if cv>min_score: # min_score ,default:0.25, 0.25의 의미?, 타일링 이미지의 1/4는 object여야 한다는 의미?\n",
    "                coord.append([cx,cy,cv])\n",
    "            else:\n",
    "                reject.append([cx,cy,cv])\n",
    "    #-----\n",
    "    if 1: # resize한 image를 tiling 하여 리스트만든다\n",
    "        tile_image = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = image_small[cy - half:cy + half, cx - half:cx + half] # resize한 image에서 indexing만 하는과정\n",
    "            assert (t.shape == (size, size, 3))\n",
    "            tile_image.append(t)\n",
    "\n",
    "    if mask is not None: # mask를 resize하고 tiling하여 리스트 만든다\n",
    "        mask_small = cv2.resize(mask, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        tile_mask = []\n",
    "        for cx,cy,cv in coord:\n",
    "            t = mask_small[cy - half:cy + half, cx - half:cx + half]\n",
    "            assert (t.shape == (size, size))\n",
    "            tile_mask.append(t)\n",
    "    else:\n",
    "        mask_small = None\n",
    "        tile_mask  = None\n",
    "\n",
    "    return {\n",
    "        'image_small': image_small,\n",
    "        'mask_small' : mask_small,\n",
    "        'structure_small' : structure_small,\n",
    "        'tile_image' : tile_image,\n",
    "        'tile_mask'  : tile_mask,\n",
    "        'coord'  : coord,\n",
    "        'reject' : reject,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "submission할때 쓰임\n",
    "\"\"\"\n",
    "def to_mask(tile, coord, height, width, scale, size, step, min_score, aggregate='mean'):\n",
    "\n",
    "    half = size//2\n",
    "    mask  = np.zeros((height, width), np.float32)\n",
    "\n",
    "    if 'mean' in aggregate:\n",
    "        w = np.ones((size,size), np.float32)\n",
    "\n",
    "        #if 'sq' in aggregate:\n",
    "        if 1:\n",
    "            #https://stackoverflow.com/questions/17190649/how-to-obtain-a-gaussian-filter-in-python\n",
    "            y,x = np.mgrid[-half:half,-half:half]\n",
    "            y = half-abs(y)\n",
    "            x = half-abs(x)\n",
    "            w = np.minimum(x,y)\n",
    "            w = w/w.max()#*2.5\n",
    "            w = np.minimum(w,1)\n",
    "\n",
    "        #--------------\n",
    "        count = np.zeros((height, width), np.float32)\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask [cy - half:cy + half, cx - half:cx + half] += tile[t]*w\n",
    "            count[cy - half:cy + half, cx - half:cx + half] += w\n",
    "               # see unet paper for \"Overlap-tile strategy for seamless segmentation of arbitrary large images\"\n",
    "        m = (count != 0)\n",
    "        mask[m] /= count[m]\n",
    "\n",
    "    if aggregate=='max':\n",
    "        for t, (cx, cy, cv) in enumerate(coord):\n",
    "            mask[cy - half:cy + half, cx - half:cx + half] = np.maximum(\n",
    "                mask[cy - half:cy + half, cx - half:cx + half], tile[t] )\n",
    "\n",
    "    return mask\n",
    "\n",
    "# --------------이 아래로 안씀 ------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# --draw ------------------------------------------\n",
    "\"\"\"\n",
    "경계선을 그리게 만든다, 컨투어\n",
    "하지만 안씀\n",
    "\"\"\"\n",
    "def mask_to_inner_contour(mask):\n",
    "    mask = mask>0.5\n",
    "    pad = np.lib.pad(mask, ((1, 1), (1, 1)), 'reflect')\n",
    "    contour = mask & (\n",
    "            (pad[1:-1,1:-1] != pad[:-2,1:-1]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[2:,1:-1])  \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,:-2]) \\\n",
    "          | (pad[1:-1,1:-1] != pad[1:-1,2:])\n",
    "    )\n",
    "    return contour\n",
    "\n",
    "\n",
    "def draw_contour_overlay(image, mask, color=(0,0,255), thickness=1):\n",
    "    contour =  mask_to_inner_contour(mask)\n",
    "    if thickness==1:\n",
    "        image[contour] = color\n",
    "    else:\n",
    "        r = max(1,thickness//2)\n",
    "        for y,x in np.stack(np.where(contour)).T:\n",
    "            cv2.circle(image, (x,y), r, color, lineType=cv2.LINE_4 )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-external",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-thing",
   "metadata": {},
   "source": [
    "# Dataset & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hired-updating",
   "metadata": {
    "code_folding": [
     0,
     6,
     44,
     92,
     130,
     131,
     242
    ]
   },
   "outputs": [],
   "source": [
    "#--------------- Dataset ----------------#\n",
    "##########################################\n",
    "\n",
    "#--------------- \n",
    "# Old version\n",
    "#--------------- \n",
    "def make_image_id_v1(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset_v1(Dataset):\n",
    "    def __init__(self, image_id, image_dir, augment=None):\n",
    "        self.augment = augment\n",
    "        self.image_id = image_id\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        tile_id = []\n",
    "        for i in range(len(image_dir)):\n",
    "            for id in image_id[i]: \n",
    "                df = pd.read_csv(data_dir + '/tile/%s/%s.csv'% (self.image_dir[i],id) )\n",
    "                tile_id += ('%s/%s/'%(self.image_dir[i],id) + df.tile_id).tolist()\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        string += '\\timage_dir = %s\\n'%self.image_dir\n",
    "        string += '\\timage_id  = %s\\n'%str(self.image_id)\n",
    "        string += '\\t          = %d\\n'%sum(len(i) for i in self.image_id)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(data_dir + '/tile/%s.png'%(id), cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(data_dir + '/tile/%s.mask.png'%(id), cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "\n",
    "#--------------- \n",
    "# Old version (simple fold)\n",
    "#--------------- \n",
    "def make_image_id_(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset_(Dataset):\n",
    "    def __init__(self, tile_id, augment=None):\n",
    "        self.augment = augment\n",
    "\n",
    "        self.tile_id = tile_id\n",
    "        self.len =len(self.tile_id)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.tile_id[index]\n",
    "        image = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.png', cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(f'{data_dir}/tile/{args.dataset}/{id}.mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment()\n",
    "        \n",
    "        return r\n",
    "\n",
    "#--------------- \n",
    "# New version(image fold)\n",
    "#--------------- \n",
    "def make_image_id(mode):\n",
    "    train_image_id = {\n",
    "        0 : '0486052bb', 1 : '095bf7a1f',\n",
    "        2 : '1e2425f28', 3 : '26dc41664',\n",
    "        4 : '2f6ecfcdf', 5 : '4ef6695ce',\n",
    "        6 : '54f2eec69', 7 : '8242609fa',\n",
    "        8 : 'aaa6a05cc', 9 : 'afa5e8098', \n",
    "        10 :'b2dc8411c', 11: 'b9a3865fc',\n",
    "        12 :'c68fe75ea', 13: 'cb2d976f4',\n",
    "        14 :'e79de561c'\n",
    "    }\n",
    "\n",
    "    test_image_id = {\n",
    "        0 : '2ec3f1bb9', 1 : '3589adb90',\n",
    "        2 : '57512b7f1', 3 : 'aa05346ff',\n",
    "        4 : 'd488c759a',\n",
    "    }\n",
    "    if 'pseudo-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ]\n",
    "        return test_id\n",
    "\n",
    "    if 'test-all'==mode:\n",
    "        test_id = [ test_image_id[i] for i in [0,1,2,3,4] ] # list(test_image_id.values()) #\n",
    "        return test_id\n",
    "\n",
    "    if 'train-all'==mode:\n",
    "        train_id = [ train_image_id[i] for i in [x for x in train_image_id] ] # list(test_image_id.values()) #\n",
    "        return train_id\n",
    "\n",
    "    if 'valid' in mode or 'train' in mode:\n",
    "        fold = {int(x) for x in mode.split('-')[1].split(',')}\n",
    "        #valid = [fold,]\n",
    "        train = list({x for x in train_image_id}-fold)\n",
    "        valid_id = [ train_image_id[i] for i in fold ]\n",
    "        train_id = [ train_image_id[i] for i in train ]\n",
    "\n",
    "        if 'valid' in mode: return valid_id\n",
    "        if 'train' in mode: return train_id\n",
    "class HuDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "        self.augment = augment\n",
    "\n",
    "        #self.tile_id = tile_id\n",
    "        #self.len =len(self.tile_id)\n",
    "        self.df = df\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen  = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.df['tile_id'].loc[index]\n",
    "        image = cv2.imread(f'{id}.png', cv2.IMREAD_COLOR)\n",
    "        mask  = cv2.imread(f'{id}.mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "        #print(data_dir + '/tile/%s/%s.png'%(self.image_dir,id))\n",
    "\n",
    "        image = image.astype(np.float32) / 255\n",
    "        mask  = mask.astype(np.float32) / 255\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'tile_id' : id,\n",
    "            'mask' : mask,\n",
    "            'image' : image,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        #if self.augment is not None: r = self.augment(image=r['image'], mask=r['mask'])\n",
    "        return r\n",
    "\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    index = []\n",
    "    mask = []\n",
    "    image = []\n",
    "    for r in batch:\n",
    "        index.append(r['index'])\n",
    "        mask.append(r['mask'])\n",
    "        image.append(r['image'])\n",
    "\n",
    "    image = np.stack(image)\n",
    "    image = image[...,::-1]\n",
    "    image = image.transpose(0,3,1,2)\n",
    "    image = np.ascontiguousarray(image)\n",
    "\n",
    "    mask  = np.stack(mask)\n",
    "    mask  = np.ascontiguousarray(mask)\n",
    "\n",
    "    #---\n",
    "    image = torch.from_numpy(image).contiguous().float()\n",
    "    mask  = torch.from_numpy(mask).contiguous().unsqueeze(1)\n",
    "    mask  = (mask>0.5).float()\n",
    "\n",
    "    return {\n",
    "        'index' : index,\n",
    "        'mask' : mask,\n",
    "        'image' : image,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "narrow-schema",
   "metadata": {
    "code_folding": [
     0,
     3,
     19,
     27,
     42,
     82,
     91,
     97,
     103,
     122
    ]
   },
   "outputs": [],
   "source": [
    "#---------- augmentation ---------------------#\n",
    "###############################################\n",
    "#flip\n",
    "def do_random_flip_transpose(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = image.transpose(1,0,2)\n",
    "        mask = mask.transpose(1,0)\n",
    "\n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return image, mask\n",
    "\n",
    "#geometric\n",
    "def do_random_crop(image, mask, size):\n",
    "    height, width = image.shape[:2]\n",
    "    x = np.random.choice(width -size)\n",
    "    y = np.random.choice(height-size)\n",
    "    image = image[y:y+size,x:x+size]\n",
    "    mask  = mask[y:y+size,x:x+size]\n",
    "    return image, mask\n",
    "\n",
    "def do_random_scale_crop(image, mask, size, mag):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    s = 1 + np.random.uniform(-1, 1)*mag\n",
    "    s =  int(s*size)\n",
    "\n",
    "    x = np.random.choice(width -s)\n",
    "    y = np.random.choice(height-s)\n",
    "    image = image[y:y+s,x:x+s]\n",
    "    mask  = mask[y:y+s,x:x+s]\n",
    "    if s!=size:\n",
    "        image = cv2.resize(image, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "        mask  = cv2.resize(mask, dsize=(size,size), interpolation=cv2.INTER_LINEAR)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rotate_crop(image, mask, size, mag=30 ):\n",
    "    angle = 1+np.random.uniform(-1, 1)*mag\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    dst = np.array([\n",
    "        [0,0],[size,size], [size,0], [0,size],\n",
    "    ])\n",
    "\n",
    "    c = np.cos(angle/180*2*PI)\n",
    "    s = np.sin(angle/180*2*PI)\n",
    "    src = (dst-size//2)@np.array([[c, -s],[s, c]]).T\n",
    "    src[:,0] -= src[:,0].min()\n",
    "    src[:,1] -= src[:,1].min()\n",
    "\n",
    "    src[:,0] = src[:,0] + np.random.uniform(0,width -src[:,0].max())\n",
    "    src[:,1] = src[:,1] + np.random.uniform(0,height-src[:,1].max())\n",
    "\n",
    "    if 0: #debug\n",
    "        def to_int(f):\n",
    "            return (int(f[0]),int(f[1]))\n",
    "\n",
    "        cv2.line(image, to_int(src[0]), to_int(src[1]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[1]), to_int(src[2]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[2]), to_int(src[3]), (0,0,1), 16)\n",
    "        cv2.line(image, to_int(src[3]), to_int(src[0]), (0,0,1), 16)\n",
    "        image_show_norm('image', image, min=0, max=1)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "    transform = cv2.getAffineTransform(src[:3].astype(np.float32), dst[:3].astype(np.float32))\n",
    "    image = cv2.warpAffine( image, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "    mask  = cv2.warpAffine( mask, transform, (size, size), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask\n",
    "\n",
    "#warp/elastic deform ...\n",
    "#<todo>\n",
    "\n",
    "#noise\n",
    "def do_random_noise(image, mask, mag=0.1):\n",
    "    height, width = image.shape[:2]\n",
    "    noise = np.random.uniform(-1,1, (height, width,1))*mag\n",
    "    image = image + noise\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "#intensity\n",
    "def do_random_contast(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image * alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_gain(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image ** alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h = hsv[:, :, 0].astype(np.float32)  # hue\n",
    "    s = hsv[:, :, 1].astype(np.float32)  # saturation\n",
    "    v = hsv[:, :, 2].astype(np.float32)  # value\n",
    "    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n",
    "    s =  s*(1 + random.uniform(-1,1)*mag[1])\n",
    "    v =  v*(1 + random.uniform(-1,1)*mag[2])\n",
    "\n",
    "    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n",
    "    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n",
    "    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    image = image.astype(np.float32)/255\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def filter_small(mask, min_size):\n",
    "\n",
    "    m = (mask*255).astype(np.uint8)\n",
    "\n",
    "    num_comp, comp, stat, centroid = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
    "    if num_comp==1: return mask\n",
    "\n",
    "    filtered = np.zeros(comp.shape,dtype=np.uint8)\n",
    "    area = stat[:, -1]\n",
    "    for i in range(1, num_comp):\n",
    "        if area[i] >= min_size:\n",
    "            filtered[comp == i] = 255\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-commander",
   "metadata": {
    "code_folding": [
     0,
     2,
     41,
     117
    ]
   },
   "outputs": [],
   "source": [
    "#---------- optimizer, scheduler ---------------------#\n",
    "############################################\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, alpha=0.5, k=6):\n",
    "\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "\n",
    "        self.slow_weights = [\n",
    "                [p.clone().detach() for p in group['params']]\n",
    "            for group in self.param_groups]\n",
    "\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        self.state = optimizer.state\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        loss = self.optimizer.step()\n",
    "\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(p.data - q.data, alpha=self.alpha )\n",
    "                p.data.copy_(q.data)\n",
    "        return loss\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "#---------- scheduler ---------------------#\n",
    "def get_scheduler(optimizer):\n",
    "    if args.scheduler =='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = args.epochs//args.T_0, T_mult=1, eta_min=0, last_epoch=-1)\n",
    "    elif args.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.min_lr, last_epoch=-1)\n",
    "    elif args.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=args.factor, patience=args.patience, verbose=True, \n",
    "                                      min_lr = args.min_lr, eps=args.eps)\n",
    "    else:\n",
    "        scheduler=None\n",
    "        assert False, 'not implement'\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-relief",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "established-glasgow",
   "metadata": {
    "code_folding": [
     0,
     1,
     11
    ]
   },
   "outputs": [],
   "source": [
    "class DOWNBLOCK(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DOWNBLOCK, self).__init__()\n",
    "        self.down_conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.down_bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.down_bn1(self.down_conv1(x)))\n",
    "        return x\n",
    "    \n",
    "def SegModel():\n",
    "    models = []\n",
    "    # 다른 모덷들일때\n",
    "    if args.diff_arch:\n",
    "        for i in range(args.n_fold):\n",
    "            en_name = args.encoders[i]\n",
    "            de_name = args.decoders[i]\n",
    "            # decoder별로 로드\n",
    "            if de_name.lower() == \"unet\":\n",
    "                if args.clf_head:\n",
    "                    print('classification head')\n",
    "                    aux_params=dict(\n",
    "                        pooling='avg',             # one of 'avg', 'max'\n",
    "                        dropout=0.5,               # dropout ratio, default is None\n",
    "                        activation='sigmoid',      # activation function, default is None\n",
    "                        classes=1,                 # define number of output labels\n",
    "                    )\n",
    "                    model = smp.Unet(\n",
    "                        encoder_name=en_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                        in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                        classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                        aux_params=aux_params\n",
    "                        )\n",
    "                else:\n",
    "                    model = smp.Unet(\n",
    "                        encoder_name=en_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                        in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                        classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                        )\n",
    "            elif de_name.lower() == \"fpn\":\n",
    "                model = smp.FPN(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            elif de_name.lower() == \"upp\":\n",
    "                model = smp.UnetPlusPlus(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            elif de_name.lower() == \"linknet\":\n",
    "                model = smp.Linknet(\n",
    "                    encoder_name=en_name,\n",
    "                    encoder_weights=\"imagenet\",\n",
    "                    in_channels=3,\n",
    "                    classes=1\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            models.append(model)\n",
    "                \n",
    "        \n",
    "    # 같은 모델 일 때 5개 복사\n",
    "    else:\n",
    "        if args.encoder in ['b0','b1','b2','b3','b4','b5','b6','b7']:\n",
    "            encoder_name_ = f'efficientnet-{args.encoder}' #'timm-efficientnet-b4'\n",
    "            print('encoder : ', encoder_name_)\n",
    "        else:\n",
    "            encoder_name_ = args.encoder\n",
    "        if args.decoder =='fpn':\n",
    "            print('fpn loaded')\n",
    "            model = smp.FPN(\n",
    "                encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                )\n",
    "        elif args.decoder =='unet':\n",
    "            print('unet loaded')\n",
    "            if args.clf_head:\n",
    "                print('classification head')\n",
    "                aux_params=dict(\n",
    "                    pooling='avg',             # one of 'avg', 'max'\n",
    "                    dropout=0.5,               # dropout ratio, default is None\n",
    "                    activation='sigmoid',      # activation function, default is None\n",
    "                    classes=1,                 # define number of output labels\n",
    "                )\n",
    "                model = smp.Unet(\n",
    "                    encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                    aux_params=aux_params\n",
    "                    )\n",
    "            else:\n",
    "                model = smp.Unet(\n",
    "                    encoder_name=encoder_name_,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights='imagenet',     # use `imagenet` pretrained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "                    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "                #list_ = [DOWNBLOCK(), model, nn.Upsample(size=640, mode='bilinear', align_corners=True)]\n",
    "                #model = nn.Sequential(*list_)\n",
    "        if args.encoder=='R50':\n",
    "            if args.decoder=='ViT':\n",
    "                vit_name='R50-ViT-B_16'\n",
    "                config_vit = CONFIGS[vit_name]\n",
    "                config_vit.n_classes = 1\n",
    "                config_vit.n_skip = 3\n",
    "                if vit_name.find('R50') != -1:\n",
    "                    config_vit.patches.grid = (int(args.image_size / 16), int(args.image_size / 16))\n",
    "                model = VisionTransformer(config_vit, img_size=args.image_size, num_classes=config_vit.n_classes) \n",
    "        for i in range(args.n_fold):\n",
    "            models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-influence",
   "metadata": {},
   "source": [
    "# validation 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "antique-ready",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    server ='local' # ['kaggle', 'local'] local은 cv측정용도\n",
    "    amp = False\n",
    "    gpu = 4\n",
    "    \n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    n_fold = 5\n",
    "    diff_arch = True\n",
    "    encoders = [\"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\", \"efficientnet-b4\"]\n",
    "    decoders = [\"unet\", \"unet\", \"unet\", \"unet\", \"unet\"]\n",
    "    batch_size=16\n",
    "    #fold=0\n",
    "    mode = 'eval' # ['eval', 'gen_image']\n",
    "    loss = 'bce'\n",
    "    clf_head=False\n",
    "    dataset = '0.5_640_320_train_fold'#'[0.25_256_128_train', '0.25_480_240_train' ]# dataset size\n",
    "    val_dataset = '0.5_640_640_val_fold'\n",
    "    \n",
    "    model_path = [\"./data/result/50_['xception', 'efficientnet-b4', 'xception', 'efficientnet-b4', 'xception']_['unet', 'fpn', 'upp', 'unet', 'linknet']_512_640_320_0.5_b4_512\" +\n",
    "                  \"/checkpoint/\" + x for x in \\\n",
    "                 ['0fold_13epoch_0.9375_xception_unetmodel.pth','1fold_34epoch_0.9367_efficientnet-b4_fpnmodel.pth',\n",
    "                 '2fold_8epoch_0.9372_xception_uppmodel.pth','3fold_36epoch_0.9529_efficientnet-b4_unetmodel.pth',\n",
    "                 '4fold_15epoch_0.9241_xception_linknetmodel.pth']]\n",
    "    \n",
    "    sub = '[visualize][04.05]_0.9337_models'# submission name\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    \n",
    "    tile_size = 640\n",
    "    tile_average_step = 320\n",
    "    tile_scale = 0.25\n",
    "    tile_min_score = 0.25  \n",
    "\n",
    "#assert args.server!='local', 'not implement'\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "round-stranger",
   "metadata": {
    "code_folding": [
     2,
     41,
     189,
     199,
     204,
     214,
     218
    ]
   },
   "outputs": [],
   "source": [
    "all_dice_dict={}\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    valid_num = 0\n",
    "    total = 0 ; dice=0 ; loss=0 ; tp = 0 ; tn = 0\n",
    "    dice2=0 ; loss2=0\n",
    "    valid_probability, valid_probability2, valid_probability3 = [],[],[]\n",
    "    valid_mask, valid_mask2, valid_mask3 = [],[],[]\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    #start_timer = timer()\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            mask  = batch['mask']\n",
    "            image = batch['image'].to(device)\n",
    "            \n",
    "            if args.clf_head:\n",
    "                logit, _ = net(image) # seg, clf\n",
    "            else:\n",
    "                logit = net(image)#data_parallel(net, image) #net(input)#\n",
    "            probability = torch.sigmoid(logit)\n",
    "                \n",
    "            valid_probability.append(probability.data.cpu().numpy())\n",
    "            valid_mask.append(mask.data.cpu().numpy())\n",
    "\n",
    "    #assert(valid_num == len(valid_loader.dataset)) # drop last True이면 assert되는거임\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    mask = np.concatenate(valid_mask)\n",
    "    if args.loss =='bce':\n",
    "        loss = np_binary_cross_entropy_loss(probability, mask)\n",
    "    elif args.loss =='lovasz':\n",
    "        loss = 0\n",
    "    \n",
    "    dice = [np_dice_score2(probability, mask, round(th, 2)) for th in np.arange(0.1, 0.7, 0.05)]\n",
    "    #tp, tn = np_accuracy(probability, mask)\n",
    "\n",
    "    return np.array(dice)#[dice_dict, loss,  tp, tn]\n",
    "\n",
    "\n",
    "def gen_val_image(args):\n",
    "    out_dir = args.model_path[0].split('checkpoint')[0]\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint','train','valid','backup'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.val.txt',mode='a')\n",
    "\n",
    "    # my log argument\n",
    "    print_args(args, log)\n",
    "\n",
    "    submit_dir = out_dir + '/valid/%s-mean'%(args.server)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    #\n",
    "    for fold in range(5):\n",
    "        scaler = GradScaler()\n",
    "        net = SegModel() \n",
    "        net = net.to(device)\n",
    "        state_dict = torch.load(args.model_path[fold], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        net.load_state_dict(state_dict,strict=True)  #True\n",
    "        net = net.eval()\n",
    "\n",
    "        #log.write('schduler\\n  %s\\n'%(schduler))\n",
    "        log.write('\\n')\n",
    "\n",
    "        #----      \n",
    "\n",
    "        # make validation predict images\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "        #\n",
    "        a = pd.read_csv('../hubmap/tile/0.25_320_160_train_fold/image_id_split.csv')\n",
    "        b = a[a['fold']==fold]\n",
    "        valid_image_id = b['tile_id'].apply(lambda x : x.split('/')[-2]).unique()\n",
    "\n",
    "        #\n",
    "        start_timer = timer()\n",
    "        for id in valid_image_id:\n",
    "            image_file = data_dir + '/train/%s.tiff' % id\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "            structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "            mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "            mask  = read_mask(mask_file)\n",
    "\n",
    "            #--- predict here!  ---\n",
    "            tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "            tile_image = tile['tile_image']\n",
    "            tile_image = np.stack(tile_image)[..., ::-1]\n",
    "            tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "            tile_image = tile_image.astype(np.float32)/255\n",
    "            print(tile_image.shape)\n",
    "            tile_probability = []\n",
    "\n",
    "            batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "            for t,m in enumerate(batch):\n",
    "                print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "                m = torch.from_numpy(m).to(device)\n",
    "\n",
    "                p = []\n",
    "                with torch.no_grad():\n",
    "                    logit = net(m)\n",
    "                    p.append(torch.sigmoid(logit))\n",
    "                    if args.server == 'local':\n",
    "                        if 0: #tta here\n",
    "                            #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.cat(p)\n",
    "\n",
    "                tile_probability.append(p.data.cpu().numpy())\n",
    "            print('\\r' , end='',flush=True)\n",
    "            log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "            tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "            height, width = tile['image_small'].shape[:2]\n",
    "            probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                                  tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                                  aggregate='mean')\n",
    "            #\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            overlay = np.dstack([\n",
    "                np.zeros_like(truth),\n",
    "                probability, #green\n",
    "                truth, #red\n",
    "            ])\n",
    "\n",
    "            image_small = tile['image_small'].astype(np.float32)/255\n",
    "            #predict = (probability>thres).astype(np.float32)\n",
    "            overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "            overlay2 = image_small.copy()\n",
    "            overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "            overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "            overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "            if 1:\n",
    "                #cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "                #cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "                cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "def eval_image(args):\n",
    "\n",
    "    #-----------dataset split --------------------#\n",
    "    tile_id = []\n",
    "    image_dir_ = f'{args.dataset}'#'0.25_320_160_train'\n",
    "    image_dir=[image_dir_, ] # pseudo할때 뒤에 추가\n",
    "    \n",
    "    image_dir_val_ = f'{args.val_dataset}'#'0.25_320_320_val'\n",
    "    image_dir_val=[image_dir_val_, ]\n",
    "    \n",
    "    for i in range(len(image_dir)):\n",
    "        df = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir[i]) )\n",
    "\n",
    "    for i in range(len(image_dir_val)):\n",
    "        df2 = pd.read_csv(data_dir + '/tile/%s/image_id_split.csv'% (image_dir_val[i]) )\n",
    "    df2['img_id'] = df2['tile_id'].apply(lambda x: x.split('/')[-2])\n",
    "        \n",
    "    all_dice = []\n",
    "    for n_fold in range(5):\n",
    "\n",
    "        train_df = df[df['fold']!= n_fold].reset_index(drop=True)\n",
    "        val_df = df2[df2['fold']== n_fold].reset_index(drop=True).copy()\n",
    "        \n",
    "        # validation loader 3개 만들기 위함\n",
    "        unique_value = val_df['tile_id'].apply(lambda x: x.split('/')[-2]).unique() #[valid_id1, valid_id2, valid_id3 ]\n",
    "        val_img_id1 = unique_value[0] ; val_img_id2 = unique_value[1] ; val_img_id3= unique_value[2]\n",
    "        val_df1= val_df[val_df['img_id']==val_img_id1].reset_index(drop=True)\n",
    "        val_df2= val_df[val_df['img_id']==val_img_id2].reset_index(drop=True)\n",
    "        val_df3= val_df[val_df['img_id']==val_img_id3].reset_index(drop=True)\n",
    "        #####################################################\n",
    "        # val loader1\n",
    "        valid_dataset1 = HuDataset(\n",
    "            df = val_df1\n",
    "            ,\n",
    "        )\n",
    "        valid_loader1 = DataLoader(\n",
    "            valid_dataset1,\n",
    "            sampler = SequentialSampler(valid_dataset1),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader2\n",
    "        valid_dataset2 = HuDataset(\n",
    "            df = val_df2\n",
    "            ,\n",
    "        )\n",
    "        \n",
    "        valid_loader2 = DataLoader(\n",
    "            valid_dataset2,\n",
    "            sampler = SequentialSampler(valid_dataset2),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # val loader3\n",
    "        valid_dataset3 = HuDataset(\n",
    "            df = val_df3\n",
    "            ,\n",
    "        )\n",
    "        valid_loader3 = DataLoader(\n",
    "            valid_dataset3,\n",
    "            sampler = SequentialSampler(valid_dataset3),\n",
    "            batch_size  = args.batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "        # ------------------------\n",
    "        #  Model\n",
    "        # ------------------------\n",
    "\n",
    "        scaler = GradScaler()\n",
    "        models = SegModel() \n",
    "        net = models[n_fold].to(device)\n",
    "        state_dict = torch.load(args.model_path[n_fold], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        # 병렬처리를 했으면 앞에 module이 붙으므로 키를 바꿔줘야 한다. \n",
    "        for key in list(state_dict.keys()):\n",
    "            if \"module.\" in key:\n",
    "                state_dict[key.replace(\"module.\", \"\")] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        net.load_state_dict(state_dict,strict=True)  #True\n",
    "        net = net.eval()\n",
    "        \n",
    "        print(\"model load success!!!\")\n",
    "        # scheudler\n",
    "        valid_loss1 = do_valid(net, valid_loader1) #\n",
    "        valid_loss2 = do_valid(net, valid_loader2)\n",
    "        valid_loss3 = do_valid(net, valid_loader3)\n",
    "        valid_loss = (valid_loss1 + valid_loss2 + valid_loss3)/3\n",
    "        \n",
    "        all_dice.append(valid_loss)\n",
    "        \n",
    "    dice = sum(all_dice)/len(all_dice)\n",
    "    for n, th in enumerate(np.arange(0.1, 0.7, 0.05)):\n",
    "        th = round(th, 2)\n",
    "        print(f'th:{th}, dice score : {dice[n] : .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "documented-enforcement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "model load success!!!\n",
      "th:0.1, dice score :  0.9089\n",
      "th:0.15, dice score :  0.9189\n",
      "th:0.2, dice score :  0.9257\n",
      "th:0.25, dice score :  0.9304\n",
      "th:0.3, dice score :  0.9338\n",
      "th:0.35, dice score :  0.9361\n",
      "th:0.4, dice score :  0.9374\n",
      "th:0.45, dice score :  0.9379\n",
      "th:0.5, dice score :  0.9377\n",
      "th:0.55, dice score :  0.9367\n",
      "th:0.6, dice score :  0.9347\n",
      "th:0.65, dice score :  0.9316\n"
     ]
    }
   ],
   "source": [
    "\"\"\"red is real\"\"\"\n",
    "if 1: #normal\n",
    "    if __name__ == '__main__':\n",
    "        if args.mode == 'eval':\n",
    "            eval_image(args)\n",
    "        elif args.mode =='gen_image':\n",
    "            gen_val_image(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-hydrogen",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "varied-feeling",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    server ='kaggle' # ['kaggle', 'local'] local은 cv측정용도\n",
    "    amp = False\n",
    "    gpu = 0\n",
    "    \n",
    "    encoder='b4'#'resnet34'\n",
    "    decoder='unet'\n",
    "    \n",
    "    diff_arch = True\n",
    "    encoders = [\"xception\", \"efficientnet-b4\", \"xception\", \"efficientnet-b4\", \"xception\"]\n",
    "    decoders = [\"unet\", \"fpn\", \"upp\", \"unet\", \"linknet\"]\n",
    "    n_fold = 5\n",
    "    batch_size=64\n",
    "    clf_head=False\n",
    "    \n",
    "    threshold = 0.45\n",
    "    \n",
    "    model_path = '../hubmap/result/'\n",
    "\n",
    "    en_model_path = [\"./data/result/50_['xception', 'efficientnet-b4', 'xception', 'efficientnet-b4', 'xception']_['unet', 'fpn', 'upp', 'unet', 'linknet']_512_640_320_0.5_b4_512\" +\n",
    "                  \"/checkpoint/\" + x for x in \\\n",
    "                 ['0fold_13epoch_0.9375_xception_unetmodel.pth','1fold_34epoch_0.9367_efficientnet-b4_fpnmodel.pth',\n",
    "                 '2fold_8epoch_0.9372_xception_uppmodel.pth','3fold_36epoch_0.9529_efficientnet-b4_unetmodel.pth',\n",
    "                 '4fold_15epoch_0.9241_xception_linknetmodel.pth']]\n",
    "    sub = '30epoch_imagefold_0.9338_320_160'# submission name\n",
    "    \n",
    "    # ---- Dataset ---- #\n",
    "    \n",
    "    tile_size = 640\n",
    "    tile_average_step = 320\n",
    "    tile_scale = 0.5\n",
    "    tile_min_score = 0.25  \n",
    "\n",
    "assert args.server!='local', 'not implement'\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "horizontal-booking",
   "metadata": {
    "code_folding": [
     4,
     26,
     257,
     353,
     363
    ]
   },
   "outputs": [],
   "source": [
    "thres = args.threshold\n",
    "\n",
    "prob = []\n",
    "\n",
    "def mask_to_csv(image_id, submit_dir):\n",
    "\n",
    "    predicted = []\n",
    "    for id in image_id:\n",
    "        image_file = data_dir + '/test/%s.tiff' % id\n",
    "        image = read_tiff(image_file)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        predict_file = submit_dir + '/%s.predict.png' % id\n",
    "        # predict = cv2.imread(predict_file, cv2.IMREAD_GRAYSCALE)\n",
    "        predict = np.array(Image.open(predict_file))\n",
    "        predict = cv2.resize(predict, dsize=(width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        predict = (predict > 128).astype(np.uint8) * 255\n",
    "\n",
    "        p = rle_encode(predict)\n",
    "        predicted.append(p)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = image_id\n",
    "    df['predicted'] = predicted\n",
    "    return df\n",
    "\n",
    "def run_submit(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.model_path.split('checkpoint')[0]\n",
    "    initial_checkpoint = out_dir + '/checkpoint' + args.model_path.split('checkpoint')[1]\n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-mean-thres(%s)'%(server, initial_checkpoint[-18:-4],thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    # ----- model -------\n",
    "    net = SegModel() \n",
    "    net.to(device)\n",
    "    state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "    net.load_state_dict(state_dict,strict=True)  #True\n",
    "    net = net.eval()\n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        if server == 'local':\n",
    "            image_file = data_dir + '/train/%s.tiff' % id\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "            structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "            mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "            mask  = read_mask(mask_file)\n",
    "\n",
    "        if server == 'kaggle':\n",
    "            image_file = data_dir + '/test/%s.tiff' % id\n",
    "            json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "            image = read_tiff(image_file)\n",
    "            height, width = image.shape[:2]\n",
    "            structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "            mask = None\n",
    "\n",
    "\n",
    "        #--- predict here!  ---\n",
    "        tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "        tile_image = tile['tile_image']\n",
    "        tile_image = np.stack(tile_image)[..., ::-1]\n",
    "        tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "        tile_image = tile_image.astype(np.float32)/255\n",
    "        print(tile_image.shape)\n",
    "        tile_probability = []\n",
    "        \n",
    "        batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "        for t,m in enumerate(batch):\n",
    "            print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "            m = torch.from_numpy(m).to(device)\n",
    "\n",
    "            p = []\n",
    "            with torch.no_grad():\n",
    "                logit = net(m)\n",
    "                p.append(torch.sigmoid(logit))\n",
    "\n",
    "                #---\n",
    "                if server == 'kaggle':\n",
    "                    if 1: #tta here\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.stack(p).mean(0)\n",
    "                if server == 'local':\n",
    "                    if 0: #tta here\n",
    "                        #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                        logit = net(m.flip(dims=(2,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                        #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                        logit = net(m.flip(dims=(3,)))\n",
    "                        p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                    p = torch.cat(p)\n",
    "                    #p = torch.stack(p)\n",
    "\n",
    "            tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "        print('\\r' , end='',flush=True)\n",
    "        log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "        tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "        height, width = tile['image_small'].shape[:2]\n",
    "        probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                              tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                              aggregate='mean')\n",
    "        \n",
    "\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = (probability>thres).astype(np.float32)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir + args.sub+'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0\n",
    "    \n",
    "def run_submit_ensemble(args):\n",
    "\n",
    "    #fold = 6\n",
    "    out_dir = args.en_model_path[0].split('checkpoint')[0]\n",
    "    \n",
    "    \n",
    "    # local은 cv측정 용도\n",
    "\n",
    "    server = args.server#'kaggle' , 'local'\n",
    "\n",
    "    #---\n",
    "    submit_dir = out_dir + '/test/%s-%s-thres(%s)'%(server, args.sub,thres)\n",
    "    os.makedirs(submit_dir,exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.submit.txt',mode='a')\n",
    "\n",
    "    #---\n",
    "    if server == 'local':\n",
    "        valid_image_id = make_image_id('valid-%d' % fold)\n",
    "    if server == 'kaggle':\n",
    "        valid_image_id = make_image_id('test-all')\n",
    "\n",
    "    if server == 'local':\n",
    "        tile_size = args.tile_size #320\n",
    "        tile_average_step = args.tile_average_step#320 #192\n",
    "        tile_scale = args.tile_scale\n",
    "        tile_min_score = args.tile_min_score\n",
    "    if server == 'kaggle' :\n",
    "        tile_size = args.tile_size#640#640 #320\n",
    "        tile_average_step = args.tile_average_step#320#320 #192\n",
    "        tile_scale = args.tile_scale#0.25\n",
    "        tile_min_score = args.tile_min_score#0.25   \n",
    "\n",
    "    log.write('tile_size = %d \\n'%tile_size)\n",
    "    log.write('tile_average_step = %d \\n'%tile_average_step)\n",
    "    log.write('tile_scale = %f \\n'%tile_scale)\n",
    "    log.write('tile_min_score = %f \\n'%tile_min_score)\n",
    "    log.write('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    start_timer = timer()\n",
    "    for id in valid_image_id:\n",
    "        fold_prob = []\n",
    "        models = SegModel()\n",
    "        for i, m_p in enumerate(args.en_model_path):\n",
    "            initial_checkpoint = m_p\n",
    "            # ----- model -------\n",
    "            net = models[i]\n",
    "            net.to(device)\n",
    "            state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "            for key in list(state_dict.keys()):\n",
    "                if \"module.\" in key:\n",
    "                    state_dict[key.replace(\"module.\", \"\")] = state_dict[key]\n",
    "                    del state_dict[key]\n",
    "            net.load_state_dict(state_dict,strict=True)  #True\n",
    "            net = net.eval()\n",
    "            print(\"model load success!!!\")\n",
    "            if server == 'local':\n",
    "                image_file = data_dir + '/train/%s.tiff' % id\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                json_file  = data_dir + '/train/%s-anatomical-structure.json' % id\n",
    "                structure = draw_strcuture_from_hue(image, fill=255, scale=tile_scale/32)   \n",
    "                mask_file = data_dir + '/train/%s.mask.png' % id\n",
    "                mask  = read_mask(mask_file)\n",
    "\n",
    "            if server == 'kaggle':\n",
    "                image_file = data_dir + '/test/%s.tiff' % id\n",
    "                json_file  = data_dir + '/test/%s-anatomical-structure.json' % id\n",
    "\n",
    "                image = read_tiff(image_file)\n",
    "                height, width = image.shape[:2]\n",
    "                structure = draw_strcuture(read_json_as_df(json_file), height, width, structure=['Cortex'])\n",
    "\n",
    "                mask = None\n",
    "\n",
    "\n",
    "            #--- predict here!  ---\n",
    "            tile = to_tile(image, mask, structure, tile_scale, tile_size, tile_average_step, tile_min_score)\n",
    "\n",
    "            tile_image = tile['tile_image']\n",
    "            tile_image = np.stack(tile_image)[..., ::-1]\n",
    "            tile_image = np.ascontiguousarray(tile_image.transpose(0,3,1,2))\n",
    "            tile_image = tile_image.astype(np.float32)/255\n",
    "            print(tile_image.shape)\n",
    "            tile_probability = []\n",
    "\n",
    "            batch = np.array_split(tile_image, len(tile_image)//4)\n",
    "            for t,m in enumerate(batch):\n",
    "                print('\\r %s  %d / %d   %s'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')), end='',flush=True)\n",
    "                m = torch.from_numpy(m).to(device)\n",
    "\n",
    "                p = []\n",
    "                with torch.no_grad():\n",
    "                    logit = net(m)\n",
    "                    p.append(torch.sigmoid(logit))\n",
    "\n",
    "                    #---\n",
    "                    if server == 'kaggle':\n",
    "                        if 1: #tta here\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.stack(p).mean(0)\n",
    "                    if server == 'local':\n",
    "                        if 0: #tta here\n",
    "                            #logit = data_parallel(net, m.flip(dims=(2,)))\n",
    "                            logit = net(m.flip(dims=(2,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(2,))))\n",
    "\n",
    "                            #logit = data_parallel(net, m.flip(dims=(3,)))\n",
    "                            logit = net(m.flip(dims=(3,)))\n",
    "                            p.append(torch.sigmoid(logit.flip(dims=(3,))))\n",
    "                        p = torch.cat(p)\n",
    "                        #p = torch.stack(p)\n",
    "\n",
    "                tile_probability.append(p.data.cpu().numpy())\n",
    "\n",
    "            print('\\r' , end='',flush=True)\n",
    "            log.write('%s  %d / %d   %s\\n'%(id, t, len(batch), time_to_str(timer() - start_timer, 'sec')))\n",
    "\n",
    "            tile_probability = np.concatenate(tile_probability).squeeze(1)\n",
    "            height, width = tile['image_small'].shape[:2]\n",
    "            probability = to_mask(tile_probability, tile['coord'], height, width,\n",
    "                                  tile_scale, tile_size, tile_average_step, tile_min_score,\n",
    "                                  aggregate='mean')\n",
    "\n",
    "            fold_prob.append(probability)\n",
    "        \n",
    "        probability = sum(fold_prob)/len(args.en_model_path)\n",
    "        #--- show results ---\n",
    "        if server == 'local':\n",
    "            truth = tile['mask_small'].astype(np.float32)/255\n",
    "            truth2 = np.concatenate(tile['tile_mask']).astype(np.float32)/255\n",
    "        if server == 'kaggle':\n",
    "            truth = np.zeros((height, width), np.float32)\n",
    "\n",
    "        overlay = np.dstack([\n",
    "            np.zeros_like(truth),\n",
    "            probability, #green\n",
    "            truth, #red\n",
    "        ])\n",
    "        image_small = tile['image_small'].astype(np.float32)/255\n",
    "        predict = (probability>thres).astype(np.float32)\n",
    "        overlay1 = 1-(1-image_small)*(1-overlay)\n",
    "        overlay2 = image_small.copy()\n",
    "        overlay2 = draw_contour_overlay(overlay2, tile['structure_small'], color=(1, 1, 1), thickness=3)\n",
    "        overlay2 = draw_contour_overlay(overlay2, truth, color=(0, 0, 1), thickness=8)\n",
    "        overlay2 = draw_contour_overlay(overlay2, probability, color=(0, 1, 0), thickness=3)\n",
    "\n",
    "        if 1:\n",
    "            cv2.imwrite(submit_dir+'/%s.image_small.png'%id, (image_small*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.probability.png'%id, (probability*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.predict.png'%id, (predict*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay.png'%id, (overlay*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay1.png'%id, (overlay1*255).astype(np.uint8))\n",
    "            cv2.imwrite(submit_dir+'/%s.overlay2.png'%id, (overlay2*255).astype(np.uint8))\n",
    "\n",
    "        #---\n",
    "\n",
    "        if server == 'local':\n",
    "\n",
    "            loss = np_binary_cross_entropy_loss(probability, truth)\n",
    "            dice = np_dice_score(probability, truth) # 여기는 큰이미지로 바꾼상태에서 dice\n",
    "            dice2 = np_dice_score(tile_probability, truth2) # 작은이미지상태, 즉 training과 같은 cv구할려고 dice\n",
    "            tp, tn = np_accuracy(probability, truth)\n",
    "            log.write('submit_dir = %s \\n'%submit_dir)\n",
    "            log.write('initial_checkpoint = %s \\n'%initial_checkpoint)\n",
    "            log.write('loss   = %0.8f \\n'%loss)\n",
    "            log.write('dice   = %0.8f \\n'%dice)\n",
    "            log.write('dice2   = %0.8f \\n'%dice2)\n",
    "            log.write('tp, tn = %0.8f, %0.8f \\n'%(tp, tn))\n",
    "            log.write('\\n')\n",
    "            #cv2.waitKey(0)\n",
    "\n",
    "    #-----\n",
    "    if server == 'kaggle':\n",
    "        csv_file = submit_dir +'.csv'\n",
    "        df = mask_to_csv(valid_image_id, submit_dir)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(df)\n",
    "\n",
    "    zz=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "brazilian-pennsylvania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_size = 640 \n",
      "tile_average_step = 320 \n",
      "tile_scale = 0.500000 \n",
      "tile_min_score = 0.250000 \n",
      "\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    1 min 25 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    3 min 01 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    5 min 45 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    7 min 26 secc\n",
      "model load success!!!\n",
      "(1423, 3, 640, 640)\n",
      "2ec3f1bb9  354 / 355    8 min 49 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   10 min 39 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   11 min 13 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   12 min 12 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   13 min 06 secc\n",
      "model load success!!!\n",
      "(501, 3, 640, 640)\n",
      "3589adb90  124 / 125   13 min 45 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   16 min 44 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   19 min 03 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   22 min 50 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   25 min 19 secc\n",
      "model load success!!!\n",
      "(915, 3, 640, 640)\n",
      "57512b7f1  227 / 228   26 min 44 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   31 min 37 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   35 min 23 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   41 min 10 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   45 min 11 secc\n",
      "model load success!!!\n",
      "(1567, 3, 640, 640)\n",
      "aa05346ff  390 / 391   48 min 24 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   51 min 50 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   54 min 05 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   57 min 47 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   60 min 09 secc\n",
      "model load success!!!\n",
      "(888, 3, 640, 640)\n",
      "d488c759a  221 / 222   62 min 08 secc\n",
      "          id                                          predicted\n",
      "0  2ec3f1bb9  60762289 44 60786279 44 60810257 64 60834247 6...\n",
      "1  3589adb90  68600108 39 68629540 41 68658950 71 68688382 7...\n",
      "2  57512b7f1  329019035 34 329052275 34 329085507 54 3291187...\n",
      "3  aa05346ff  59860739 370 59891459 370 59922177 372 5995289...\n",
      "4  d488c759a  548482197 46 548528857 46 548575509 64 5486221...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 0: #normal\n",
    "    if __name__ == '__main__':\n",
    "        run_submit(args)\n",
    "elif 1:# ensemble\n",
    "    if __name__ == '__main__':\n",
    "        run_submit_ensemble(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-cornell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-fields",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubmap",
   "language": "python",
   "name": "hubmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
